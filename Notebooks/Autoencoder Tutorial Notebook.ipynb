{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 Autoencoder Tutorial Notebook\n",
    "\n",
    "This Notebook should give a quick introduction to how Autoencoders work. It will be trained on the MNIST handwritten digit dataset. The code is dependant on keras, matplotlib and numpy while using Tensorflow 2.X.\n",
    "\n",
    "\n",
    "## Sources:\n",
    "MNIST Database:\n",
    " - http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Papers:\n",
    " - https://arxiv.org/abs/1805.00251\n",
    " - https://arxiv.org/abs/1805.09730\n",
    "\n",
    "Tutorials:\n",
    " - https://blog.keras.io/building-autoencoders-in-keras.html\n",
    " \n",
    " \n",
    " ## Training the Autoencoder\n",
    " \n",
    " The Autoencoder is composed of an encoder and a decoder half. Once an MNIST image is fed into the network, it is compressed to the *feature_map* layer, which maps the small input space to the generated MNIST images. The image fed into the autoencoder and outputted by it should be identical. The loss function penalizes the network when there is a big difference.\n",
    " \n",
    " <img src=\"Autoencoder MNIST Explanation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2037b680308>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape, Dense, Flatten, InputLayer, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Expand Dimensions to 3D to add one for the color channel of black and white: expand_dims(train_images, axis=-1)\n",
    "# Normalize the image by dividing by 255 --> faster convergence\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = np.expand_dims(train_images, axis=-1) / 255.0, np.expand_dims(train_images, axis=-1) / 255.0\n",
    "\n",
    "\n",
    "# Examples\n",
    "# Shape is (60000, 28, 28, 1) for 60 000 images with dimensions 28 x 28 and 1 color channel (black and white)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "plt.imshow(train_images[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2037b6e3648>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to create a batch\n",
    "# It returns identical image pairs: *train_images* = *y_train_images*\n",
    "\n",
    "# Sample minibatch from MNIST\n",
    "def get_MNIST_samples(n_samples):\n",
    "    # Pick new image samples from dataset\n",
    "    # Pick random images to help with gradient descent\n",
    "    image_samples = np.zeros((n_samples, 28, 28, 1))\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "        index = random.randint(0, 59999)\n",
    "        image_samples[i] = train_images[index]\n",
    "\n",
    "    # Both x and y are the same, since the autoencoder should produce the same image\n",
    "    return image_samples, image_samples\n",
    "\n",
    "\n",
    "# Examples\n",
    "train_images_example, y_train_images_example = get_MNIST_samples(32)\n",
    "plt.imshow(train_images_example[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder, Decoder and the whole Autoencoder with Keras\n",
    "\n",
    "# Build the encoder\n",
    "def build_encoder():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the decoder\n",
    "def build_decoder():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=2))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # activation='sigmoid' to get the values for the image between 0 and 1\n",
    "    model.add(Dense(784, activation='sigmoid'))\n",
    "\n",
    "    # None ist the batch size, 28 x 28 is the input image with 1 color channel\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the Autoencoder by combining the encoder and the decoder\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(encoder)\n",
    "    model.add(decoder)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(encoder, decoder, autoencoder_model, n_epochs=100, n_batch=32):\n",
    "    batches_per_epoch = int(60000 / n_batch)\n",
    "\n",
    "    for epoch in range(0, n_epochs):\n",
    "        for batch in range(0, batches_per_epoch):\n",
    "\n",
    "            # Get MNIST images\n",
    "            train_images, y_train_images = get_MNIST_samples(n_batch)\n",
    "\n",
    "            # Run a batch through the autoencoder_model\n",
    "            autoencoder_loss = autoencoder_model.train_on_batch(train_images, y_train_images)\n",
    "\n",
    "            print('Epoch: ', epoch, '   Batch Number: ', batch, ' / ', batches_per_epoch, '   Autoencoder loss: ',  autoencoder_loss)\n",
    "\n",
    "            # Save values and models\n",
    "            if batch % 1800 == 0:\n",
    "                # Save the generator and discriminator\n",
    "                encoder.save('encoder.h5', )\n",
    "                decoder.save('decoder.h5')\n",
    "                autoencoder_model.save('autoencoder_model.h5')\n",
    "\n",
    "                # Predict some images to read out the progress of the autoencoder\n",
    "                autoencoder_model_output = autoencoder_model.predict(train_images)\n",
    "\n",
    "                # Print generated_images with matplotlib\n",
    "                # First the images from train_images, then autoencoder_moder_output\n",
    "                for i in range(1, 5):\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"MNIST\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(train_images[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 1 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"Generated\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(autoencoder_model_output[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 2 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"MNIST\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(train_images[i + 4, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 3 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"Generated\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(autoencoder_model_output[i + 4, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                plt.savefig('plot  epoch ' + '%02d' % epoch + '  batch ' + '%05d' % batch + '.png', dpi=600)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  0  /  1875    Autoencoder loss:  0.6931300163269043\n",
      "Epoch:  0    Batch Number:  1  /  1875    Autoencoder loss:  0.692430317401886\n",
      "Epoch:  0    Batch Number:  2  /  1875    Autoencoder loss:  0.6913483142852783\n",
      "Epoch:  0    Batch Number:  3  /  1875    Autoencoder loss:  0.6891655325889587\n",
      "Epoch:  0    Batch Number:  4  /  1875    Autoencoder loss:  0.6857895851135254\n",
      "Epoch:  0    Batch Number:  5  /  1875    Autoencoder loss:  0.6800934672355652\n",
      "Epoch:  0    Batch Number:  6  /  1875    Autoencoder loss:  0.670768678188324\n",
      "Epoch:  0    Batch Number:  7  /  1875    Autoencoder loss:  0.6567944288253784\n",
      "Epoch:  0    Batch Number:  8  /  1875    Autoencoder loss:  0.6388950943946838\n",
      "Epoch:  0    Batch Number:  9  /  1875    Autoencoder loss:  0.6063756942749023\n",
      "Epoch:  0    Batch Number:  10  /  1875    Autoencoder loss:  0.575364887714386\n",
      "Epoch:  0    Batch Number:  11  /  1875    Autoencoder loss:  0.526074230670929\n",
      "Epoch:  0    Batch Number:  12  /  1875    Autoencoder loss:  0.46610867977142334\n",
      "Epoch:  0    Batch Number:  13  /  1875    Autoencoder loss:  0.4187062978744507\n",
      "Epoch:  0    Batch Number:  14  /  1875    Autoencoder loss:  0.38232484459877014\n",
      "Epoch:  0    Batch Number:  15  /  1875    Autoencoder loss:  0.3475416600704193\n",
      "Epoch:  0    Batch Number:  16  /  1875    Autoencoder loss:  0.3740389943122864\n",
      "Epoch:  0    Batch Number:  17  /  1875    Autoencoder loss:  0.33360522985458374\n",
      "Epoch:  0    Batch Number:  18  /  1875    Autoencoder loss:  0.35773786902427673\n",
      "Epoch:  0    Batch Number:  19  /  1875    Autoencoder loss:  0.32062485814094543\n",
      "Epoch:  0    Batch Number:  20  /  1875    Autoencoder loss:  0.3254237174987793\n",
      "Epoch:  0    Batch Number:  21  /  1875    Autoencoder loss:  0.3242146968841553\n",
      "Epoch:  0    Batch Number:  22  /  1875    Autoencoder loss:  0.2918025851249695\n",
      "Epoch:  0    Batch Number:  23  /  1875    Autoencoder loss:  0.2915359437465668\n",
      "Epoch:  0    Batch Number:  24  /  1875    Autoencoder loss:  0.28013041615486145\n",
      "Epoch:  0    Batch Number:  25  /  1875    Autoencoder loss:  0.2847973704338074\n",
      "Epoch:  0    Batch Number:  26  /  1875    Autoencoder loss:  0.2961021661758423\n",
      "Epoch:  0    Batch Number:  27  /  1875    Autoencoder loss:  0.2926750183105469\n",
      "Epoch:  0    Batch Number:  28  /  1875    Autoencoder loss:  0.298733115196228\n",
      "Epoch:  0    Batch Number:  29  /  1875    Autoencoder loss:  0.2887168824672699\n",
      "Epoch:  0    Batch Number:  30  /  1875    Autoencoder loss:  0.28243163228034973\n",
      "Epoch:  0    Batch Number:  31  /  1875    Autoencoder loss:  0.2792872488498688\n",
      "Epoch:  0    Batch Number:  32  /  1875    Autoencoder loss:  0.2850908935070038\n",
      "Epoch:  0    Batch Number:  33  /  1875    Autoencoder loss:  0.28934288024902344\n",
      "Epoch:  0    Batch Number:  34  /  1875    Autoencoder loss:  0.27851760387420654\n",
      "Epoch:  0    Batch Number:  35  /  1875    Autoencoder loss:  0.28666648268699646\n",
      "Epoch:  0    Batch Number:  36  /  1875    Autoencoder loss:  0.2890317440032959\n",
      "Epoch:  0    Batch Number:  37  /  1875    Autoencoder loss:  0.2848730683326721\n",
      "Epoch:  0    Batch Number:  38  /  1875    Autoencoder loss:  0.2877426743507385\n",
      "Epoch:  0    Batch Number:  39  /  1875    Autoencoder loss:  0.276857852935791\n",
      "Epoch:  0    Batch Number:  40  /  1875    Autoencoder loss:  0.27738478779792786\n",
      "Epoch:  0    Batch Number:  41  /  1875    Autoencoder loss:  0.2702949345111847\n",
      "Epoch:  0    Batch Number:  42  /  1875    Autoencoder loss:  0.2724240720272064\n",
      "Epoch:  0    Batch Number:  43  /  1875    Autoencoder loss:  0.27700239419937134\n",
      "Epoch:  0    Batch Number:  44  /  1875    Autoencoder loss:  0.2757980227470398\n",
      "Epoch:  0    Batch Number:  45  /  1875    Autoencoder loss:  0.27705666422843933\n",
      "Epoch:  0    Batch Number:  46  /  1875    Autoencoder loss:  0.26792165637016296\n",
      "Epoch:  0    Batch Number:  47  /  1875    Autoencoder loss:  0.28067487478256226\n",
      "Epoch:  0    Batch Number:  48  /  1875    Autoencoder loss:  0.30033817887306213\n",
      "Epoch:  0    Batch Number:  49  /  1875    Autoencoder loss:  0.28241071105003357\n",
      "Epoch:  0    Batch Number:  50  /  1875    Autoencoder loss:  0.26761844754219055\n",
      "Epoch:  0    Batch Number:  51  /  1875    Autoencoder loss:  0.2777203619480133\n",
      "Epoch:  0    Batch Number:  52  /  1875    Autoencoder loss:  0.277608722448349\n",
      "Epoch:  0    Batch Number:  53  /  1875    Autoencoder loss:  0.2562326192855835\n",
      "Epoch:  0    Batch Number:  54  /  1875    Autoencoder loss:  0.2692125141620636\n",
      "Epoch:  0    Batch Number:  55  /  1875    Autoencoder loss:  0.2708241939544678\n",
      "Epoch:  0    Batch Number:  56  /  1875    Autoencoder loss:  0.24122464656829834\n",
      "Epoch:  0    Batch Number:  57  /  1875    Autoencoder loss:  0.27234622836112976\n",
      "Epoch:  0    Batch Number:  58  /  1875    Autoencoder loss:  0.2833966612815857\n",
      "Epoch:  0    Batch Number:  59  /  1875    Autoencoder loss:  0.26993003487586975\n",
      "Epoch:  0    Batch Number:  60  /  1875    Autoencoder loss:  0.27941426634788513\n",
      "Epoch:  0    Batch Number:  61  /  1875    Autoencoder loss:  0.2772308886051178\n",
      "Epoch:  0    Batch Number:  62  /  1875    Autoencoder loss:  0.27955368161201477\n",
      "Epoch:  0    Batch Number:  63  /  1875    Autoencoder loss:  0.2678900361061096\n",
      "Epoch:  0    Batch Number:  64  /  1875    Autoencoder loss:  0.2745627164840698\n",
      "Epoch:  0    Batch Number:  65  /  1875    Autoencoder loss:  0.2570038437843323\n",
      "Epoch:  0    Batch Number:  66  /  1875    Autoencoder loss:  0.26883265376091003\n",
      "Epoch:  0    Batch Number:  67  /  1875    Autoencoder loss:  0.2579096853733063\n",
      "Epoch:  0    Batch Number:  68  /  1875    Autoencoder loss:  0.2656664252281189\n",
      "Epoch:  0    Batch Number:  69  /  1875    Autoencoder loss:  0.2792796194553375\n",
      "Epoch:  0    Batch Number:  70  /  1875    Autoencoder loss:  0.2738698422908783\n",
      "Epoch:  0    Batch Number:  71  /  1875    Autoencoder loss:  0.26379895210266113\n",
      "Epoch:  0    Batch Number:  72  /  1875    Autoencoder loss:  0.2528105676174164\n",
      "Epoch:  0    Batch Number:  73  /  1875    Autoencoder loss:  0.25662490725517273\n",
      "Epoch:  0    Batch Number:  74  /  1875    Autoencoder loss:  0.288160115480423\n",
      "Epoch:  0    Batch Number:  75  /  1875    Autoencoder loss:  0.2763654589653015\n",
      "Epoch:  0    Batch Number:  76  /  1875    Autoencoder loss:  0.28006941080093384\n",
      "Epoch:  0    Batch Number:  77  /  1875    Autoencoder loss:  0.2777737081050873\n",
      "Epoch:  0    Batch Number:  78  /  1875    Autoencoder loss:  0.2512649893760681\n",
      "Epoch:  0    Batch Number:  79  /  1875    Autoencoder loss:  0.26587560772895813\n",
      "Epoch:  0    Batch Number:  80  /  1875    Autoencoder loss:  0.2700144350528717\n",
      "Epoch:  0    Batch Number:  81  /  1875    Autoencoder loss:  0.2752302587032318\n",
      "Epoch:  0    Batch Number:  82  /  1875    Autoencoder loss:  0.2631361484527588\n",
      "Epoch:  0    Batch Number:  83  /  1875    Autoencoder loss:  0.2848038673400879\n",
      "Epoch:  0    Batch Number:  84  /  1875    Autoencoder loss:  0.25488609075546265\n",
      "Epoch:  0    Batch Number:  85  /  1875    Autoencoder loss:  0.2513428330421448\n",
      "Epoch:  0    Batch Number:  86  /  1875    Autoencoder loss:  0.25928041338920593\n",
      "Epoch:  0    Batch Number:  87  /  1875    Autoencoder loss:  0.2802610993385315\n",
      "Epoch:  0    Batch Number:  88  /  1875    Autoencoder loss:  0.2772638499736786\n",
      "Epoch:  0    Batch Number:  89  /  1875    Autoencoder loss:  0.2634700834751129\n",
      "Epoch:  0    Batch Number:  90  /  1875    Autoencoder loss:  0.2786467671394348\n",
      "Epoch:  0    Batch Number:  91  /  1875    Autoencoder loss:  0.2644675374031067\n",
      "Epoch:  0    Batch Number:  92  /  1875    Autoencoder loss:  0.265226811170578\n",
      "Epoch:  0    Batch Number:  93  /  1875    Autoencoder loss:  0.26694872975349426\n",
      "Epoch:  0    Batch Number:  94  /  1875    Autoencoder loss:  0.2555994391441345\n",
      "Epoch:  0    Batch Number:  95  /  1875    Autoencoder loss:  0.26566413044929504\n",
      "Epoch:  0    Batch Number:  96  /  1875    Autoencoder loss:  0.26405808329582214\n",
      "Epoch:  0    Batch Number:  97  /  1875    Autoencoder loss:  0.27287906408309937\n",
      "Epoch:  0    Batch Number:  98  /  1875    Autoencoder loss:  0.27535441517829895\n",
      "Epoch:  0    Batch Number:  99  /  1875    Autoencoder loss:  0.26658201217651367\n",
      "Epoch:  0    Batch Number:  100  /  1875    Autoencoder loss:  0.25656360387802124\n",
      "Epoch:  0    Batch Number:  101  /  1875    Autoencoder loss:  0.2608660161495209\n",
      "Epoch:  0    Batch Number:  102  /  1875    Autoencoder loss:  0.24874062836170197\n",
      "Epoch:  0    Batch Number:  103  /  1875    Autoencoder loss:  0.27597200870513916\n",
      "Epoch:  0    Batch Number:  104  /  1875    Autoencoder loss:  0.268733948469162\n",
      "Epoch:  0    Batch Number:  105  /  1875    Autoencoder loss:  0.2547134757041931\n",
      "Epoch:  0    Batch Number:  106  /  1875    Autoencoder loss:  0.2721998393535614\n",
      "Epoch:  0    Batch Number:  107  /  1875    Autoencoder loss:  0.24816329777240753\n",
      "Epoch:  0    Batch Number:  108  /  1875    Autoencoder loss:  0.2939685583114624\n",
      "Epoch:  0    Batch Number:  109  /  1875    Autoencoder loss:  0.25641101598739624\n",
      "Epoch:  0    Batch Number:  110  /  1875    Autoencoder loss:  0.2533627152442932\n",
      "Epoch:  0    Batch Number:  111  /  1875    Autoencoder loss:  0.25823959708213806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  112  /  1875    Autoencoder loss:  0.25927504897117615\n",
      "Epoch:  0    Batch Number:  113  /  1875    Autoencoder loss:  0.26446717977523804\n",
      "Epoch:  0    Batch Number:  114  /  1875    Autoencoder loss:  0.26477599143981934\n",
      "Epoch:  0    Batch Number:  115  /  1875    Autoencoder loss:  0.2692669630050659\n",
      "Epoch:  0    Batch Number:  116  /  1875    Autoencoder loss:  0.2703609764575958\n",
      "Epoch:  0    Batch Number:  117  /  1875    Autoencoder loss:  0.2699694335460663\n",
      "Epoch:  0    Batch Number:  118  /  1875    Autoencoder loss:  0.25735223293304443\n",
      "Epoch:  0    Batch Number:  119  /  1875    Autoencoder loss:  0.27180877327919006\n",
      "Epoch:  0    Batch Number:  120  /  1875    Autoencoder loss:  0.2696841359138489\n",
      "Epoch:  0    Batch Number:  121  /  1875    Autoencoder loss:  0.26790082454681396\n",
      "Epoch:  0    Batch Number:  122  /  1875    Autoencoder loss:  0.2547975182533264\n",
      "Epoch:  0    Batch Number:  123  /  1875    Autoencoder loss:  0.2552618980407715\n",
      "Epoch:  0    Batch Number:  124  /  1875    Autoencoder loss:  0.24703426659107208\n",
      "Epoch:  0    Batch Number:  125  /  1875    Autoencoder loss:  0.2686122953891754\n",
      "Epoch:  0    Batch Number:  126  /  1875    Autoencoder loss:  0.2650763690471649\n",
      "Epoch:  0    Batch Number:  127  /  1875    Autoencoder loss:  0.28748565912246704\n",
      "Epoch:  0    Batch Number:  128  /  1875    Autoencoder loss:  0.2518286108970642\n",
      "Epoch:  0    Batch Number:  129  /  1875    Autoencoder loss:  0.24778252840042114\n",
      "Epoch:  0    Batch Number:  130  /  1875    Autoencoder loss:  0.2832931578159332\n",
      "Epoch:  0    Batch Number:  131  /  1875    Autoencoder loss:  0.2634310722351074\n",
      "Epoch:  0    Batch Number:  132  /  1875    Autoencoder loss:  0.2619333565235138\n",
      "Epoch:  0    Batch Number:  133  /  1875    Autoencoder loss:  0.24880638718605042\n",
      "Epoch:  0    Batch Number:  134  /  1875    Autoencoder loss:  0.2684764862060547\n",
      "Epoch:  0    Batch Number:  135  /  1875    Autoencoder loss:  0.2646214962005615\n",
      "Epoch:  0    Batch Number:  136  /  1875    Autoencoder loss:  0.2742552161216736\n",
      "Epoch:  0    Batch Number:  137  /  1875    Autoencoder loss:  0.2735728919506073\n",
      "Epoch:  0    Batch Number:  138  /  1875    Autoencoder loss:  0.27809780836105347\n",
      "Epoch:  0    Batch Number:  139  /  1875    Autoencoder loss:  0.25933924317359924\n",
      "Epoch:  0    Batch Number:  140  /  1875    Autoencoder loss:  0.24905991554260254\n",
      "Epoch:  0    Batch Number:  141  /  1875    Autoencoder loss:  0.2596533000469208\n",
      "Epoch:  0    Batch Number:  142  /  1875    Autoencoder loss:  0.26517513394355774\n",
      "Epoch:  0    Batch Number:  143  /  1875    Autoencoder loss:  0.2748138904571533\n",
      "Epoch:  0    Batch Number:  144  /  1875    Autoencoder loss:  0.2608238458633423\n",
      "Epoch:  0    Batch Number:  145  /  1875    Autoencoder loss:  0.2578500211238861\n",
      "Epoch:  0    Batch Number:  146  /  1875    Autoencoder loss:  0.25679832696914673\n",
      "Epoch:  0    Batch Number:  147  /  1875    Autoencoder loss:  0.26173529028892517\n",
      "Epoch:  0    Batch Number:  148  /  1875    Autoencoder loss:  0.24534162878990173\n",
      "Epoch:  0    Batch Number:  149  /  1875    Autoencoder loss:  0.2762076258659363\n",
      "Epoch:  0    Batch Number:  150  /  1875    Autoencoder loss:  0.26427391171455383\n",
      "Epoch:  0    Batch Number:  151  /  1875    Autoencoder loss:  0.24800510704517365\n",
      "Epoch:  0    Batch Number:  152  /  1875    Autoencoder loss:  0.25536060333251953\n",
      "Epoch:  0    Batch Number:  153  /  1875    Autoencoder loss:  0.2709309458732605\n",
      "Epoch:  0    Batch Number:  154  /  1875    Autoencoder loss:  0.256061315536499\n",
      "Epoch:  0    Batch Number:  155  /  1875    Autoencoder loss:  0.260143905878067\n",
      "Epoch:  0    Batch Number:  156  /  1875    Autoencoder loss:  0.26467153429985046\n",
      "Epoch:  0    Batch Number:  157  /  1875    Autoencoder loss:  0.2619868218898773\n",
      "Epoch:  0    Batch Number:  158  /  1875    Autoencoder loss:  0.27378416061401367\n",
      "Epoch:  0    Batch Number:  159  /  1875    Autoencoder loss:  0.27309730648994446\n",
      "Epoch:  0    Batch Number:  160  /  1875    Autoencoder loss:  0.24549336731433868\n",
      "Epoch:  0    Batch Number:  161  /  1875    Autoencoder loss:  0.2735358774662018\n",
      "Epoch:  0    Batch Number:  162  /  1875    Autoencoder loss:  0.26527079939842224\n",
      "Epoch:  0    Batch Number:  163  /  1875    Autoencoder loss:  0.26407575607299805\n",
      "Epoch:  0    Batch Number:  164  /  1875    Autoencoder loss:  0.23782353103160858\n",
      "Epoch:  0    Batch Number:  165  /  1875    Autoencoder loss:  0.27502620220184326\n",
      "Epoch:  0    Batch Number:  166  /  1875    Autoencoder loss:  0.27650347352027893\n",
      "Epoch:  0    Batch Number:  167  /  1875    Autoencoder loss:  0.24194149672985077\n",
      "Epoch:  0    Batch Number:  168  /  1875    Autoencoder loss:  0.25616082549095154\n",
      "Epoch:  0    Batch Number:  169  /  1875    Autoencoder loss:  0.2716084420681\n",
      "Epoch:  0    Batch Number:  170  /  1875    Autoencoder loss:  0.24354973435401917\n",
      "Epoch:  0    Batch Number:  171  /  1875    Autoencoder loss:  0.2586192786693573\n",
      "Epoch:  0    Batch Number:  172  /  1875    Autoencoder loss:  0.28779956698417664\n",
      "Epoch:  0    Batch Number:  173  /  1875    Autoencoder loss:  0.2711277902126312\n",
      "Epoch:  0    Batch Number:  174  /  1875    Autoencoder loss:  0.2691746652126312\n",
      "Epoch:  0    Batch Number:  175  /  1875    Autoencoder loss:  0.27292874455451965\n",
      "Epoch:  0    Batch Number:  176  /  1875    Autoencoder loss:  0.26500627398490906\n",
      "Epoch:  0    Batch Number:  177  /  1875    Autoencoder loss:  0.2732573449611664\n",
      "Epoch:  0    Batch Number:  178  /  1875    Autoencoder loss:  0.25778812170028687\n",
      "Epoch:  0    Batch Number:  179  /  1875    Autoencoder loss:  0.2501734793186188\n",
      "Epoch:  0    Batch Number:  180  /  1875    Autoencoder loss:  0.27508872747421265\n",
      "Epoch:  0    Batch Number:  181  /  1875    Autoencoder loss:  0.23581109941005707\n",
      "Epoch:  0    Batch Number:  182  /  1875    Autoencoder loss:  0.26980891823768616\n",
      "Epoch:  0    Batch Number:  183  /  1875    Autoencoder loss:  0.26014503836631775\n",
      "Epoch:  0    Batch Number:  184  /  1875    Autoencoder loss:  0.26233822107315063\n",
      "Epoch:  0    Batch Number:  185  /  1875    Autoencoder loss:  0.28327834606170654\n",
      "Epoch:  0    Batch Number:  186  /  1875    Autoencoder loss:  0.23545998334884644\n",
      "Epoch:  0    Batch Number:  187  /  1875    Autoencoder loss:  0.24765540659427643\n",
      "Epoch:  0    Batch Number:  188  /  1875    Autoencoder loss:  0.24904964864253998\n",
      "Epoch:  0    Batch Number:  189  /  1875    Autoencoder loss:  0.2507582902908325\n",
      "Epoch:  0    Batch Number:  190  /  1875    Autoencoder loss:  0.2593361735343933\n",
      "Epoch:  0    Batch Number:  191  /  1875    Autoencoder loss:  0.2736220359802246\n",
      "Epoch:  0    Batch Number:  192  /  1875    Autoencoder loss:  0.2676147222518921\n",
      "Epoch:  0    Batch Number:  193  /  1875    Autoencoder loss:  0.25766095519065857\n",
      "Epoch:  0    Batch Number:  194  /  1875    Autoencoder loss:  0.26104843616485596\n",
      "Epoch:  0    Batch Number:  195  /  1875    Autoencoder loss:  0.2511158883571625\n",
      "Epoch:  0    Batch Number:  196  /  1875    Autoencoder loss:  0.2379520833492279\n",
      "Epoch:  0    Batch Number:  197  /  1875    Autoencoder loss:  0.2552935481071472\n",
      "Epoch:  0    Batch Number:  198  /  1875    Autoencoder loss:  0.25842130184173584\n",
      "Epoch:  0    Batch Number:  199  /  1875    Autoencoder loss:  0.2655787765979767\n",
      "Epoch:  0    Batch Number:  200  /  1875    Autoencoder loss:  0.2542858421802521\n",
      "Epoch:  0    Batch Number:  201  /  1875    Autoencoder loss:  0.2595732510089874\n",
      "Epoch:  0    Batch Number:  202  /  1875    Autoencoder loss:  0.25081565976142883\n",
      "Epoch:  0    Batch Number:  203  /  1875    Autoencoder loss:  0.24792765080928802\n",
      "Epoch:  0    Batch Number:  204  /  1875    Autoencoder loss:  0.27421343326568604\n",
      "Epoch:  0    Batch Number:  205  /  1875    Autoencoder loss:  0.24816544353961945\n",
      "Epoch:  0    Batch Number:  206  /  1875    Autoencoder loss:  0.26013097167015076\n",
      "Epoch:  0    Batch Number:  207  /  1875    Autoencoder loss:  0.2712808847427368\n",
      "Epoch:  0    Batch Number:  208  /  1875    Autoencoder loss:  0.2534536123275757\n",
      "Epoch:  0    Batch Number:  209  /  1875    Autoencoder loss:  0.23916251957416534\n",
      "Epoch:  0    Batch Number:  210  /  1875    Autoencoder loss:  0.24642831087112427\n",
      "Epoch:  0    Batch Number:  211  /  1875    Autoencoder loss:  0.2529298961162567\n",
      "Epoch:  0    Batch Number:  212  /  1875    Autoencoder loss:  0.23808728158473969\n",
      "Epoch:  0    Batch Number:  213  /  1875    Autoencoder loss:  0.2537190616130829\n",
      "Epoch:  0    Batch Number:  214  /  1875    Autoencoder loss:  0.24474047124385834\n",
      "Epoch:  0    Batch Number:  215  /  1875    Autoencoder loss:  0.2530493438243866\n",
      "Epoch:  0    Batch Number:  216  /  1875    Autoencoder loss:  0.26537904143333435\n",
      "Epoch:  0    Batch Number:  217  /  1875    Autoencoder loss:  0.2570023536682129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  218  /  1875    Autoencoder loss:  0.2447180598974228\n",
      "Epoch:  0    Batch Number:  219  /  1875    Autoencoder loss:  0.24957045912742615\n",
      "Epoch:  0    Batch Number:  220  /  1875    Autoencoder loss:  0.2609799802303314\n",
      "Epoch:  0    Batch Number:  221  /  1875    Autoencoder loss:  0.24464687705039978\n",
      "Epoch:  0    Batch Number:  222  /  1875    Autoencoder loss:  0.26076897978782654\n",
      "Epoch:  0    Batch Number:  223  /  1875    Autoencoder loss:  0.23708997666835785\n",
      "Epoch:  0    Batch Number:  224  /  1875    Autoencoder loss:  0.2646907567977905\n",
      "Epoch:  0    Batch Number:  225  /  1875    Autoencoder loss:  0.24214069545269012\n",
      "Epoch:  0    Batch Number:  226  /  1875    Autoencoder loss:  0.23920153081417084\n",
      "Epoch:  0    Batch Number:  227  /  1875    Autoencoder loss:  0.21250911056995392\n",
      "Epoch:  0    Batch Number:  228  /  1875    Autoencoder loss:  0.2468346208333969\n",
      "Epoch:  0    Batch Number:  229  /  1875    Autoencoder loss:  0.23146745562553406\n",
      "Epoch:  0    Batch Number:  230  /  1875    Autoencoder loss:  0.2384728044271469\n",
      "Epoch:  0    Batch Number:  231  /  1875    Autoencoder loss:  0.2540352940559387\n",
      "Epoch:  0    Batch Number:  232  /  1875    Autoencoder loss:  0.24891714751720428\n",
      "Epoch:  0    Batch Number:  233  /  1875    Autoencoder loss:  0.2592039406299591\n",
      "Epoch:  0    Batch Number:  234  /  1875    Autoencoder loss:  0.2332347184419632\n",
      "Epoch:  0    Batch Number:  235  /  1875    Autoencoder loss:  0.25329235196113586\n",
      "Epoch:  0    Batch Number:  236  /  1875    Autoencoder loss:  0.25580233335494995\n",
      "Epoch:  0    Batch Number:  237  /  1875    Autoencoder loss:  0.23938636481761932\n",
      "Epoch:  0    Batch Number:  238  /  1875    Autoencoder loss:  0.2485136091709137\n",
      "Epoch:  0    Batch Number:  239  /  1875    Autoencoder loss:  0.25846973061561584\n",
      "Epoch:  0    Batch Number:  240  /  1875    Autoencoder loss:  0.23074971139431\n",
      "Epoch:  0    Batch Number:  241  /  1875    Autoencoder loss:  0.25235122442245483\n",
      "Epoch:  0    Batch Number:  242  /  1875    Autoencoder loss:  0.2625466585159302\n",
      "Epoch:  0    Batch Number:  243  /  1875    Autoencoder loss:  0.24573206901550293\n",
      "Epoch:  0    Batch Number:  244  /  1875    Autoencoder loss:  0.23074468970298767\n",
      "Epoch:  0    Batch Number:  245  /  1875    Autoencoder loss:  0.24691663682460785\n",
      "Epoch:  0    Batch Number:  246  /  1875    Autoencoder loss:  0.23817813396453857\n",
      "Epoch:  0    Batch Number:  247  /  1875    Autoencoder loss:  0.2447919398546219\n",
      "Epoch:  0    Batch Number:  248  /  1875    Autoencoder loss:  0.2528345286846161\n",
      "Epoch:  0    Batch Number:  249  /  1875    Autoencoder loss:  0.24759408831596375\n",
      "Epoch:  0    Batch Number:  250  /  1875    Autoencoder loss:  0.22900480031967163\n",
      "Epoch:  0    Batch Number:  251  /  1875    Autoencoder loss:  0.22604574263095856\n",
      "Epoch:  0    Batch Number:  252  /  1875    Autoencoder loss:  0.25303915143013\n",
      "Epoch:  0    Batch Number:  253  /  1875    Autoencoder loss:  0.25079527497291565\n",
      "Epoch:  0    Batch Number:  254  /  1875    Autoencoder loss:  0.2483014464378357\n",
      "Epoch:  0    Batch Number:  255  /  1875    Autoencoder loss:  0.24122948944568634\n",
      "Epoch:  0    Batch Number:  256  /  1875    Autoencoder loss:  0.23692898452281952\n",
      "Epoch:  0    Batch Number:  257  /  1875    Autoencoder loss:  0.22444097697734833\n",
      "Epoch:  0    Batch Number:  258  /  1875    Autoencoder loss:  0.24216328561306\n",
      "Epoch:  0    Batch Number:  259  /  1875    Autoencoder loss:  0.24286507070064545\n",
      "Epoch:  0    Batch Number:  260  /  1875    Autoencoder loss:  0.252366304397583\n",
      "Epoch:  0    Batch Number:  261  /  1875    Autoencoder loss:  0.24086697399616241\n",
      "Epoch:  0    Batch Number:  262  /  1875    Autoencoder loss:  0.23532238602638245\n",
      "Epoch:  0    Batch Number:  263  /  1875    Autoencoder loss:  0.2424720674753189\n",
      "Epoch:  0    Batch Number:  264  /  1875    Autoencoder loss:  0.23618558049201965\n",
      "Epoch:  0    Batch Number:  265  /  1875    Autoencoder loss:  0.23577402532100677\n",
      "Epoch:  0    Batch Number:  266  /  1875    Autoencoder loss:  0.22694909572601318\n",
      "Epoch:  0    Batch Number:  267  /  1875    Autoencoder loss:  0.22519899904727936\n",
      "Epoch:  0    Batch Number:  268  /  1875    Autoencoder loss:  0.22102172672748566\n",
      "Epoch:  0    Batch Number:  269  /  1875    Autoencoder loss:  0.23288267850875854\n",
      "Epoch:  0    Batch Number:  270  /  1875    Autoencoder loss:  0.25531911849975586\n",
      "Epoch:  0    Batch Number:  271  /  1875    Autoencoder loss:  0.2458137571811676\n",
      "Epoch:  0    Batch Number:  272  /  1875    Autoencoder loss:  0.22430545091629028\n",
      "Epoch:  0    Batch Number:  273  /  1875    Autoencoder loss:  0.22756671905517578\n",
      "Epoch:  0    Batch Number:  274  /  1875    Autoencoder loss:  0.2426401823759079\n",
      "Epoch:  0    Batch Number:  275  /  1875    Autoencoder loss:  0.2437887042760849\n",
      "Epoch:  0    Batch Number:  276  /  1875    Autoencoder loss:  0.2247837632894516\n",
      "Epoch:  0    Batch Number:  277  /  1875    Autoencoder loss:  0.2507863938808441\n",
      "Epoch:  0    Batch Number:  278  /  1875    Autoencoder loss:  0.23646986484527588\n",
      "Epoch:  0    Batch Number:  279  /  1875    Autoencoder loss:  0.22699692845344543\n",
      "Epoch:  0    Batch Number:  280  /  1875    Autoencoder loss:  0.25375300645828247\n",
      "Epoch:  0    Batch Number:  281  /  1875    Autoencoder loss:  0.21889621019363403\n",
      "Epoch:  0    Batch Number:  282  /  1875    Autoencoder loss:  0.25584664940834045\n",
      "Epoch:  0    Batch Number:  283  /  1875    Autoencoder loss:  0.2294379323720932\n",
      "Epoch:  0    Batch Number:  284  /  1875    Autoencoder loss:  0.21677342057228088\n",
      "Epoch:  0    Batch Number:  285  /  1875    Autoencoder loss:  0.24296371638774872\n",
      "Epoch:  0    Batch Number:  286  /  1875    Autoencoder loss:  0.2495650202035904\n",
      "Epoch:  0    Batch Number:  287  /  1875    Autoencoder loss:  0.23039838671684265\n",
      "Epoch:  0    Batch Number:  288  /  1875    Autoencoder loss:  0.23163415491580963\n",
      "Epoch:  0    Batch Number:  289  /  1875    Autoencoder loss:  0.22679366171360016\n",
      "Epoch:  0    Batch Number:  290  /  1875    Autoencoder loss:  0.24192878603935242\n",
      "Epoch:  0    Batch Number:  291  /  1875    Autoencoder loss:  0.25123244524002075\n",
      "Epoch:  0    Batch Number:  292  /  1875    Autoencoder loss:  0.23546509444713593\n",
      "Epoch:  0    Batch Number:  293  /  1875    Autoencoder loss:  0.24537783861160278\n",
      "Epoch:  0    Batch Number:  294  /  1875    Autoencoder loss:  0.23738478124141693\n",
      "Epoch:  0    Batch Number:  295  /  1875    Autoencoder loss:  0.23486775159835815\n",
      "Epoch:  0    Batch Number:  296  /  1875    Autoencoder loss:  0.23165620863437653\n",
      "Epoch:  0    Batch Number:  297  /  1875    Autoencoder loss:  0.22807638347148895\n",
      "Epoch:  0    Batch Number:  298  /  1875    Autoencoder loss:  0.2471594512462616\n",
      "Epoch:  0    Batch Number:  299  /  1875    Autoencoder loss:  0.2441931515932083\n",
      "Epoch:  0    Batch Number:  300  /  1875    Autoencoder loss:  0.22599917650222778\n",
      "Epoch:  0    Batch Number:  301  /  1875    Autoencoder loss:  0.2407347857952118\n",
      "Epoch:  0    Batch Number:  302  /  1875    Autoencoder loss:  0.24260933697223663\n",
      "Epoch:  0    Batch Number:  303  /  1875    Autoencoder loss:  0.2577480673789978\n",
      "Epoch:  0    Batch Number:  304  /  1875    Autoencoder loss:  0.2352680116891861\n",
      "Epoch:  0    Batch Number:  305  /  1875    Autoencoder loss:  0.2472972720861435\n",
      "Epoch:  0    Batch Number:  306  /  1875    Autoencoder loss:  0.2175389677286148\n",
      "Epoch:  0    Batch Number:  307  /  1875    Autoencoder loss:  0.23208777606487274\n",
      "Epoch:  0    Batch Number:  308  /  1875    Autoencoder loss:  0.22951184213161469\n",
      "Epoch:  0    Batch Number:  309  /  1875    Autoencoder loss:  0.2135520726442337\n",
      "Epoch:  0    Batch Number:  310  /  1875    Autoencoder loss:  0.23261266946792603\n",
      "Epoch:  0    Batch Number:  311  /  1875    Autoencoder loss:  0.2393326312303543\n",
      "Epoch:  0    Batch Number:  312  /  1875    Autoencoder loss:  0.23582012951374054\n",
      "Epoch:  0    Batch Number:  313  /  1875    Autoencoder loss:  0.25163382291793823\n",
      "Epoch:  0    Batch Number:  314  /  1875    Autoencoder loss:  0.22929678857326508\n",
      "Epoch:  0    Batch Number:  315  /  1875    Autoencoder loss:  0.2411385029554367\n",
      "Epoch:  0    Batch Number:  316  /  1875    Autoencoder loss:  0.23231059312820435\n",
      "Epoch:  0    Batch Number:  317  /  1875    Autoencoder loss:  0.23918767273426056\n",
      "Epoch:  0    Batch Number:  318  /  1875    Autoencoder loss:  0.24087665975093842\n",
      "Epoch:  0    Batch Number:  319  /  1875    Autoencoder loss:  0.24256457388401031\n",
      "Epoch:  0    Batch Number:  320  /  1875    Autoencoder loss:  0.21589501202106476\n",
      "Epoch:  0    Batch Number:  321  /  1875    Autoencoder loss:  0.24407997727394104\n",
      "Epoch:  0    Batch Number:  322  /  1875    Autoencoder loss:  0.2299528568983078\n",
      "Epoch:  0    Batch Number:  323  /  1875    Autoencoder loss:  0.22766607999801636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  324  /  1875    Autoencoder loss:  0.23688502609729767\n",
      "Epoch:  0    Batch Number:  325  /  1875    Autoencoder loss:  0.22940470278263092\n",
      "Epoch:  0    Batch Number:  326  /  1875    Autoencoder loss:  0.2150745391845703\n",
      "Epoch:  0    Batch Number:  327  /  1875    Autoencoder loss:  0.21554355323314667\n",
      "Epoch:  0    Batch Number:  328  /  1875    Autoencoder loss:  0.22716040909290314\n",
      "Epoch:  0    Batch Number:  329  /  1875    Autoencoder loss:  0.23993909358978271\n",
      "Epoch:  0    Batch Number:  330  /  1875    Autoencoder loss:  0.2386777251958847\n",
      "Epoch:  0    Batch Number:  331  /  1875    Autoencoder loss:  0.24300700426101685\n",
      "Epoch:  0    Batch Number:  332  /  1875    Autoencoder loss:  0.2420405149459839\n",
      "Epoch:  0    Batch Number:  333  /  1875    Autoencoder loss:  0.23119887709617615\n",
      "Epoch:  0    Batch Number:  334  /  1875    Autoencoder loss:  0.24154916405677795\n",
      "Epoch:  0    Batch Number:  335  /  1875    Autoencoder loss:  0.23720188438892365\n",
      "Epoch:  0    Batch Number:  336  /  1875    Autoencoder loss:  0.21955078840255737\n",
      "Epoch:  0    Batch Number:  337  /  1875    Autoencoder loss:  0.2362246960401535\n",
      "Epoch:  0    Batch Number:  338  /  1875    Autoencoder loss:  0.2381659746170044\n",
      "Epoch:  0    Batch Number:  339  /  1875    Autoencoder loss:  0.21953684091567993\n",
      "Epoch:  0    Batch Number:  340  /  1875    Autoencoder loss:  0.24430279433727264\n",
      "Epoch:  0    Batch Number:  341  /  1875    Autoencoder loss:  0.2169535905122757\n",
      "Epoch:  0    Batch Number:  342  /  1875    Autoencoder loss:  0.24218742549419403\n",
      "Epoch:  0    Batch Number:  343  /  1875    Autoencoder loss:  0.23716311156749725\n",
      "Epoch:  0    Batch Number:  344  /  1875    Autoencoder loss:  0.22906315326690674\n",
      "Epoch:  0    Batch Number:  345  /  1875    Autoencoder loss:  0.22681325674057007\n",
      "Epoch:  0    Batch Number:  346  /  1875    Autoencoder loss:  0.2581997811794281\n",
      "Epoch:  0    Batch Number:  347  /  1875    Autoencoder loss:  0.23211868107318878\n",
      "Epoch:  0    Batch Number:  348  /  1875    Autoencoder loss:  0.25333160161972046\n",
      "Epoch:  0    Batch Number:  349  /  1875    Autoencoder loss:  0.24305951595306396\n",
      "Epoch:  0    Batch Number:  350  /  1875    Autoencoder loss:  0.2408362478017807\n",
      "Epoch:  0    Batch Number:  351  /  1875    Autoencoder loss:  0.231368750333786\n",
      "Epoch:  0    Batch Number:  352  /  1875    Autoencoder loss:  0.22549138963222504\n",
      "Epoch:  0    Batch Number:  353  /  1875    Autoencoder loss:  0.22964586317539215\n",
      "Epoch:  0    Batch Number:  354  /  1875    Autoencoder loss:  0.22757187485694885\n",
      "Epoch:  0    Batch Number:  355  /  1875    Autoencoder loss:  0.2384614199399948\n",
      "Epoch:  0    Batch Number:  356  /  1875    Autoencoder loss:  0.22988244891166687\n",
      "Epoch:  0    Batch Number:  357  /  1875    Autoencoder loss:  0.24060478806495667\n",
      "Epoch:  0    Batch Number:  358  /  1875    Autoencoder loss:  0.2096010446548462\n",
      "Epoch:  0    Batch Number:  359  /  1875    Autoencoder loss:  0.20478256046772003\n",
      "Epoch:  0    Batch Number:  360  /  1875    Autoencoder loss:  0.22955721616744995\n",
      "Epoch:  0    Batch Number:  361  /  1875    Autoencoder loss:  0.2379952073097229\n",
      "Epoch:  0    Batch Number:  362  /  1875    Autoencoder loss:  0.23392681777477264\n",
      "Epoch:  0    Batch Number:  363  /  1875    Autoencoder loss:  0.2104984074831009\n",
      "Epoch:  0    Batch Number:  364  /  1875    Autoencoder loss:  0.2280551791191101\n",
      "Epoch:  0    Batch Number:  365  /  1875    Autoencoder loss:  0.21781139075756073\n",
      "Epoch:  0    Batch Number:  366  /  1875    Autoencoder loss:  0.2513202428817749\n",
      "Epoch:  0    Batch Number:  367  /  1875    Autoencoder loss:  0.24467724561691284\n",
      "Epoch:  0    Batch Number:  368  /  1875    Autoencoder loss:  0.21155515313148499\n",
      "Epoch:  0    Batch Number:  369  /  1875    Autoencoder loss:  0.23158708214759827\n",
      "Epoch:  0    Batch Number:  370  /  1875    Autoencoder loss:  0.23890073597431183\n",
      "Epoch:  0    Batch Number:  371  /  1875    Autoencoder loss:  0.23198866844177246\n",
      "Epoch: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x000002036E609C18>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\boldi.DESKTOP-774BUAF\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 538, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"C:\\Users\\boldi.DESKTOP-774BUAF\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1139, in delete_iterator\n",
      "    tld.op_callbacks, handle, deleter)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    Batch Number:  372  /  1875    Autoencoder loss:  0.22598132491111755\n",
      "Epoch:  0    Batch Number:  373  /  1875    Autoencoder loss:  0.24574173986911774\n",
      "Epoch:  0    Batch Number:  374  /  1875    Autoencoder loss:  0.23504382371902466\n",
      "Epoch:  0    Batch Number:  375  /  1875    Autoencoder loss:  0.22175687551498413\n",
      "Epoch:  0    Batch Number:  376  /  1875    Autoencoder loss:  0.22719846665859222\n",
      "Epoch:  0    Batch Number:  377  /  1875    Autoencoder loss:  0.24852487444877625\n",
      "Epoch:  0    Batch Number:  378  /  1875    Autoencoder loss:  0.23264990746974945\n",
      "Epoch:  0    Batch Number:  379  /  1875    Autoencoder loss:  0.2425108104944229\n",
      "Epoch:  0    Batch Number:  380  /  1875    Autoencoder loss:  0.23402224481105804\n",
      "Epoch:  0    Batch Number:  381  /  1875    Autoencoder loss:  0.2155829221010208\n",
      "Epoch:  0    Batch Number:  382  /  1875    Autoencoder loss:  0.22894546389579773\n",
      "Epoch:  0    Batch Number:  383  /  1875    Autoencoder loss:  0.2429477572441101\n",
      "Epoch:  0    Batch Number:  384  /  1875    Autoencoder loss:  0.230552077293396\n",
      "Epoch:  0    Batch Number:  385  /  1875    Autoencoder loss:  0.23696261644363403\n",
      "Epoch:  0    Batch Number:  386  /  1875    Autoencoder loss:  0.23489505052566528\n",
      "Epoch:  0    Batch Number:  387  /  1875    Autoencoder loss:  0.23558832705020905\n",
      "Epoch:  0    Batch Number:  388  /  1875    Autoencoder loss:  0.2283792942762375\n",
      "Epoch:  0    Batch Number:  389  /  1875    Autoencoder loss:  0.21952667832374573\n",
      "Epoch:  0    Batch Number:  390  /  1875    Autoencoder loss:  0.22845973074436188\n",
      "Epoch:  0    Batch Number:  391  /  1875    Autoencoder loss:  0.2211964726448059\n",
      "Epoch:  0    Batch Number:  392  /  1875    Autoencoder loss:  0.22347013652324677\n",
      "Epoch:  0    Batch Number:  393  /  1875    Autoencoder loss:  0.2295728474855423\n",
      "Epoch:  0    Batch Number:  394  /  1875    Autoencoder loss:  0.2301878184080124\n",
      "Epoch:  0    Batch Number:  395  /  1875    Autoencoder loss:  0.2444065809249878\n",
      "Epoch:  0    Batch Number:  396  /  1875    Autoencoder loss:  0.23221352696418762\n",
      "Epoch:  0    Batch Number:  397  /  1875    Autoencoder loss:  0.2297176569700241\n",
      "Epoch:  0    Batch Number:  398  /  1875    Autoencoder loss:  0.2302737683057785\n",
      "Epoch:  0    Batch Number:  399  /  1875    Autoencoder loss:  0.2157750278711319\n",
      "Epoch:  0    Batch Number:  400  /  1875    Autoencoder loss:  0.23399265110492706\n",
      "Epoch:  0    Batch Number:  401  /  1875    Autoencoder loss:  0.22021961212158203\n",
      "Epoch:  0    Batch Number:  402  /  1875    Autoencoder loss:  0.22677180171012878\n",
      "Epoch:  0    Batch Number:  403  /  1875    Autoencoder loss:  0.2228136658668518\n",
      "Epoch:  0    Batch Number:  404  /  1875    Autoencoder loss:  0.24288812279701233\n",
      "Epoch:  0    Batch Number:  405  /  1875    Autoencoder loss:  0.21863485872745514\n",
      "Epoch:  0    Batch Number:  406  /  1875    Autoencoder loss:  0.20225746929645538\n",
      "Epoch:  0    Batch Number:  407  /  1875    Autoencoder loss:  0.22930392622947693\n",
      "Epoch:  0    Batch Number:  408  /  1875    Autoencoder loss:  0.23401586711406708\n",
      "Epoch:  0    Batch Number:  409  /  1875    Autoencoder loss:  0.23806171119213104\n",
      "Epoch:  0    Batch Number:  410  /  1875    Autoencoder loss:  0.21502932906150818\n",
      "Epoch:  0    Batch Number:  411  /  1875    Autoencoder loss:  0.2341345250606537\n",
      "Epoch:  0    Batch Number:  412  /  1875    Autoencoder loss:  0.21981753408908844\n",
      "Epoch:  0    Batch Number:  413  /  1875    Autoencoder loss:  0.223175048828125\n",
      "Epoch:  0    Batch Number:  414  /  1875    Autoencoder loss:  0.24877133965492249\n",
      "Epoch:  0    Batch Number:  415  /  1875    Autoencoder loss:  0.2165190577507019\n",
      "Epoch:  0    Batch Number:  416  /  1875    Autoencoder loss:  0.22545675933361053\n",
      "Epoch:  0    Batch Number:  417  /  1875    Autoencoder loss:  0.22810496389865875\n",
      "Epoch:  0    Batch Number:  418  /  1875    Autoencoder loss:  0.23642729222774506\n",
      "Epoch:  0    Batch Number:  419  /  1875    Autoencoder loss:  0.21053504943847656\n",
      "Epoch:  0    Batch Number:  420  /  1875    Autoencoder loss:  0.22989128530025482\n",
      "Epoch:  0    Batch Number:  421  /  1875    Autoencoder loss:  0.20579423010349274\n",
      "Epoch:  0    Batch Number:  422  /  1875    Autoencoder loss:  0.22219452261924744\n",
      "Epoch:  0    Batch Number:  423  /  1875    Autoencoder loss:  0.20204423367977142\n",
      "Epoch:  0    Batch Number:  424  /  1875    Autoencoder loss:  0.22164510190486908\n",
      "Epoch:  0    Batch Number:  425  /  1875    Autoencoder loss:  0.24349188804626465\n",
      "Epoch:  0    Batch Number:  426  /  1875    Autoencoder loss:  0.24388480186462402\n",
      "Epoch:  0    Batch Number:  427  /  1875    Autoencoder loss:  0.23486363887786865\n",
      "Epoch:  0    Batch Number:  428  /  1875    Autoencoder loss:  0.23485004901885986\n",
      "Epoch:  0    Batch Number:  429  /  1875    Autoencoder loss:  0.24016012251377106\n",
      "Epoch:  0    Batch Number:  430  /  1875    Autoencoder loss:  0.2409399449825287\n",
      "Epoch:  0    Batch Number:  431  /  1875    Autoencoder loss:  0.21794283390045166\n",
      "Epoch:  0    Batch Number:  432  /  1875    Autoencoder loss:  0.21438802778720856\n",
      "Epoch:  0    Batch Number:  433  /  1875    Autoencoder loss:  0.23136913776397705\n",
      "Epoch:  0    Batch Number:  434  /  1875    Autoencoder loss:  0.2311733365058899\n",
      "Epoch:  0    Batch Number:  435  /  1875    Autoencoder loss:  0.22684521973133087\n",
      "Epoch:  0    Batch Number:  436  /  1875    Autoencoder loss:  0.21050196886062622\n",
      "Epoch:  0    Batch Number:  437  /  1875    Autoencoder loss:  0.23666147887706757\n",
      "Epoch:  0    Batch Number:  438  /  1875    Autoencoder loss:  0.22089216113090515\n",
      "Epoch:  0    Batch Number:  439  /  1875    Autoencoder loss:  0.21118782460689545\n",
      "Epoch:  0    Batch Number:  440  /  1875    Autoencoder loss:  0.21776004135608673\n",
      "Epoch:  0    Batch Number:  441  /  1875    Autoencoder loss:  0.22540314495563507\n",
      "Epoch:  0    Batch Number:  442  /  1875    Autoencoder loss:  0.19933731853961945\n",
      "Epoch:  0    Batch Number:  443  /  1875    Autoencoder loss:  0.2414705455303192\n",
      "Epoch:  0    Batch Number:  444  /  1875    Autoencoder loss:  0.2343374490737915\n",
      "Epoch:  0    Batch Number:  445  /  1875    Autoencoder loss:  0.22483284771442413\n",
      "Epoch:  0    Batch Number:  446  /  1875    Autoencoder loss:  0.2281937152147293\n",
      "Epoch:  0    Batch Number:  447  /  1875    Autoencoder loss:  0.21896040439605713\n",
      "Epoch:  0    Batch Number:  448  /  1875    Autoencoder loss:  0.22162850201129913\n",
      "Epoch:  0    Batch Number:  449  /  1875    Autoencoder loss:  0.22964835166931152\n",
      "Epoch:  0    Batch Number:  450  /  1875    Autoencoder loss:  0.22949343919754028\n",
      "Epoch:  0    Batch Number:  451  /  1875    Autoencoder loss:  0.20860396325588226\n",
      "Epoch:  0    Batch Number:  452  /  1875    Autoencoder loss:  0.22895634174346924\n",
      "Epoch:  0    Batch Number:  453  /  1875    Autoencoder loss:  0.21443912386894226\n",
      "Epoch:  0    Batch Number:  454  /  1875    Autoencoder loss:  0.24024485051631927\n",
      "Epoch:  0    Batch Number:  455  /  1875    Autoencoder loss:  0.23493213951587677\n",
      "Epoch:  0    Batch Number:  456  /  1875    Autoencoder loss:  0.21431095898151398\n",
      "Epoch:  0    Batch Number:  457  /  1875    Autoencoder loss:  0.22923220694065094\n",
      "Epoch:  0    Batch Number:  458  /  1875    Autoencoder loss:  0.24522800743579865\n",
      "Epoch:  0    Batch Number:  459  /  1875    Autoencoder loss:  0.21969479322433472\n",
      "Epoch:  0    Batch Number:  460  /  1875    Autoencoder loss:  0.20563291013240814\n",
      "Epoch:  0    Batch Number:  461  /  1875    Autoencoder loss:  0.22554080188274384\n",
      "Epoch:  0    Batch Number:  462  /  1875    Autoencoder loss:  0.23020946979522705\n",
      "Epoch:  0    Batch Number:  463  /  1875    Autoencoder loss:  0.2190699279308319\n",
      "Epoch:  0    Batch Number:  464  /  1875    Autoencoder loss:  0.2293633073568344\n",
      "Epoch:  0    Batch Number:  465  /  1875    Autoencoder loss:  0.2212793380022049\n",
      "Epoch:  0    Batch Number:  466  /  1875    Autoencoder loss:  0.20768432319164276\n",
      "Epoch:  0    Batch Number:  467  /  1875    Autoencoder loss:  0.22326436638832092\n",
      "Epoch:  0    Batch Number:  468  /  1875    Autoencoder loss:  0.23517954349517822\n",
      "Epoch:  0    Batch Number:  469  /  1875    Autoencoder loss:  0.20301871001720428\n",
      "Epoch:  0    Batch Number:  470  /  1875    Autoencoder loss:  0.20402947068214417\n",
      "Epoch:  0    Batch Number:  471  /  1875    Autoencoder loss:  0.20974458754062653\n",
      "Epoch:  0    Batch Number:  472  /  1875    Autoencoder loss:  0.21547801792621613\n",
      "Epoch:  0    Batch Number:  473  /  1875    Autoencoder loss:  0.23896069824695587\n",
      "Epoch:  0    Batch Number:  474  /  1875    Autoencoder loss:  0.22482362389564514\n",
      "Epoch:  0    Batch Number:  475  /  1875    Autoencoder loss:  0.21416325867176056\n",
      "Epoch:  0    Batch Number:  476  /  1875    Autoencoder loss:  0.21749088168144226\n",
      "Epoch:  0    Batch Number:  477  /  1875    Autoencoder loss:  0.20842786133289337\n",
      "Epoch:  0    Batch Number:  478  /  1875    Autoencoder loss:  0.2186659574508667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  479  /  1875    Autoencoder loss:  0.22779342532157898\n",
      "Epoch:  0    Batch Number:  480  /  1875    Autoencoder loss:  0.24181720614433289\n",
      "Epoch:  0    Batch Number:  481  /  1875    Autoencoder loss:  0.22609961032867432\n",
      "Epoch:  0    Batch Number:  482  /  1875    Autoencoder loss:  0.2138580083847046\n",
      "Epoch:  0    Batch Number:  483  /  1875    Autoencoder loss:  0.20856598019599915\n",
      "Epoch:  0    Batch Number:  484  /  1875    Autoencoder loss:  0.22522613406181335\n",
      "Epoch:  0    Batch Number:  485  /  1875    Autoencoder loss:  0.21920503675937653\n",
      "Epoch:  0    Batch Number:  486  /  1875    Autoencoder loss:  0.2294992357492447\n",
      "Epoch:  0    Batch Number:  487  /  1875    Autoencoder loss:  0.21309195458889008\n",
      "Epoch:  0    Batch Number:  488  /  1875    Autoencoder loss:  0.21713483333587646\n",
      "Epoch:  0    Batch Number:  489  /  1875    Autoencoder loss:  0.21039840579032898\n",
      "Epoch:  0    Batch Number:  490  /  1875    Autoencoder loss:  0.2354695051908493\n",
      "Epoch:  0    Batch Number:  491  /  1875    Autoencoder loss:  0.20278732478618622\n",
      "Epoch:  0    Batch Number:  492  /  1875    Autoencoder loss:  0.21557453274726868\n",
      "Epoch:  0    Batch Number:  493  /  1875    Autoencoder loss:  0.2321256548166275\n",
      "Epoch:  0    Batch Number:  494  /  1875    Autoencoder loss:  0.24049197137355804\n",
      "Epoch:  0    Batch Number:  495  /  1875    Autoencoder loss:  0.21553535759449005\n",
      "Epoch:  0    Batch Number:  496  /  1875    Autoencoder loss:  0.2364874929189682\n",
      "Epoch:  0    Batch Number:  497  /  1875    Autoencoder loss:  0.22495560348033905\n",
      "Epoch:  0    Batch Number:  498  /  1875    Autoencoder loss:  0.2229633331298828\n",
      "Epoch:  0    Batch Number:  499  /  1875    Autoencoder loss:  0.22664320468902588\n",
      "Epoch:  0    Batch Number:  500  /  1875    Autoencoder loss:  0.23395785689353943\n",
      "Epoch:  0    Batch Number:  501  /  1875    Autoencoder loss:  0.2186039537191391\n",
      "Epoch:  0    Batch Number:  502  /  1875    Autoencoder loss:  0.21274632215499878\n",
      "Epoch:  0    Batch Number:  503  /  1875    Autoencoder loss:  0.23893652856349945\n",
      "Epoch:  0    Batch Number:  504  /  1875    Autoencoder loss:  0.2200615108013153\n",
      "Epoch:  0    Batch Number:  505  /  1875    Autoencoder loss:  0.2210981845855713\n",
      "Epoch:  0    Batch Number:  506  /  1875    Autoencoder loss:  0.21990254521369934\n",
      "Epoch:  0    Batch Number:  507  /  1875    Autoencoder loss:  0.22161945700645447\n",
      "Epoch:  0    Batch Number:  508  /  1875    Autoencoder loss:  0.20459987223148346\n",
      "Epoch:  0    Batch Number:  509  /  1875    Autoencoder loss:  0.1999840885400772\n",
      "Epoch:  0    Batch Number:  510  /  1875    Autoencoder loss:  0.21806466579437256\n",
      "Epoch:  0    Batch Number:  511  /  1875    Autoencoder loss:  0.24256744980812073\n",
      "Epoch:  0    Batch Number:  512  /  1875    Autoencoder loss:  0.20888569951057434\n",
      "Epoch:  0    Batch Number:  513  /  1875    Autoencoder loss:  0.21408388018608093\n",
      "Epoch:  0    Batch Number:  514  /  1875    Autoencoder loss:  0.21631471812725067\n",
      "Epoch:  0    Batch Number:  515  /  1875    Autoencoder loss:  0.1918906420469284\n",
      "Epoch:  0    Batch Number:  516  /  1875    Autoencoder loss:  0.21699035167694092\n",
      "Epoch:  0    Batch Number:  517  /  1875    Autoencoder loss:  0.22003547847270966\n",
      "Epoch:  0    Batch Number:  518  /  1875    Autoencoder loss:  0.22725942730903625\n",
      "Epoch:  0    Batch Number:  519  /  1875    Autoencoder loss:  0.22453491389751434\n",
      "Epoch:  0    Batch Number:  520  /  1875    Autoencoder loss:  0.21264559030532837\n",
      "Epoch:  0    Batch Number:  521  /  1875    Autoencoder loss:  0.22673070430755615\n",
      "Epoch:  0    Batch Number:  522  /  1875    Autoencoder loss:  0.21378719806671143\n",
      "Epoch:  0    Batch Number:  523  /  1875    Autoencoder loss:  0.20795166492462158\n",
      "Epoch:  0    Batch Number:  524  /  1875    Autoencoder loss:  0.23327527940273285\n",
      "Epoch:  0    Batch Number:  525  /  1875    Autoencoder loss:  0.2079579085111618\n",
      "Epoch:  0    Batch Number:  526  /  1875    Autoencoder loss:  0.23064321279525757\n",
      "Epoch:  0    Batch Number:  527  /  1875    Autoencoder loss:  0.21669107675552368\n",
      "Epoch:  0    Batch Number:  528  /  1875    Autoencoder loss:  0.2147737592458725\n",
      "Epoch:  0    Batch Number:  529  /  1875    Autoencoder loss:  0.19310419261455536\n",
      "Epoch:  0    Batch Number:  530  /  1875    Autoencoder loss:  0.23157858848571777\n",
      "Epoch:  0    Batch Number:  531  /  1875    Autoencoder loss:  0.2154044359922409\n",
      "Epoch:  0    Batch Number:  532  /  1875    Autoencoder loss:  0.20417094230651855\n",
      "Epoch:  0    Batch Number:  533  /  1875    Autoencoder loss:  0.21200008690357208\n",
      "Epoch:  0    Batch Number:  534  /  1875    Autoencoder loss:  0.2290266752243042\n",
      "Epoch:  0    Batch Number:  535  /  1875    Autoencoder loss:  0.2281256765127182\n",
      "Epoch:  0    Batch Number:  536  /  1875    Autoencoder loss:  0.22196024656295776\n",
      "Epoch:  0    Batch Number:  537  /  1875    Autoencoder loss:  0.23914268612861633\n",
      "Epoch:  0    Batch Number:  538  /  1875    Autoencoder loss:  0.22568103671073914\n",
      "Epoch:  0    Batch Number:  539  /  1875    Autoencoder loss:  0.2198895663022995\n",
      "Epoch:  0    Batch Number:  540  /  1875    Autoencoder loss:  0.22405263781547546\n",
      "Epoch:  0    Batch Number:  541  /  1875    Autoencoder loss:  0.2268349975347519\n",
      "Epoch:  0    Batch Number:  542  /  1875    Autoencoder loss:  0.229165717959404\n",
      "Epoch:  0    Batch Number:  543  /  1875    Autoencoder loss:  0.20751895010471344\n",
      "Epoch:  0    Batch Number:  544  /  1875    Autoencoder loss:  0.21831302344799042\n",
      "Epoch:  0    Batch Number:  545  /  1875    Autoencoder loss:  0.23670801520347595\n",
      "Epoch:  0    Batch Number:  546  /  1875    Autoencoder loss:  0.22357508540153503\n",
      "Epoch:  0    Batch Number:  547  /  1875    Autoencoder loss:  0.2209588587284088\n",
      "Epoch:  0    Batch Number:  548  /  1875    Autoencoder loss:  0.22286254167556763\n",
      "Epoch:  0    Batch Number:  549  /  1875    Autoencoder loss:  0.22381548583507538\n",
      "Epoch:  0    Batch Number:  550  /  1875    Autoencoder loss:  0.213591530919075\n",
      "Epoch:  0    Batch Number:  551  /  1875    Autoencoder loss:  0.20878270268440247\n",
      "Epoch:  0    Batch Number:  552  /  1875    Autoencoder loss:  0.20683424174785614\n",
      "Epoch:  0    Batch Number:  553  /  1875    Autoencoder loss:  0.2220795750617981\n",
      "Epoch:  0    Batch Number:  554  /  1875    Autoencoder loss:  0.21703992784023285\n",
      "Epoch:  0    Batch Number:  555  /  1875    Autoencoder loss:  0.22090385854244232\n",
      "Epoch:  0    Batch Number:  556  /  1875    Autoencoder loss:  0.20330467820167542\n",
      "Epoch:  0    Batch Number:  557  /  1875    Autoencoder loss:  0.20747555792331696\n",
      "Epoch:  0    Batch Number:  558  /  1875    Autoencoder loss:  0.2124537229537964\n",
      "Epoch:  0    Batch Number:  559  /  1875    Autoencoder loss:  0.22082336246967316\n",
      "Epoch:  0    Batch Number:  560  /  1875    Autoencoder loss:  0.21457216143608093\n",
      "Epoch:  0    Batch Number:  561  /  1875    Autoencoder loss:  0.2212919294834137\n",
      "Epoch:  0    Batch Number:  562  /  1875    Autoencoder loss:  0.2263980358839035\n",
      "Epoch:  0    Batch Number:  563  /  1875    Autoencoder loss:  0.23337388038635254\n",
      "Epoch:  0    Batch Number:  564  /  1875    Autoencoder loss:  0.22480405867099762\n",
      "Epoch:  0    Batch Number:  565  /  1875    Autoencoder loss:  0.20337608456611633\n",
      "Epoch:  0    Batch Number:  566  /  1875    Autoencoder loss:  0.20910309255123138\n",
      "Epoch:  0    Batch Number:  567  /  1875    Autoencoder loss:  0.21322740614414215\n",
      "Epoch:  0    Batch Number:  568  /  1875    Autoencoder loss:  0.23156742751598358\n",
      "Epoch:  0    Batch Number:  569  /  1875    Autoencoder loss:  0.2162771075963974\n",
      "Epoch:  0    Batch Number:  570  /  1875    Autoencoder loss:  0.2337844967842102\n",
      "Epoch:  0    Batch Number:  571  /  1875    Autoencoder loss:  0.23102624714374542\n",
      "Epoch:  0    Batch Number:  572  /  1875    Autoencoder loss:  0.21511918306350708\n",
      "Epoch:  0    Batch Number:  573  /  1875    Autoencoder loss:  0.2274472713470459\n",
      "Epoch:  0    Batch Number:  574  /  1875    Autoencoder loss:  0.2489081472158432\n",
      "Epoch:  0    Batch Number:  575  /  1875    Autoencoder loss:  0.216243177652359\n",
      "Epoch:  0    Batch Number:  576  /  1875    Autoencoder loss:  0.21776188910007477\n",
      "Epoch:  0    Batch Number:  577  /  1875    Autoencoder loss:  0.2102455049753189\n",
      "Epoch:  0    Batch Number:  578  /  1875    Autoencoder loss:  0.19479143619537354\n",
      "Epoch:  0    Batch Number:  579  /  1875    Autoencoder loss:  0.2315368503332138\n",
      "Epoch:  0    Batch Number:  580  /  1875    Autoencoder loss:  0.1914682537317276\n",
      "Epoch:  0    Batch Number:  581  /  1875    Autoencoder loss:  0.2223202884197235\n",
      "Epoch:  0    Batch Number:  582  /  1875    Autoencoder loss:  0.2210153341293335\n",
      "Epoch:  0    Batch Number:  583  /  1875    Autoencoder loss:  0.19834856688976288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  584  /  1875    Autoencoder loss:  0.21574532985687256\n",
      "Epoch:  0    Batch Number:  585  /  1875    Autoencoder loss:  0.21608169376850128\n",
      "Epoch:  0    Batch Number:  586  /  1875    Autoencoder loss:  0.20911967754364014\n",
      "Epoch:  0    Batch Number:  587  /  1875    Autoencoder loss:  0.21771372854709625\n",
      "Epoch:  0    Batch Number:  588  /  1875    Autoencoder loss:  0.23315297067165375\n",
      "Epoch:  0    Batch Number:  589  /  1875    Autoencoder loss:  0.2021784484386444\n",
      "Epoch:  0    Batch Number:  590  /  1875    Autoencoder loss:  0.2232808917760849\n",
      "Epoch:  0    Batch Number:  591  /  1875    Autoencoder loss:  0.2073468118906021\n",
      "Epoch:  0    Batch Number:  592  /  1875    Autoencoder loss:  0.23211351037025452\n",
      "Epoch:  0    Batch Number:  593  /  1875    Autoencoder loss:  0.20106692612171173\n",
      "Epoch:  0    Batch Number:  594  /  1875    Autoencoder loss:  0.20515243709087372\n",
      "Epoch:  0    Batch Number:  595  /  1875    Autoencoder loss:  0.19919472932815552\n",
      "Epoch:  0    Batch Number:  596  /  1875    Autoencoder loss:  0.22397637367248535\n",
      "Epoch:  0    Batch Number:  597  /  1875    Autoencoder loss:  0.22353413701057434\n",
      "Epoch:  0    Batch Number:  598  /  1875    Autoencoder loss:  0.20600438117980957\n",
      "Epoch:  0    Batch Number:  599  /  1875    Autoencoder loss:  0.22755739092826843\n",
      "Epoch:  0    Batch Number:  600  /  1875    Autoencoder loss:  0.23175811767578125\n",
      "Epoch:  0    Batch Number:  601  /  1875    Autoencoder loss:  0.2002747803926468\n",
      "Epoch:  0    Batch Number:  602  /  1875    Autoencoder loss:  0.22589391469955444\n",
      "Epoch:  0    Batch Number:  603  /  1875    Autoencoder loss:  0.21497243642807007\n",
      "Epoch:  0    Batch Number:  604  /  1875    Autoencoder loss:  0.21910737454891205\n",
      "Epoch:  0    Batch Number:  605  /  1875    Autoencoder loss:  0.2147437185049057\n",
      "Epoch:  0    Batch Number:  606  /  1875    Autoencoder loss:  0.21659328043460846\n",
      "Epoch:  0    Batch Number:  607  /  1875    Autoencoder loss:  0.2203761488199234\n",
      "Epoch:  0    Batch Number:  608  /  1875    Autoencoder loss:  0.21528205275535583\n",
      "Epoch:  0    Batch Number:  609  /  1875    Autoencoder loss:  0.2194821536540985\n",
      "Epoch:  0    Batch Number:  610  /  1875    Autoencoder loss:  0.21585078537464142\n",
      "Epoch:  0    Batch Number:  611  /  1875    Autoencoder loss:  0.210746631026268\n",
      "Epoch:  0    Batch Number:  612  /  1875    Autoencoder loss:  0.20768363773822784\n",
      "Epoch:  0    Batch Number:  613  /  1875    Autoencoder loss:  0.22324296832084656\n",
      "Epoch:  0    Batch Number:  614  /  1875    Autoencoder loss:  0.2243622988462448\n",
      "Epoch:  0    Batch Number:  615  /  1875    Autoencoder loss:  0.22436079382896423\n",
      "Epoch:  0    Batch Number:  616  /  1875    Autoencoder loss:  0.21245770156383514\n",
      "Epoch:  0    Batch Number:  617  /  1875    Autoencoder loss:  0.19958725571632385\n",
      "Epoch:  0    Batch Number:  618  /  1875    Autoencoder loss:  0.20902620255947113\n",
      "Epoch:  0    Batch Number:  619  /  1875    Autoencoder loss:  0.20880572497844696\n",
      "Epoch:  0    Batch Number:  620  /  1875    Autoencoder loss:  0.20039869844913483\n",
      "Epoch:  0    Batch Number:  621  /  1875    Autoencoder loss:  0.1875942051410675\n",
      "Epoch:  0    Batch Number:  622  /  1875    Autoencoder loss:  0.2236471176147461\n",
      "Epoch:  0    Batch Number:  623  /  1875    Autoencoder loss:  0.23102907836437225\n",
      "Epoch:  0    Batch Number:  624  /  1875    Autoencoder loss:  0.20168761909008026\n",
      "Epoch:  0    Batch Number:  625  /  1875    Autoencoder loss:  0.21200428903102875\n",
      "Epoch:  0    Batch Number:  626  /  1875    Autoencoder loss:  0.21166223287582397\n",
      "Epoch:  0    Batch Number:  627  /  1875    Autoencoder loss:  0.2155662477016449\n",
      "Epoch:  0    Batch Number:  628  /  1875    Autoencoder loss:  0.2256912738084793\n",
      "Epoch:  0    Batch Number:  629  /  1875    Autoencoder loss:  0.234392911195755\n",
      "Epoch:  0    Batch Number:  630  /  1875    Autoencoder loss:  0.22668468952178955\n",
      "Epoch:  0    Batch Number:  631  /  1875    Autoencoder loss:  0.20645171403884888\n",
      "Epoch:  0    Batch Number:  632  /  1875    Autoencoder loss:  0.20709407329559326\n",
      "Epoch:  0    Batch Number:  633  /  1875    Autoencoder loss:  0.21671992540359497\n",
      "Epoch:  0    Batch Number:  634  /  1875    Autoencoder loss:  0.20393694937229156\n",
      "Epoch:  0    Batch Number:  635  /  1875    Autoencoder loss:  0.2302233874797821\n",
      "Epoch:  0    Batch Number:  636  /  1875    Autoencoder loss:  0.21051788330078125\n",
      "Epoch:  0    Batch Number:  637  /  1875    Autoencoder loss:  0.22553957998752594\n",
      "Epoch:  0    Batch Number:  638  /  1875    Autoencoder loss:  0.22485816478729248\n",
      "Epoch:  0    Batch Number:  639  /  1875    Autoencoder loss:  0.20796748995780945\n",
      "Epoch:  0    Batch Number:  640  /  1875    Autoencoder loss:  0.2197822779417038\n",
      "Epoch:  0    Batch Number:  641  /  1875    Autoencoder loss:  0.21778608858585358\n",
      "Epoch:  0    Batch Number:  642  /  1875    Autoencoder loss:  0.19395816326141357\n",
      "Epoch:  0    Batch Number:  643  /  1875    Autoencoder loss:  0.193228617310524\n",
      "Epoch:  0    Batch Number:  644  /  1875    Autoencoder loss:  0.22253890335559845\n",
      "Epoch:  0    Batch Number:  645  /  1875    Autoencoder loss:  0.22162924706935883\n",
      "Epoch:  0    Batch Number:  646  /  1875    Autoencoder loss:  0.21669749915599823\n",
      "Epoch:  0    Batch Number:  647  /  1875    Autoencoder loss:  0.2334946244955063\n",
      "Epoch:  0    Batch Number:  648  /  1875    Autoencoder loss:  0.21535339951515198\n",
      "Epoch:  0    Batch Number:  649  /  1875    Autoencoder loss:  0.21231640875339508\n",
      "Epoch:  0    Batch Number:  650  /  1875    Autoencoder loss:  0.21169915795326233\n",
      "Epoch:  0    Batch Number:  651  /  1875    Autoencoder loss:  0.2053074687719345\n",
      "Epoch:  0    Batch Number:  652  /  1875    Autoencoder loss:  0.22012905776500702\n",
      "Epoch:  0    Batch Number:  653  /  1875    Autoencoder loss:  0.21115843951702118\n",
      "Epoch:  0    Batch Number:  654  /  1875    Autoencoder loss:  0.18939998745918274\n",
      "Epoch:  0    Batch Number:  655  /  1875    Autoencoder loss:  0.22385643422603607\n",
      "Epoch:  0    Batch Number:  656  /  1875    Autoencoder loss:  0.22216038405895233\n",
      "Epoch:  0    Batch Number:  657  /  1875    Autoencoder loss:  0.21286068856716156\n",
      "Epoch:  0    Batch Number:  658  /  1875    Autoencoder loss:  0.21573513746261597\n",
      "Epoch:  0    Batch Number:  659  /  1875    Autoencoder loss:  0.19797466695308685\n",
      "Epoch:  0    Batch Number:  660  /  1875    Autoencoder loss:  0.206838458776474\n",
      "Epoch:  0    Batch Number:  661  /  1875    Autoencoder loss:  0.20340976119041443\n",
      "Epoch:  0    Batch Number:  662  /  1875    Autoencoder loss:  0.21301861107349396\n",
      "Epoch:  0    Batch Number:  663  /  1875    Autoencoder loss:  0.21450629830360413\n",
      "Epoch:  0    Batch Number:  664  /  1875    Autoencoder loss:  0.2042657434940338\n",
      "Epoch:  0    Batch Number:  665  /  1875    Autoencoder loss:  0.20551981031894684\n",
      "Epoch:  0    Batch Number:  666  /  1875    Autoencoder loss:  0.21855050325393677\n",
      "Epoch:  0    Batch Number:  667  /  1875    Autoencoder loss:  0.2075808346271515\n",
      "Epoch:  0    Batch Number:  668  /  1875    Autoencoder loss:  0.2027939260005951\n",
      "Epoch:  0    Batch Number:  669  /  1875    Autoencoder loss:  0.21810275316238403\n",
      "Epoch:  0    Batch Number:  670  /  1875    Autoencoder loss:  0.2111605554819107\n",
      "Epoch:  0    Batch Number:  671  /  1875    Autoencoder loss:  0.21017034351825714\n",
      "Epoch:  0    Batch Number:  672  /  1875    Autoencoder loss:  0.21719683706760406\n",
      "Epoch:  0    Batch Number:  673  /  1875    Autoencoder loss:  0.2036910206079483\n",
      "Epoch:  0    Batch Number:  674  /  1875    Autoencoder loss:  0.19397735595703125\n",
      "Epoch:  0    Batch Number:  675  /  1875    Autoencoder loss:  0.19309744238853455\n",
      "Epoch:  0    Batch Number:  676  /  1875    Autoencoder loss:  0.20044004917144775\n",
      "Epoch:  0    Batch Number:  677  /  1875    Autoencoder loss:  0.2236352264881134\n",
      "Epoch:  0    Batch Number:  678  /  1875    Autoencoder loss:  0.22428447008132935\n",
      "Epoch:  0    Batch Number:  679  /  1875    Autoencoder loss:  0.21549174189567566\n",
      "Epoch:  0    Batch Number:  680  /  1875    Autoencoder loss:  0.2304646223783493\n",
      "Epoch:  0    Batch Number:  681  /  1875    Autoencoder loss:  0.19371134042739868\n",
      "Epoch:  0    Batch Number:  682  /  1875    Autoencoder loss:  0.19923584163188934\n",
      "Epoch:  0    Batch Number:  683  /  1875    Autoencoder loss:  0.20458705723285675\n",
      "Epoch:  0    Batch Number:  684  /  1875    Autoencoder loss:  0.2108469158411026\n",
      "Epoch:  0    Batch Number:  685  /  1875    Autoencoder loss:  0.21446652710437775\n",
      "Epoch:  0    Batch Number:  686  /  1875    Autoencoder loss:  0.20974311232566833\n",
      "Epoch:  0    Batch Number:  687  /  1875    Autoencoder loss:  0.22985532879829407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  688  /  1875    Autoencoder loss:  0.21334870159626007\n",
      "Epoch:  0    Batch Number:  689  /  1875    Autoencoder loss:  0.2039274424314499\n",
      "Epoch:  0    Batch Number:  690  /  1875    Autoencoder loss:  0.2399372160434723\n",
      "Epoch:  0    Batch Number:  691  /  1875    Autoencoder loss:  0.2275402694940567\n",
      "Epoch:  0    Batch Number:  692  /  1875    Autoencoder loss:  0.2238852083683014\n",
      "Epoch:  0    Batch Number:  693  /  1875    Autoencoder loss:  0.2082720845937729\n",
      "Epoch:  0    Batch Number:  694  /  1875    Autoencoder loss:  0.22359392046928406\n",
      "Epoch:  0    Batch Number:  695  /  1875    Autoencoder loss:  0.19595274329185486\n",
      "Epoch:  0    Batch Number:  696  /  1875    Autoencoder loss:  0.21743366122245789\n",
      "Epoch:  0    Batch Number:  697  /  1875    Autoencoder loss:  0.2233734279870987\n",
      "Epoch:  0    Batch Number:  698  /  1875    Autoencoder loss:  0.21272610127925873\n",
      "Epoch:  0    Batch Number:  699  /  1875    Autoencoder loss:  0.23172760009765625\n",
      "Epoch:  0    Batch Number:  700  /  1875    Autoencoder loss:  0.20643891394138336\n",
      "Epoch:  0    Batch Number:  701  /  1875    Autoencoder loss:  0.22262145578861237\n",
      "Epoch:  0    Batch Number:  702  /  1875    Autoencoder loss:  0.20563094317913055\n",
      "Epoch:  0    Batch Number:  703  /  1875    Autoencoder loss:  0.2107694000005722\n",
      "Epoch:  0    Batch Number:  704  /  1875    Autoencoder loss:  0.19166214764118195\n",
      "Epoch:  0    Batch Number:  705  /  1875    Autoencoder loss:  0.23522038757801056\n",
      "Epoch:  0    Batch Number:  706  /  1875    Autoencoder loss:  0.20523397624492645\n",
      "Epoch:  0    Batch Number:  707  /  1875    Autoencoder loss:  0.2141232043504715\n",
      "Epoch:  0    Batch Number:  708  /  1875    Autoencoder loss:  0.20694470405578613\n",
      "Epoch:  0    Batch Number:  709  /  1875    Autoencoder loss:  0.21857759356498718\n",
      "Epoch:  0    Batch Number:  710  /  1875    Autoencoder loss:  0.2160383015871048\n",
      "Epoch:  0    Batch Number:  711  /  1875    Autoencoder loss:  0.20243799686431885\n",
      "Epoch:  0    Batch Number:  712  /  1875    Autoencoder loss:  0.2173546552658081\n",
      "Epoch:  0    Batch Number:  713  /  1875    Autoencoder loss:  0.20489192008972168\n",
      "Epoch:  0    Batch Number:  714  /  1875    Autoencoder loss:  0.19201181828975677\n",
      "Epoch:  0    Batch Number:  715  /  1875    Autoencoder loss:  0.20257829129695892\n",
      "Epoch:  0    Batch Number:  716  /  1875    Autoencoder loss:  0.22865594923496246\n",
      "Epoch:  0    Batch Number:  717  /  1875    Autoencoder loss:  0.2106480896472931\n",
      "Epoch:  0    Batch Number:  718  /  1875    Autoencoder loss:  0.22254103422164917\n",
      "Epoch:  0    Batch Number:  719  /  1875    Autoencoder loss:  0.21489053964614868\n",
      "Epoch:  0    Batch Number:  720  /  1875    Autoencoder loss:  0.20959465205669403\n",
      "Epoch:  0    Batch Number:  721  /  1875    Autoencoder loss:  0.22108103334903717\n",
      "Epoch:  0    Batch Number:  722  /  1875    Autoencoder loss:  0.2042890042066574\n",
      "Epoch:  0    Batch Number:  723  /  1875    Autoencoder loss:  0.20985639095306396\n",
      "Epoch:  0    Batch Number:  724  /  1875    Autoencoder loss:  0.215188667178154\n",
      "Epoch:  0    Batch Number:  725  /  1875    Autoencoder loss:  0.2040959894657135\n",
      "Epoch:  0    Batch Number:  726  /  1875    Autoencoder loss:  0.20797304809093475\n",
      "Epoch:  0    Batch Number:  727  /  1875    Autoencoder loss:  0.20708994567394257\n",
      "Epoch:  0    Batch Number:  728  /  1875    Autoencoder loss:  0.2118086963891983\n",
      "Epoch:  0    Batch Number:  729  /  1875    Autoencoder loss:  0.21970969438552856\n",
      "Epoch:  0    Batch Number:  730  /  1875    Autoencoder loss:  0.22286777198314667\n",
      "Epoch:  0    Batch Number:  731  /  1875    Autoencoder loss:  0.20782233774662018\n",
      "Epoch:  0    Batch Number:  732  /  1875    Autoencoder loss:  0.20504118502140045\n",
      "Epoch:  0    Batch Number:  733  /  1875    Autoencoder loss:  0.21502342820167542\n",
      "Epoch:  0    Batch Number:  734  /  1875    Autoencoder loss:  0.22442835569381714\n",
      "Epoch:  0    Batch Number:  735  /  1875    Autoencoder loss:  0.18974582850933075\n",
      "Epoch:  0    Batch Number:  736  /  1875    Autoencoder loss:  0.2096920609474182\n",
      "Epoch:  0    Batch Number:  737  /  1875    Autoencoder loss:  0.2003464549779892\n",
      "Epoch:  0    Batch Number:  738  /  1875    Autoencoder loss:  0.2143731415271759\n",
      "Epoch:  0    Batch Number:  739  /  1875    Autoencoder loss:  0.21931332349777222\n",
      "Epoch:  0    Batch Number:  740  /  1875    Autoencoder loss:  0.18009698390960693\n",
      "Epoch:  0    Batch Number:  741  /  1875    Autoencoder loss:  0.20745600759983063\n",
      "Epoch:  0    Batch Number:  742  /  1875    Autoencoder loss:  0.2244357019662857\n",
      "Epoch:  0    Batch Number:  743  /  1875    Autoencoder loss:  0.2116127610206604\n",
      "Epoch:  0    Batch Number:  744  /  1875    Autoencoder loss:  0.2001124769449234\n",
      "Epoch:  0    Batch Number:  745  /  1875    Autoencoder loss:  0.20425839722156525\n",
      "Epoch:  0    Batch Number:  746  /  1875    Autoencoder loss:  0.22166793048381805\n",
      "Epoch:  0    Batch Number:  747  /  1875    Autoencoder loss:  0.1882304698228836\n",
      "Epoch:  0    Batch Number:  748  /  1875    Autoencoder loss:  0.18247845768928528\n",
      "Epoch:  0    Batch Number:  749  /  1875    Autoencoder loss:  0.2261400818824768\n",
      "Epoch:  0    Batch Number:  750  /  1875    Autoencoder loss:  0.2041710466146469\n",
      "Epoch:  0    Batch Number:  751  /  1875    Autoencoder loss:  0.2006869614124298\n",
      "Epoch:  0    Batch Number:  752  /  1875    Autoencoder loss:  0.20519165694713593\n",
      "Epoch:  0    Batch Number:  753  /  1875    Autoencoder loss:  0.22773399949073792\n",
      "Epoch:  0    Batch Number:  754  /  1875    Autoencoder loss:  0.21501164138317108\n",
      "Epoch:  0    Batch Number:  755  /  1875    Autoencoder loss:  0.22615981101989746\n",
      "Epoch:  0    Batch Number:  756  /  1875    Autoencoder loss:  0.20087866485118866\n",
      "Epoch:  0    Batch Number:  757  /  1875    Autoencoder loss:  0.19373838603496552\n",
      "Epoch:  0    Batch Number:  758  /  1875    Autoencoder loss:  0.20142009854316711\n",
      "Epoch:  0    Batch Number:  759  /  1875    Autoencoder loss:  0.20111390948295593\n",
      "Epoch:  0    Batch Number:  760  /  1875    Autoencoder loss:  0.21729063987731934\n",
      "Epoch:  0    Batch Number:  761  /  1875    Autoencoder loss:  0.19628870487213135\n",
      "Epoch:  0    Batch Number:  762  /  1875    Autoencoder loss:  0.22119390964508057\n",
      "Epoch:  0    Batch Number:  763  /  1875    Autoencoder loss:  0.21044600009918213\n",
      "Epoch:  0    Batch Number:  764  /  1875    Autoencoder loss:  0.2179732471704483\n",
      "Epoch:  0    Batch Number:  765  /  1875    Autoencoder loss:  0.2092713713645935\n",
      "Epoch:  0    Batch Number:  766  /  1875    Autoencoder loss:  0.20067772269248962\n",
      "Epoch:  0    Batch Number:  767  /  1875    Autoencoder loss:  0.2136104553937912\n",
      "Epoch:  0    Batch Number:  768  /  1875    Autoencoder loss:  0.2131195217370987\n",
      "Epoch:  0    Batch Number:  769  /  1875    Autoencoder loss:  0.20534612238407135\n",
      "Epoch:  0    Batch Number:  770  /  1875    Autoencoder loss:  0.1976507604122162\n",
      "Epoch:  0    Batch Number:  771  /  1875    Autoencoder loss:  0.22608555853366852\n",
      "Epoch:  0    Batch Number:  772  /  1875    Autoencoder loss:  0.2120918333530426\n",
      "Epoch:  0    Batch Number:  773  /  1875    Autoencoder loss:  0.2333993762731552\n",
      "Epoch:  0    Batch Number:  774  /  1875    Autoencoder loss:  0.19828349351882935\n",
      "Epoch:  0    Batch Number:  775  /  1875    Autoencoder loss:  0.19277171790599823\n",
      "Epoch:  0    Batch Number:  776  /  1875    Autoencoder loss:  0.2194693386554718\n",
      "Epoch:  0    Batch Number:  777  /  1875    Autoencoder loss:  0.2093486785888672\n",
      "Epoch:  0    Batch Number:  778  /  1875    Autoencoder loss:  0.21559520065784454\n",
      "Epoch:  0    Batch Number:  779  /  1875    Autoencoder loss:  0.20715460181236267\n",
      "Epoch:  0    Batch Number:  780  /  1875    Autoencoder loss:  0.19106128811836243\n",
      "Epoch:  0    Batch Number:  781  /  1875    Autoencoder loss:  0.2223946750164032\n",
      "Epoch:  0    Batch Number:  782  /  1875    Autoencoder loss:  0.19706210494041443\n",
      "Epoch:  0    Batch Number:  783  /  1875    Autoencoder loss:  0.2116035372018814\n",
      "Epoch:  0    Batch Number:  784  /  1875    Autoencoder loss:  0.18307098746299744\n",
      "Epoch:  0    Batch Number:  785  /  1875    Autoencoder loss:  0.21466611325740814\n",
      "Epoch:  0    Batch Number:  786  /  1875    Autoencoder loss:  0.2202014923095703\n",
      "Epoch:  0    Batch Number:  787  /  1875    Autoencoder loss:  0.2088533639907837\n",
      "Epoch:  0    Batch Number:  788  /  1875    Autoencoder loss:  0.1944083422422409\n",
      "Epoch:  0    Batch Number:  789  /  1875    Autoencoder loss:  0.21591366827487946\n",
      "Epoch:  0    Batch Number:  790  /  1875    Autoencoder loss:  0.2005397230386734\n",
      "Epoch:  0    Batch Number:  791  /  1875    Autoencoder loss:  0.1986967921257019\n",
      "Epoch:  0    Batch Number:  792  /  1875    Autoencoder loss:  0.20554815232753754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  793  /  1875    Autoencoder loss:  0.22341620922088623\n",
      "Epoch:  0    Batch Number:  794  /  1875    Autoencoder loss:  0.18662011623382568\n",
      "Epoch:  0    Batch Number:  795  /  1875    Autoencoder loss:  0.19412460923194885\n",
      "Epoch:  0    Batch Number:  796  /  1875    Autoencoder loss:  0.22499525547027588\n",
      "Epoch:  0    Batch Number:  797  /  1875    Autoencoder loss:  0.2101525217294693\n",
      "Epoch:  0    Batch Number:  798  /  1875    Autoencoder loss:  0.19423620402812958\n",
      "Epoch:  0    Batch Number:  799  /  1875    Autoencoder loss:  0.22997380793094635\n",
      "Epoch:  0    Batch Number:  800  /  1875    Autoencoder loss:  0.19876888394355774\n",
      "Epoch:  0    Batch Number:  801  /  1875    Autoencoder loss:  0.1924944669008255\n",
      "Epoch:  0    Batch Number:  802  /  1875    Autoencoder loss:  0.2161441147327423\n",
      "Epoch:  0    Batch Number:  803  /  1875    Autoencoder loss:  0.22229976952075958\n",
      "Epoch:  0    Batch Number:  804  /  1875    Autoencoder loss:  0.19807709753513336\n",
      "Epoch:  0    Batch Number:  805  /  1875    Autoencoder loss:  0.2200096696615219\n",
      "Epoch:  0    Batch Number:  806  /  1875    Autoencoder loss:  0.22081834077835083\n",
      "Epoch:  0    Batch Number:  807  /  1875    Autoencoder loss:  0.20281147956848145\n",
      "Epoch:  0    Batch Number:  808  /  1875    Autoencoder loss:  0.2122175395488739\n",
      "Epoch:  0    Batch Number:  809  /  1875    Autoencoder loss:  0.2132982760667801\n",
      "Epoch:  0    Batch Number:  810  /  1875    Autoencoder loss:  0.1982724964618683\n",
      "Epoch:  0    Batch Number:  811  /  1875    Autoencoder loss:  0.22301775217056274\n",
      "Epoch:  0    Batch Number:  812  /  1875    Autoencoder loss:  0.20189952850341797\n",
      "Epoch:  0    Batch Number:  813  /  1875    Autoencoder loss:  0.23158249258995056\n",
      "Epoch:  0    Batch Number:  814  /  1875    Autoencoder loss:  0.19012051820755005\n",
      "Epoch:  0    Batch Number:  815  /  1875    Autoencoder loss:  0.22003096342086792\n",
      "Epoch:  0    Batch Number:  816  /  1875    Autoencoder loss:  0.2095780074596405\n",
      "Epoch:  0    Batch Number:  817  /  1875    Autoencoder loss:  0.1985786259174347\n",
      "Epoch:  0    Batch Number:  818  /  1875    Autoencoder loss:  0.22283746302127838\n",
      "Epoch:  0    Batch Number:  819  /  1875    Autoencoder loss:  0.20380082726478577\n",
      "Epoch:  0    Batch Number:  820  /  1875    Autoencoder loss:  0.20242106914520264\n",
      "Epoch:  0    Batch Number:  821  /  1875    Autoencoder loss:  0.2024947553873062\n",
      "Epoch:  0    Batch Number:  822  /  1875    Autoencoder loss:  0.22690606117248535\n",
      "Epoch:  0    Batch Number:  823  /  1875    Autoencoder loss:  0.21909120678901672\n",
      "Epoch:  0    Batch Number:  824  /  1875    Autoencoder loss:  0.2218550443649292\n",
      "Epoch:  0    Batch Number:  825  /  1875    Autoencoder loss:  0.20213913917541504\n",
      "Epoch:  0    Batch Number:  826  /  1875    Autoencoder loss:  0.19576191902160645\n",
      "Epoch:  0    Batch Number:  827  /  1875    Autoencoder loss:  0.2002212554216385\n",
      "Epoch:  0    Batch Number:  828  /  1875    Autoencoder loss:  0.2065959870815277\n",
      "Epoch:  0    Batch Number:  829  /  1875    Autoencoder loss:  0.21950550377368927\n",
      "Epoch:  0    Batch Number:  830  /  1875    Autoencoder loss:  0.1947028934955597\n",
      "Epoch:  0    Batch Number:  831  /  1875    Autoencoder loss:  0.19789141416549683\n",
      "Epoch:  0    Batch Number:  832  /  1875    Autoencoder loss:  0.2095550298690796\n",
      "Epoch:  0    Batch Number:  833  /  1875    Autoencoder loss:  0.2197948396205902\n",
      "Epoch:  0    Batch Number:  834  /  1875    Autoencoder loss:  0.20481517910957336\n",
      "Epoch:  0    Batch Number:  835  /  1875    Autoencoder loss:  0.22280023992061615\n",
      "Epoch:  0    Batch Number:  836  /  1875    Autoencoder loss:  0.21012231707572937\n",
      "Epoch:  0    Batch Number:  837  /  1875    Autoencoder loss:  0.21737021207809448\n",
      "Epoch:  0    Batch Number:  838  /  1875    Autoencoder loss:  0.19637322425842285\n",
      "Epoch:  0    Batch Number:  839  /  1875    Autoencoder loss:  0.21471066772937775\n",
      "Epoch:  0    Batch Number:  840  /  1875    Autoencoder loss:  0.18731124699115753\n",
      "Epoch:  0    Batch Number:  841  /  1875    Autoencoder loss:  0.2214527577161789\n",
      "Epoch:  0    Batch Number:  842  /  1875    Autoencoder loss:  0.2040576934814453\n",
      "Epoch:  0    Batch Number:  843  /  1875    Autoencoder loss:  0.2117927372455597\n",
      "Epoch:  0    Batch Number:  844  /  1875    Autoencoder loss:  0.21662522852420807\n",
      "Epoch:  0    Batch Number:  845  /  1875    Autoencoder loss:  0.2221544086933136\n",
      "Epoch:  0    Batch Number:  846  /  1875    Autoencoder loss:  0.21048393845558167\n",
      "Epoch:  0    Batch Number:  847  /  1875    Autoencoder loss:  0.207633376121521\n",
      "Epoch:  0    Batch Number:  848  /  1875    Autoencoder loss:  0.19571954011917114\n",
      "Epoch:  0    Batch Number:  849  /  1875    Autoencoder loss:  0.21211090683937073\n",
      "Epoch:  0    Batch Number:  850  /  1875    Autoencoder loss:  0.19601842761039734\n",
      "Epoch:  0    Batch Number:  851  /  1875    Autoencoder loss:  0.20746344327926636\n",
      "Epoch:  0    Batch Number:  852  /  1875    Autoencoder loss:  0.2131945937871933\n",
      "Epoch:  0    Batch Number:  853  /  1875    Autoencoder loss:  0.20352639257907867\n",
      "Epoch:  0    Batch Number:  854  /  1875    Autoencoder loss:  0.21175983548164368\n",
      "Epoch:  0    Batch Number:  855  /  1875    Autoencoder loss:  0.20544247329235077\n",
      "Epoch:  0    Batch Number:  856  /  1875    Autoencoder loss:  0.19864661991596222\n",
      "Epoch:  0    Batch Number:  857  /  1875    Autoencoder loss:  0.22128386795520782\n",
      "Epoch:  0    Batch Number:  858  /  1875    Autoencoder loss:  0.22019919753074646\n",
      "Epoch:  0    Batch Number:  859  /  1875    Autoencoder loss:  0.20695766806602478\n",
      "Epoch:  0    Batch Number:  860  /  1875    Autoencoder loss:  0.20734328031539917\n",
      "Epoch:  0    Batch Number:  861  /  1875    Autoencoder loss:  0.20900750160217285\n",
      "Epoch:  0    Batch Number:  862  /  1875    Autoencoder loss:  0.20215587317943573\n",
      "Epoch:  0    Batch Number:  863  /  1875    Autoencoder loss:  0.22788865864276886\n",
      "Epoch:  0    Batch Number:  864  /  1875    Autoencoder loss:  0.21945393085479736\n",
      "Epoch:  0    Batch Number:  865  /  1875    Autoencoder loss:  0.21568459272384644\n",
      "Epoch:  0    Batch Number:  866  /  1875    Autoencoder loss:  0.20263594388961792\n",
      "Epoch:  0    Batch Number:  867  /  1875    Autoencoder loss:  0.1993059366941452\n",
      "Epoch:  0    Batch Number:  868  /  1875    Autoencoder loss:  0.21796829998493195\n",
      "Epoch:  0    Batch Number:  869  /  1875    Autoencoder loss:  0.21079853177070618\n",
      "Epoch:  0    Batch Number:  870  /  1875    Autoencoder loss:  0.21056681871414185\n",
      "Epoch:  0    Batch Number:  871  /  1875    Autoencoder loss:  0.20083600282669067\n",
      "Epoch:  0    Batch Number:  872  /  1875    Autoencoder loss:  0.21529695391654968\n",
      "Epoch:  0    Batch Number:  873  /  1875    Autoencoder loss:  0.21333369612693787\n",
      "Epoch:  0    Batch Number:  874  /  1875    Autoencoder loss:  0.1999788135290146\n",
      "Epoch:  0    Batch Number:  875  /  1875    Autoencoder loss:  0.19050879776477814\n",
      "Epoch:  0    Batch Number:  876  /  1875    Autoencoder loss:  0.22073699533939362\n",
      "Epoch:  0    Batch Number:  877  /  1875    Autoencoder loss:  0.20659604668617249\n",
      "Epoch:  0    Batch Number:  878  /  1875    Autoencoder loss:  0.1965288370847702\n",
      "Epoch:  0    Batch Number:  879  /  1875    Autoencoder loss:  0.20436310768127441\n",
      "Epoch:  0    Batch Number:  880  /  1875    Autoencoder loss:  0.21990631520748138\n",
      "Epoch:  0    Batch Number:  881  /  1875    Autoencoder loss:  0.2033921778202057\n",
      "Epoch:  0    Batch Number:  882  /  1875    Autoencoder loss:  0.21207155287265778\n",
      "Epoch:  0    Batch Number:  883  /  1875    Autoencoder loss:  0.21327289938926697\n",
      "Epoch:  0    Batch Number:  884  /  1875    Autoencoder loss:  0.20693905651569366\n",
      "Epoch:  0    Batch Number:  885  /  1875    Autoencoder loss:  0.19519786536693573\n",
      "Epoch:  0    Batch Number:  886  /  1875    Autoencoder loss:  0.21032828092575073\n",
      "Epoch:  0    Batch Number:  887  /  1875    Autoencoder loss:  0.19516123831272125\n",
      "Epoch:  0    Batch Number:  888  /  1875    Autoencoder loss:  0.20347937941551208\n",
      "Epoch:  0    Batch Number:  889  /  1875    Autoencoder loss:  0.20661112666130066\n",
      "Epoch:  0    Batch Number:  890  /  1875    Autoencoder loss:  0.20470720529556274\n",
      "Epoch:  0    Batch Number:  891  /  1875    Autoencoder loss:  0.2301090806722641\n",
      "Epoch:  0    Batch Number:  892  /  1875    Autoencoder loss:  0.22449277341365814\n",
      "Epoch:  0    Batch Number:  893  /  1875    Autoencoder loss:  0.22333507239818573\n",
      "Epoch:  0    Batch Number:  894  /  1875    Autoencoder loss:  0.21151146292686462\n",
      "Epoch:  0    Batch Number:  895  /  1875    Autoencoder loss:  0.1864766627550125\n",
      "Epoch:  0    Batch Number:  896  /  1875    Autoencoder loss:  0.1955282986164093\n",
      "Epoch:  0    Batch Number:  897  /  1875    Autoencoder loss:  0.1941307634115219\n",
      "Epoch:  0    Batch Number:  898  /  1875    Autoencoder loss:  0.20066021382808685\n",
      "Epoch:  0    Batch Number:  899  /  1875    Autoencoder loss:  0.19895915687084198\n",
      "Epoch:  0    Batch Number:  900  /  1875    Autoencoder loss:  0.21009591221809387\n",
      "Epoch:  0    Batch Number:  901  /  1875    Autoencoder loss:  0.22602318227291107\n",
      "Epoch:  0    Batch Number:  902  /  1875    Autoencoder loss:  0.2239854335784912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  903  /  1875    Autoencoder loss:  0.1981874406337738\n",
      "Epoch:  0    Batch Number:  904  /  1875    Autoencoder loss:  0.22545106709003448\n",
      "Epoch:  0    Batch Number:  905  /  1875    Autoencoder loss:  0.206107497215271\n",
      "Epoch:  0    Batch Number:  906  /  1875    Autoencoder loss:  0.20826968550682068\n",
      "Epoch:  0    Batch Number:  907  /  1875    Autoencoder loss:  0.2065797597169876\n",
      "Epoch:  0    Batch Number:  908  /  1875    Autoencoder loss:  0.21541766822338104\n",
      "Epoch:  0    Batch Number:  909  /  1875    Autoencoder loss:  0.22049127519130707\n",
      "Epoch:  0    Batch Number:  910  /  1875    Autoencoder loss:  0.22826500236988068\n",
      "Epoch:  0    Batch Number:  911  /  1875    Autoencoder loss:  0.19782212376594543\n",
      "Epoch:  0    Batch Number:  912  /  1875    Autoencoder loss:  0.20675398409366608\n",
      "Epoch:  0    Batch Number:  913  /  1875    Autoencoder loss:  0.17969267070293427\n",
      "Epoch:  0    Batch Number:  914  /  1875    Autoencoder loss:  0.21301615238189697\n",
      "Epoch:  0    Batch Number:  915  /  1875    Autoencoder loss:  0.1931963562965393\n",
      "Epoch:  0    Batch Number:  916  /  1875    Autoencoder loss:  0.20283769071102142\n",
      "Epoch:  0    Batch Number:  917  /  1875    Autoencoder loss:  0.21145513653755188\n",
      "Epoch:  0    Batch Number:  918  /  1875    Autoencoder loss:  0.19231334328651428\n",
      "Epoch:  0    Batch Number:  919  /  1875    Autoencoder loss:  0.21035714447498322\n",
      "Epoch:  0    Batch Number:  920  /  1875    Autoencoder loss:  0.21347248554229736\n",
      "Epoch:  0    Batch Number:  921  /  1875    Autoencoder loss:  0.226106658577919\n",
      "Epoch:  0    Batch Number:  922  /  1875    Autoencoder loss:  0.2233125865459442\n",
      "Epoch:  0    Batch Number:  923  /  1875    Autoencoder loss:  0.21496449410915375\n",
      "Epoch:  0    Batch Number:  924  /  1875    Autoencoder loss:  0.21831881999969482\n",
      "Epoch:  0    Batch Number:  925  /  1875    Autoencoder loss:  0.2165568619966507\n",
      "Epoch:  0    Batch Number:  926  /  1875    Autoencoder loss:  0.22948868572711945\n",
      "Epoch:  0    Batch Number:  927  /  1875    Autoencoder loss:  0.21509508788585663\n",
      "Epoch:  0    Batch Number:  928  /  1875    Autoencoder loss:  0.21889974176883698\n",
      "Epoch:  0    Batch Number:  929  /  1875    Autoencoder loss:  0.19730190932750702\n",
      "Epoch:  0    Batch Number:  930  /  1875    Autoencoder loss:  0.22214703261852264\n",
      "Epoch:  0    Batch Number:  931  /  1875    Autoencoder loss:  0.215937539935112\n",
      "Epoch:  0    Batch Number:  932  /  1875    Autoencoder loss:  0.20428182184696198\n",
      "Epoch:  0    Batch Number:  933  /  1875    Autoencoder loss:  0.20626245439052582\n",
      "Epoch:  0    Batch Number:  934  /  1875    Autoencoder loss:  0.21561451256275177\n",
      "Epoch:  0    Batch Number:  935  /  1875    Autoencoder loss:  0.219073086977005\n",
      "Epoch:  0    Batch Number:  936  /  1875    Autoencoder loss:  0.22838085889816284\n",
      "Epoch:  0    Batch Number:  937  /  1875    Autoencoder loss:  0.2101053148508072\n",
      "Epoch:  0    Batch Number:  938  /  1875    Autoencoder loss:  0.19852975010871887\n",
      "Epoch:  0    Batch Number:  939  /  1875    Autoencoder loss:  0.2030930072069168\n",
      "Epoch:  0    Batch Number:  940  /  1875    Autoencoder loss:  0.2155843824148178\n",
      "Epoch:  0    Batch Number:  941  /  1875    Autoencoder loss:  0.20531679689884186\n",
      "Epoch:  0    Batch Number:  942  /  1875    Autoencoder loss:  0.20577603578567505\n",
      "Epoch:  0    Batch Number:  943  /  1875    Autoencoder loss:  0.2156899869441986\n",
      "Epoch:  0    Batch Number:  944  /  1875    Autoencoder loss:  0.21796339750289917\n",
      "Epoch:  0    Batch Number:  945  /  1875    Autoencoder loss:  0.20466811954975128\n",
      "Epoch:  0    Batch Number:  946  /  1875    Autoencoder loss:  0.2049909383058548\n",
      "Epoch:  0    Batch Number:  947  /  1875    Autoencoder loss:  0.18640531599521637\n",
      "Epoch:  0    Batch Number:  948  /  1875    Autoencoder loss:  0.21474242210388184\n",
      "Epoch:  0    Batch Number:  949  /  1875    Autoencoder loss:  0.19903941452503204\n",
      "Epoch:  0    Batch Number:  950  /  1875    Autoencoder loss:  0.21153858304023743\n",
      "Epoch:  0    Batch Number:  951  /  1875    Autoencoder loss:  0.20118343830108643\n",
      "Epoch:  0    Batch Number:  952  /  1875    Autoencoder loss:  0.21329568326473236\n",
      "Epoch:  0    Batch Number:  953  /  1875    Autoencoder loss:  0.21718961000442505\n",
      "Epoch:  0    Batch Number:  954  /  1875    Autoencoder loss:  0.20677651464939117\n",
      "Epoch:  0    Batch Number:  955  /  1875    Autoencoder loss:  0.1969020813703537\n",
      "Epoch:  0    Batch Number:  956  /  1875    Autoencoder loss:  0.22382286190986633\n",
      "Epoch:  0    Batch Number:  957  /  1875    Autoencoder loss:  0.21026074886322021\n",
      "Epoch:  0    Batch Number:  958  /  1875    Autoencoder loss:  0.21813632547855377\n",
      "Epoch:  0    Batch Number:  959  /  1875    Autoencoder loss:  0.20320546627044678\n",
      "Epoch:  0    Batch Number:  960  /  1875    Autoencoder loss:  0.20896732807159424\n",
      "Epoch:  0    Batch Number:  961  /  1875    Autoencoder loss:  0.19318000972270966\n",
      "Epoch:  0    Batch Number:  962  /  1875    Autoencoder loss:  0.22956690192222595\n",
      "Epoch:  0    Batch Number:  963  /  1875    Autoencoder loss:  0.20992633700370789\n",
      "Epoch:  0    Batch Number:  964  /  1875    Autoencoder loss:  0.19085031747817993\n",
      "Epoch:  0    Batch Number:  965  /  1875    Autoencoder loss:  0.20696955919265747\n",
      "Epoch:  0    Batch Number:  966  /  1875    Autoencoder loss:  0.22312214970588684\n",
      "Epoch:  0    Batch Number:  967  /  1875    Autoencoder loss:  0.20509272813796997\n",
      "Epoch:  0    Batch Number:  968  /  1875    Autoencoder loss:  0.19942481815814972\n",
      "Epoch:  0    Batch Number:  969  /  1875    Autoencoder loss:  0.21362008154392242\n",
      "Epoch:  0    Batch Number:  970  /  1875    Autoencoder loss:  0.1998196691274643\n",
      "Epoch:  0    Batch Number:  971  /  1875    Autoencoder loss:  0.199073925614357\n",
      "Epoch:  0    Batch Number:  972  /  1875    Autoencoder loss:  0.2129536122083664\n",
      "Epoch:  0    Batch Number:  973  /  1875    Autoencoder loss:  0.2019985020160675\n",
      "Epoch:  0    Batch Number:  974  /  1875    Autoencoder loss:  0.20727694034576416\n",
      "Epoch:  0    Batch Number:  975  /  1875    Autoencoder loss:  0.19001412391662598\n",
      "Epoch:  0    Batch Number:  976  /  1875    Autoencoder loss:  0.2061050832271576\n",
      "Epoch:  0    Batch Number:  977  /  1875    Autoencoder loss:  0.21520422399044037\n",
      "Epoch:  0    Batch Number:  978  /  1875    Autoencoder loss:  0.19826483726501465\n",
      "Epoch:  0    Batch Number:  979  /  1875    Autoencoder loss:  0.20605851709842682\n",
      "Epoch:  0    Batch Number:  980  /  1875    Autoencoder loss:  0.20183920860290527\n",
      "Epoch:  0    Batch Number:  981  /  1875    Autoencoder loss:  0.2050536870956421\n",
      "Epoch:  0    Batch Number:  982  /  1875    Autoencoder loss:  0.20233140885829926\n",
      "Epoch:  0    Batch Number:  983  /  1875    Autoencoder loss:  0.21990647912025452\n",
      "Epoch:  0    Batch Number:  984  /  1875    Autoencoder loss:  0.1882263720035553\n",
      "Epoch:  0    Batch Number:  985  /  1875    Autoencoder loss:  0.20796877145767212\n",
      "Epoch:  0    Batch Number:  986  /  1875    Autoencoder loss:  0.1959860622882843\n",
      "Epoch:  0    Batch Number:  987  /  1875    Autoencoder loss:  0.19667905569076538\n",
      "Epoch:  0    Batch Number:  988  /  1875    Autoencoder loss:  0.19152267277240753\n",
      "Epoch:  0    Batch Number:  989  /  1875    Autoencoder loss:  0.2039715051651001\n",
      "Epoch:  0    Batch Number:  990  /  1875    Autoencoder loss:  0.17855410277843475\n",
      "Epoch:  0    Batch Number:  991  /  1875    Autoencoder loss:  0.18409894406795502\n",
      "Epoch:  0    Batch Number:  992  /  1875    Autoencoder loss:  0.20059694349765778\n",
      "Epoch:  0    Batch Number:  993  /  1875    Autoencoder loss:  0.20160752534866333\n",
      "Epoch:  0    Batch Number:  994  /  1875    Autoencoder loss:  0.22826004028320312\n",
      "Epoch:  0    Batch Number:  995  /  1875    Autoencoder loss:  0.21540415287017822\n",
      "Epoch:  0    Batch Number:  996  /  1875    Autoencoder loss:  0.21746233105659485\n",
      "Epoch:  0    Batch Number:  997  /  1875    Autoencoder loss:  0.1932285875082016\n",
      "Epoch:  0    Batch Number:  998  /  1875    Autoencoder loss:  0.2038654237985611\n",
      "Epoch:  0    Batch Number:  999  /  1875    Autoencoder loss:  0.207381933927536\n",
      "Epoch:  0    Batch Number:  1000  /  1875    Autoencoder loss:  0.20012186467647552\n",
      "Epoch:  0    Batch Number:  1001  /  1875    Autoencoder loss:  0.2009749859571457\n",
      "Epoch:  0    Batch Number:  1002  /  1875    Autoencoder loss:  0.21753641963005066\n",
      "Epoch:  0    Batch Number:  1003  /  1875    Autoencoder loss:  0.19316984713077545\n",
      "Epoch:  0    Batch Number:  1004  /  1875    Autoencoder loss:  0.23022881150245667\n",
      "Epoch:  0    Batch Number:  1005  /  1875    Autoencoder loss:  0.19903171062469482\n",
      "Epoch:  0    Batch Number:  1006  /  1875    Autoencoder loss:  0.19504281878471375\n",
      "Epoch:  0    Batch Number:  1007  /  1875    Autoencoder loss:  0.22349129617214203\n",
      "Epoch:  0    Batch Number:  1008  /  1875    Autoencoder loss:  0.19772231578826904\n",
      "Epoch:  0    Batch Number:  1009  /  1875    Autoencoder loss:  0.20536862313747406\n",
      "Epoch:  0    Batch Number:  1010  /  1875    Autoencoder loss:  0.20379525423049927\n",
      "Epoch:  0    Batch Number:  1011  /  1875    Autoencoder loss:  0.19977103173732758\n",
      "Epoch:  0    Batch Number:  1012  /  1875    Autoencoder loss:  0.18839867413043976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1013  /  1875    Autoencoder loss:  0.20255914330482483\n",
      "Epoch:  0    Batch Number:  1014  /  1875    Autoencoder loss:  0.20006611943244934\n",
      "Epoch:  0    Batch Number:  1015  /  1875    Autoencoder loss:  0.20224976539611816\n",
      "Epoch:  0    Batch Number:  1016  /  1875    Autoencoder loss:  0.1911790370941162\n",
      "Epoch:  0    Batch Number:  1017  /  1875    Autoencoder loss:  0.18444906175136566\n",
      "Epoch:  0    Batch Number:  1018  /  1875    Autoencoder loss:  0.2026488035917282\n",
      "Epoch:  0    Batch Number:  1019  /  1875    Autoencoder loss:  0.20040275156497955\n",
      "Epoch:  0    Batch Number:  1020  /  1875    Autoencoder loss:  0.22005264461040497\n",
      "Epoch:  0    Batch Number:  1021  /  1875    Autoencoder loss:  0.20637376606464386\n",
      "Epoch:  0    Batch Number:  1022  /  1875    Autoencoder loss:  0.21503043174743652\n",
      "Epoch:  0    Batch Number:  1023  /  1875    Autoencoder loss:  0.19338876008987427\n",
      "Epoch:  0    Batch Number:  1024  /  1875    Autoencoder loss:  0.1965227872133255\n",
      "Epoch:  0    Batch Number:  1025  /  1875    Autoencoder loss:  0.20457369089126587\n",
      "Epoch:  0    Batch Number:  1026  /  1875    Autoencoder loss:  0.19780802726745605\n",
      "Epoch:  0    Batch Number:  1027  /  1875    Autoencoder loss:  0.16701655089855194\n",
      "Epoch:  0    Batch Number:  1028  /  1875    Autoencoder loss:  0.2217118740081787\n",
      "Epoch:  0    Batch Number:  1029  /  1875    Autoencoder loss:  0.1954076737165451\n",
      "Epoch:  0    Batch Number:  1030  /  1875    Autoencoder loss:  0.19124357402324677\n",
      "Epoch:  0    Batch Number:  1031  /  1875    Autoencoder loss:  0.20070411264896393\n",
      "Epoch:  0    Batch Number:  1032  /  1875    Autoencoder loss:  0.20969994366168976\n",
      "Epoch:  0    Batch Number:  1033  /  1875    Autoencoder loss:  0.2088083028793335\n",
      "Epoch:  0    Batch Number:  1034  /  1875    Autoencoder loss:  0.19556590914726257\n",
      "Epoch:  0    Batch Number:  1035  /  1875    Autoencoder loss:  0.21049456298351288\n",
      "Epoch:  0    Batch Number:  1036  /  1875    Autoencoder loss:  0.21002045273780823\n",
      "Epoch:  0    Batch Number:  1037  /  1875    Autoencoder loss:  0.19581207633018494\n",
      "Epoch:  0    Batch Number:  1038  /  1875    Autoencoder loss:  0.19733810424804688\n",
      "Epoch:  0    Batch Number:  1039  /  1875    Autoencoder loss:  0.21944041550159454\n",
      "Epoch:  0    Batch Number:  1040  /  1875    Autoencoder loss:  0.20392343401908875\n",
      "Epoch:  0    Batch Number:  1041  /  1875    Autoencoder loss:  0.19948852062225342\n",
      "Epoch:  0    Batch Number:  1042  /  1875    Autoencoder loss:  0.19510114192962646\n",
      "Epoch:  0    Batch Number:  1043  /  1875    Autoencoder loss:  0.2104879915714264\n",
      "Epoch:  0    Batch Number:  1044  /  1875    Autoencoder loss:  0.21668429672718048\n",
      "Epoch:  0    Batch Number:  1045  /  1875    Autoencoder loss:  0.21129199862480164\n",
      "Epoch:  0    Batch Number:  1046  /  1875    Autoencoder loss:  0.21720632910728455\n",
      "Epoch:  0    Batch Number:  1047  /  1875    Autoencoder loss:  0.20720486342906952\n",
      "Epoch:  0    Batch Number:  1048  /  1875    Autoencoder loss:  0.19576524198055267\n",
      "Epoch:  0    Batch Number:  1049  /  1875    Autoencoder loss:  0.19231760501861572\n",
      "Epoch:  0    Batch Number:  1050  /  1875    Autoencoder loss:  0.18837355077266693\n",
      "Epoch:  0    Batch Number:  1051  /  1875    Autoencoder loss:  0.19253501296043396\n",
      "Epoch:  0    Batch Number:  1052  /  1875    Autoencoder loss:  0.207461878657341\n",
      "Epoch:  0    Batch Number:  1053  /  1875    Autoencoder loss:  0.19453033804893494\n",
      "Epoch:  0    Batch Number:  1054  /  1875    Autoencoder loss:  0.1997499316930771\n",
      "Epoch:  0    Batch Number:  1055  /  1875    Autoencoder loss:  0.19966378808021545\n",
      "Epoch:  0    Batch Number:  1056  /  1875    Autoencoder loss:  0.19692997634410858\n",
      "Epoch:  0    Batch Number:  1057  /  1875    Autoencoder loss:  0.20715728402137756\n",
      "Epoch:  0    Batch Number:  1058  /  1875    Autoencoder loss:  0.19448691606521606\n",
      "Epoch:  0    Batch Number:  1059  /  1875    Autoencoder loss:  0.19746233522891998\n",
      "Epoch:  0    Batch Number:  1060  /  1875    Autoencoder loss:  0.20618383586406708\n",
      "Epoch:  0    Batch Number:  1061  /  1875    Autoencoder loss:  0.19604720175266266\n",
      "Epoch:  0    Batch Number:  1062  /  1875    Autoencoder loss:  0.2044774293899536\n",
      "Epoch:  0    Batch Number:  1063  /  1875    Autoencoder loss:  0.20273108780384064\n",
      "Epoch:  0    Batch Number:  1064  /  1875    Autoencoder loss:  0.19844715297222137\n",
      "Epoch:  0    Batch Number:  1065  /  1875    Autoencoder loss:  0.20367832481861115\n",
      "Epoch:  0    Batch Number:  1066  /  1875    Autoencoder loss:  0.1954803317785263\n",
      "Epoch:  0    Batch Number:  1067  /  1875    Autoencoder loss:  0.19915758073329926\n",
      "Epoch:  0    Batch Number:  1068  /  1875    Autoencoder loss:  0.20445768535137177\n",
      "Epoch:  0    Batch Number:  1069  /  1875    Autoencoder loss:  0.21284593641757965\n",
      "Epoch:  0    Batch Number:  1070  /  1875    Autoencoder loss:  0.2331182062625885\n",
      "Epoch:  0    Batch Number:  1071  /  1875    Autoencoder loss:  0.21225281059741974\n",
      "Epoch:  0    Batch Number:  1072  /  1875    Autoencoder loss:  0.22619450092315674\n",
      "Epoch:  0    Batch Number:  1073  /  1875    Autoencoder loss:  0.21034851670265198\n",
      "Epoch:  0    Batch Number:  1074  /  1875    Autoencoder loss:  0.2075275480747223\n",
      "Epoch:  0    Batch Number:  1075  /  1875    Autoencoder loss:  0.21320103108882904\n",
      "Epoch:  0    Batch Number:  1076  /  1875    Autoencoder loss:  0.2108605057001114\n",
      "Epoch:  0    Batch Number:  1077  /  1875    Autoencoder loss:  0.195804163813591\n",
      "Epoch:  0    Batch Number:  1078  /  1875    Autoencoder loss:  0.22052255272865295\n",
      "Epoch:  0    Batch Number:  1079  /  1875    Autoencoder loss:  0.21846510469913483\n",
      "Epoch:  0    Batch Number:  1080  /  1875    Autoencoder loss:  0.18640880286693573\n",
      "Epoch:  0    Batch Number:  1081  /  1875    Autoencoder loss:  0.21580447256565094\n",
      "Epoch:  0    Batch Number:  1082  /  1875    Autoencoder loss:  0.19881470501422882\n",
      "Epoch:  0    Batch Number:  1083  /  1875    Autoencoder loss:  0.20250658690929413\n",
      "Epoch:  0    Batch Number:  1084  /  1875    Autoencoder loss:  0.19284407794475555\n",
      "Epoch:  0    Batch Number:  1085  /  1875    Autoencoder loss:  0.2055579274892807\n",
      "Epoch:  0    Batch Number:  1086  /  1875    Autoencoder loss:  0.21599052846431732\n",
      "Epoch:  0    Batch Number:  1087  /  1875    Autoencoder loss:  0.21607710421085358\n",
      "Epoch:  0    Batch Number:  1088  /  1875    Autoencoder loss:  0.21270136535167694\n",
      "Epoch:  0    Batch Number:  1089  /  1875    Autoencoder loss:  0.19886904954910278\n",
      "Epoch:  0    Batch Number:  1090  /  1875    Autoencoder loss:  0.19787932932376862\n",
      "Epoch:  0    Batch Number:  1091  /  1875    Autoencoder loss:  0.1959833800792694\n",
      "Epoch:  0    Batch Number:  1092  /  1875    Autoencoder loss:  0.20908832550048828\n",
      "Epoch:  0    Batch Number:  1093  /  1875    Autoencoder loss:  0.20143114030361176\n",
      "Epoch:  0    Batch Number:  1094  /  1875    Autoencoder loss:  0.20323453843593597\n",
      "Epoch:  0    Batch Number:  1095  /  1875    Autoencoder loss:  0.19383683800697327\n",
      "Epoch:  0    Batch Number:  1096  /  1875    Autoencoder loss:  0.1793062686920166\n",
      "Epoch:  0    Batch Number:  1097  /  1875    Autoencoder loss:  0.19876046478748322\n",
      "Epoch:  0    Batch Number:  1098  /  1875    Autoencoder loss:  0.1964688003063202\n",
      "Epoch:  0    Batch Number:  1099  /  1875    Autoencoder loss:  0.20791380107402802\n",
      "Epoch:  0    Batch Number:  1100  /  1875    Autoencoder loss:  0.18753184378147125\n",
      "Epoch:  0    Batch Number:  1101  /  1875    Autoencoder loss:  0.19985121488571167\n",
      "Epoch:  0    Batch Number:  1102  /  1875    Autoencoder loss:  0.19925150275230408\n",
      "Epoch:  0    Batch Number:  1103  /  1875    Autoencoder loss:  0.22026976943016052\n",
      "Epoch:  0    Batch Number:  1104  /  1875    Autoencoder loss:  0.22203993797302246\n",
      "Epoch:  0    Batch Number:  1105  /  1875    Autoencoder loss:  0.1846206784248352\n",
      "Epoch:  0    Batch Number:  1106  /  1875    Autoencoder loss:  0.1987542062997818\n",
      "Epoch:  0    Batch Number:  1107  /  1875    Autoencoder loss:  0.20172671973705292\n",
      "Epoch:  0    Batch Number:  1108  /  1875    Autoencoder loss:  0.2005683332681656\n",
      "Epoch:  0    Batch Number:  1109  /  1875    Autoencoder loss:  0.1876131147146225\n",
      "Epoch:  0    Batch Number:  1110  /  1875    Autoencoder loss:  0.21005581319332123\n",
      "Epoch:  0    Batch Number:  1111  /  1875    Autoencoder loss:  0.20724670588970184\n",
      "Epoch:  0    Batch Number:  1112  /  1875    Autoencoder loss:  0.21483337879180908\n",
      "Epoch:  0    Batch Number:  1113  /  1875    Autoencoder loss:  0.21426016092300415\n",
      "Epoch:  0    Batch Number:  1114  /  1875    Autoencoder loss:  0.19699542224407196\n",
      "Epoch:  0    Batch Number:  1115  /  1875    Autoencoder loss:  0.18443089723587036\n",
      "Epoch:  0    Batch Number:  1116  /  1875    Autoencoder loss:  0.19584135711193085\n",
      "Epoch:  0    Batch Number:  1117  /  1875    Autoencoder loss:  0.19404202699661255\n",
      "Epoch:  0    Batch Number:  1118  /  1875    Autoencoder loss:  0.19565415382385254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1119  /  1875    Autoencoder loss:  0.1978956013917923\n",
      "Epoch:  0    Batch Number:  1120  /  1875    Autoencoder loss:  0.2120898962020874\n",
      "Epoch:  0    Batch Number:  1121  /  1875    Autoencoder loss:  0.21423736214637756\n",
      "Epoch:  0    Batch Number:  1122  /  1875    Autoencoder loss:  0.2289694994688034\n",
      "Epoch:  0    Batch Number:  1123  /  1875    Autoencoder loss:  0.19684354960918427\n",
      "Epoch:  0    Batch Number:  1124  /  1875    Autoencoder loss:  0.19926542043685913\n",
      "Epoch:  0    Batch Number:  1125  /  1875    Autoencoder loss:  0.21730847656726837\n",
      "Epoch:  0    Batch Number:  1126  /  1875    Autoencoder loss:  0.19564153254032135\n",
      "Epoch:  0    Batch Number:  1127  /  1875    Autoencoder loss:  0.20133398473262787\n",
      "Epoch:  0    Batch Number:  1128  /  1875    Autoencoder loss:  0.1945343017578125\n",
      "Epoch:  0    Batch Number:  1129  /  1875    Autoencoder loss:  0.1973036229610443\n",
      "Epoch:  0    Batch Number:  1130  /  1875    Autoencoder loss:  0.21743063628673553\n",
      "Epoch:  0    Batch Number:  1131  /  1875    Autoencoder loss:  0.21225978434085846\n",
      "Epoch:  0    Batch Number:  1132  /  1875    Autoencoder loss:  0.20683631300926208\n",
      "Epoch:  0    Batch Number:  1133  /  1875    Autoencoder loss:  0.1865461766719818\n",
      "Epoch:  0    Batch Number:  1134  /  1875    Autoencoder loss:  0.21273328363895416\n",
      "Epoch:  0    Batch Number:  1135  /  1875    Autoencoder loss:  0.19017815589904785\n",
      "Epoch:  0    Batch Number:  1136  /  1875    Autoencoder loss:  0.21027271449565887\n",
      "Epoch:  0    Batch Number:  1137  /  1875    Autoencoder loss:  0.21077685058116913\n",
      "Epoch:  0    Batch Number:  1138  /  1875    Autoencoder loss:  0.1755145937204361\n",
      "Epoch:  0    Batch Number:  1139  /  1875    Autoencoder loss:  0.21202342212200165\n",
      "Epoch:  0    Batch Number:  1140  /  1875    Autoencoder loss:  0.20622564852237701\n",
      "Epoch:  0    Batch Number:  1141  /  1875    Autoencoder loss:  0.18945655226707458\n",
      "Epoch:  0    Batch Number:  1142  /  1875    Autoencoder loss:  0.19125568866729736\n",
      "Epoch:  0    Batch Number:  1143  /  1875    Autoencoder loss:  0.18964417278766632\n",
      "Epoch:  0    Batch Number:  1144  /  1875    Autoencoder loss:  0.20140528678894043\n",
      "Epoch:  0    Batch Number:  1145  /  1875    Autoencoder loss:  0.20757903158664703\n",
      "Epoch:  0    Batch Number:  1146  /  1875    Autoencoder loss:  0.218980610370636\n",
      "Epoch:  0    Batch Number:  1147  /  1875    Autoencoder loss:  0.17748279869556427\n",
      "Epoch:  0    Batch Number:  1148  /  1875    Autoencoder loss:  0.21099574863910675\n",
      "Epoch:  0    Batch Number:  1149  /  1875    Autoencoder loss:  0.2075648009777069\n",
      "Epoch:  0    Batch Number:  1150  /  1875    Autoencoder loss:  0.20417392253875732\n",
      "Epoch:  0    Batch Number:  1151  /  1875    Autoencoder loss:  0.1835058182477951\n",
      "Epoch:  0    Batch Number:  1152  /  1875    Autoencoder loss:  0.20083463191986084\n",
      "Epoch:  0    Batch Number:  1153  /  1875    Autoencoder loss:  0.19672821462154388\n",
      "Epoch:  0    Batch Number:  1154  /  1875    Autoencoder loss:  0.2116895467042923\n",
      "Epoch:  0    Batch Number:  1155  /  1875    Autoencoder loss:  0.20007532835006714\n",
      "Epoch:  0    Batch Number:  1156  /  1875    Autoencoder loss:  0.22002707421779633\n",
      "Epoch:  0    Batch Number:  1157  /  1875    Autoencoder loss:  0.22531047463417053\n",
      "Epoch:  0    Batch Number:  1158  /  1875    Autoencoder loss:  0.21232783794403076\n",
      "Epoch:  0    Batch Number:  1159  /  1875    Autoencoder loss:  0.18309855461120605\n",
      "Epoch:  0    Batch Number:  1160  /  1875    Autoencoder loss:  0.1927141398191452\n",
      "Epoch:  0    Batch Number:  1161  /  1875    Autoencoder loss:  0.19789168238639832\n",
      "Epoch:  0    Batch Number:  1162  /  1875    Autoencoder loss:  0.20151416957378387\n",
      "Epoch:  0    Batch Number:  1163  /  1875    Autoencoder loss:  0.21271516382694244\n",
      "Epoch:  0    Batch Number:  1164  /  1875    Autoencoder loss:  0.20736059546470642\n",
      "Epoch:  0    Batch Number:  1165  /  1875    Autoencoder loss:  0.21011652052402496\n",
      "Epoch:  0    Batch Number:  1166  /  1875    Autoencoder loss:  0.18457859754562378\n",
      "Epoch:  0    Batch Number:  1167  /  1875    Autoencoder loss:  0.20320279896259308\n",
      "Epoch:  0    Batch Number:  1168  /  1875    Autoencoder loss:  0.18742766976356506\n",
      "Epoch:  0    Batch Number:  1169  /  1875    Autoencoder loss:  0.21473748981952667\n",
      "Epoch:  0    Batch Number:  1170  /  1875    Autoencoder loss:  0.19569836556911469\n",
      "Epoch:  0    Batch Number:  1171  /  1875    Autoencoder loss:  0.1839064359664917\n",
      "Epoch:  0    Batch Number:  1172  /  1875    Autoencoder loss:  0.21186281740665436\n",
      "Epoch:  0    Batch Number:  1173  /  1875    Autoencoder loss:  0.2083527147769928\n",
      "Epoch:  0    Batch Number:  1174  /  1875    Autoencoder loss:  0.20236529409885406\n",
      "Epoch:  0    Batch Number:  1175  /  1875    Autoencoder loss:  0.21562863886356354\n",
      "Epoch:  0    Batch Number:  1176  /  1875    Autoencoder loss:  0.22363121807575226\n",
      "Epoch:  0    Batch Number:  1177  /  1875    Autoencoder loss:  0.20718853175640106\n",
      "Epoch:  0    Batch Number:  1178  /  1875    Autoencoder loss:  0.20374251902103424\n",
      "Epoch:  0    Batch Number:  1179  /  1875    Autoencoder loss:  0.21111251413822174\n",
      "Epoch:  0    Batch Number:  1180  /  1875    Autoencoder loss:  0.22967587411403656\n",
      "Epoch:  0    Batch Number:  1181  /  1875    Autoencoder loss:  0.19986578822135925\n",
      "Epoch:  0    Batch Number:  1182  /  1875    Autoencoder loss:  0.19110418856143951\n",
      "Epoch:  0    Batch Number:  1183  /  1875    Autoencoder loss:  0.18777236342430115\n",
      "Epoch:  0    Batch Number:  1184  /  1875    Autoencoder loss:  0.22234149277210236\n",
      "Epoch:  0    Batch Number:  1185  /  1875    Autoencoder loss:  0.22210736572742462\n",
      "Epoch:  0    Batch Number:  1186  /  1875    Autoencoder loss:  0.21880562603473663\n",
      "Epoch:  0    Batch Number:  1187  /  1875    Autoencoder loss:  0.1859426647424698\n",
      "Epoch:  0    Batch Number:  1188  /  1875    Autoencoder loss:  0.20847134292125702\n",
      "Epoch:  0    Batch Number:  1189  /  1875    Autoencoder loss:  0.20423708856105804\n",
      "Epoch:  0    Batch Number:  1190  /  1875    Autoencoder loss:  0.20518499612808228\n",
      "Epoch:  0    Batch Number:  1191  /  1875    Autoencoder loss:  0.20628084242343903\n",
      "Epoch:  0    Batch Number:  1192  /  1875    Autoencoder loss:  0.19277113676071167\n",
      "Epoch:  0    Batch Number:  1193  /  1875    Autoencoder loss:  0.19673612713813782\n",
      "Epoch:  0    Batch Number:  1194  /  1875    Autoencoder loss:  0.2046166956424713\n",
      "Epoch:  0    Batch Number:  1195  /  1875    Autoencoder loss:  0.20343850553035736\n",
      "Epoch:  0    Batch Number:  1196  /  1875    Autoencoder loss:  0.20818820595741272\n",
      "Epoch:  0    Batch Number:  1197  /  1875    Autoencoder loss:  0.21593627333641052\n",
      "Epoch:  0    Batch Number:  1198  /  1875    Autoencoder loss:  0.20463907718658447\n",
      "Epoch:  0    Batch Number:  1199  /  1875    Autoencoder loss:  0.20431430637836456\n",
      "Epoch:  0    Batch Number:  1200  /  1875    Autoencoder loss:  0.18834680318832397\n",
      "Epoch:  0    Batch Number:  1201  /  1875    Autoencoder loss:  0.20829202234745026\n",
      "Epoch:  0    Batch Number:  1202  /  1875    Autoencoder loss:  0.22151592373847961\n",
      "Epoch:  0    Batch Number:  1203  /  1875    Autoencoder loss:  0.18088015913963318\n",
      "Epoch:  0    Batch Number:  1204  /  1875    Autoencoder loss:  0.20880329608917236\n",
      "Epoch:  0    Batch Number:  1205  /  1875    Autoencoder loss:  0.1904214322566986\n",
      "Epoch:  0    Batch Number:  1206  /  1875    Autoencoder loss:  0.21040138602256775\n",
      "Epoch:  0    Batch Number:  1207  /  1875    Autoencoder loss:  0.20051363110542297\n",
      "Epoch:  0    Batch Number:  1208  /  1875    Autoencoder loss:  0.1945938915014267\n",
      "Epoch:  0    Batch Number:  1209  /  1875    Autoencoder loss:  0.197293221950531\n",
      "Epoch:  0    Batch Number:  1210  /  1875    Autoencoder loss:  0.18852023780345917\n",
      "Epoch:  0    Batch Number:  1211  /  1875    Autoencoder loss:  0.1923912912607193\n",
      "Epoch:  0    Batch Number:  1212  /  1875    Autoencoder loss:  0.190900981426239\n",
      "Epoch:  0    Batch Number:  1213  /  1875    Autoencoder loss:  0.17678681015968323\n",
      "Epoch:  0    Batch Number:  1214  /  1875    Autoencoder loss:  0.19532635807991028\n",
      "Epoch:  0    Batch Number:  1215  /  1875    Autoencoder loss:  0.20956319570541382\n",
      "Epoch:  0    Batch Number:  1216  /  1875    Autoencoder loss:  0.19729220867156982\n",
      "Epoch:  0    Batch Number:  1217  /  1875    Autoencoder loss:  0.19805355370044708\n",
      "Epoch:  0    Batch Number:  1218  /  1875    Autoencoder loss:  0.1880042850971222\n",
      "Epoch:  0    Batch Number:  1219  /  1875    Autoencoder loss:  0.2222374826669693\n",
      "Epoch:  0    Batch Number:  1220  /  1875    Autoencoder loss:  0.19703558087348938\n",
      "Epoch:  0    Batch Number:  1221  /  1875    Autoencoder loss:  0.19426600635051727\n",
      "Epoch:  0    Batch Number:  1222  /  1875    Autoencoder loss:  0.21375352144241333\n",
      "Epoch:  0    Batch Number:  1223  /  1875    Autoencoder loss:  0.1977241486310959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1224  /  1875    Autoencoder loss:  0.1929844617843628\n",
      "Epoch:  0    Batch Number:  1225  /  1875    Autoencoder loss:  0.19385536015033722\n",
      "Epoch:  0    Batch Number:  1226  /  1875    Autoencoder loss:  0.19563114643096924\n",
      "Epoch:  0    Batch Number:  1227  /  1875    Autoencoder loss:  0.20489710569381714\n",
      "Epoch:  0    Batch Number:  1228  /  1875    Autoencoder loss:  0.18053843080997467\n",
      "Epoch:  0    Batch Number:  1229  /  1875    Autoencoder loss:  0.21272696554660797\n",
      "Epoch:  0    Batch Number:  1230  /  1875    Autoencoder loss:  0.1947127878665924\n",
      "Epoch:  0    Batch Number:  1231  /  1875    Autoencoder loss:  0.20636986196041107\n",
      "Epoch:  0    Batch Number:  1232  /  1875    Autoencoder loss:  0.20249693095684052\n",
      "Epoch:  0    Batch Number:  1233  /  1875    Autoencoder loss:  0.20156489312648773\n",
      "Epoch:  0    Batch Number:  1234  /  1875    Autoencoder loss:  0.20966283977031708\n",
      "Epoch:  0    Batch Number:  1235  /  1875    Autoencoder loss:  0.20584481954574585\n",
      "Epoch:  0    Batch Number:  1236  /  1875    Autoencoder loss:  0.19805499911308289\n",
      "Epoch:  0    Batch Number:  1237  /  1875    Autoencoder loss:  0.19086545705795288\n",
      "Epoch:  0    Batch Number:  1238  /  1875    Autoencoder loss:  0.20820757746696472\n",
      "Epoch:  0    Batch Number:  1239  /  1875    Autoencoder loss:  0.19898682832717896\n",
      "Epoch:  0    Batch Number:  1240  /  1875    Autoencoder loss:  0.2040402889251709\n",
      "Epoch:  0    Batch Number:  1241  /  1875    Autoencoder loss:  0.225361168384552\n",
      "Epoch:  0    Batch Number:  1242  /  1875    Autoencoder loss:  0.22403550148010254\n",
      "Epoch:  0    Batch Number:  1243  /  1875    Autoencoder loss:  0.2140534669160843\n",
      "Epoch:  0    Batch Number:  1244  /  1875    Autoencoder loss:  0.19639933109283447\n",
      "Epoch:  0    Batch Number:  1245  /  1875    Autoencoder loss:  0.18566933274269104\n",
      "Epoch:  0    Batch Number:  1246  /  1875    Autoencoder loss:  0.19913291931152344\n",
      "Epoch:  0    Batch Number:  1247  /  1875    Autoencoder loss:  0.18731315433979034\n",
      "Epoch:  0    Batch Number:  1248  /  1875    Autoencoder loss:  0.19931189715862274\n",
      "Epoch:  0    Batch Number:  1249  /  1875    Autoencoder loss:  0.212905615568161\n",
      "Epoch:  0    Batch Number:  1250  /  1875    Autoencoder loss:  0.21263371407985687\n",
      "Epoch:  0    Batch Number:  1251  /  1875    Autoencoder loss:  0.2017485797405243\n",
      "Epoch:  0    Batch Number:  1252  /  1875    Autoencoder loss:  0.19662564992904663\n",
      "Epoch:  0    Batch Number:  1253  /  1875    Autoencoder loss:  0.18891841173171997\n",
      "Epoch:  0    Batch Number:  1254  /  1875    Autoencoder loss:  0.2086774706840515\n",
      "Epoch:  0    Batch Number:  1255  /  1875    Autoencoder loss:  0.19976140558719635\n",
      "Epoch:  0    Batch Number:  1256  /  1875    Autoencoder loss:  0.20424115657806396\n",
      "Epoch:  0    Batch Number:  1257  /  1875    Autoencoder loss:  0.19800257682800293\n",
      "Epoch:  0    Batch Number:  1258  /  1875    Autoencoder loss:  0.1864207535982132\n",
      "Epoch:  0    Batch Number:  1259  /  1875    Autoencoder loss:  0.21927952766418457\n",
      "Epoch:  0    Batch Number:  1260  /  1875    Autoencoder loss:  0.20351289212703705\n",
      "Epoch:  0    Batch Number:  1261  /  1875    Autoencoder loss:  0.1808699071407318\n",
      "Epoch:  0    Batch Number:  1262  /  1875    Autoencoder loss:  0.2010865956544876\n",
      "Epoch:  0    Batch Number:  1263  /  1875    Autoencoder loss:  0.19640420377254486\n",
      "Epoch:  0    Batch Number:  1264  /  1875    Autoencoder loss:  0.19014231860637665\n",
      "Epoch:  0    Batch Number:  1265  /  1875    Autoencoder loss:  0.18245938420295715\n",
      "Epoch:  0    Batch Number:  1266  /  1875    Autoencoder loss:  0.18591612577438354\n",
      "Epoch:  0    Batch Number:  1267  /  1875    Autoencoder loss:  0.20335975289344788\n",
      "Epoch:  0    Batch Number:  1268  /  1875    Autoencoder loss:  0.1958373337984085\n",
      "Epoch:  0    Batch Number:  1269  /  1875    Autoencoder loss:  0.19889213144779205\n",
      "Epoch:  0    Batch Number:  1270  /  1875    Autoencoder loss:  0.20828811824321747\n",
      "Epoch:  0    Batch Number:  1271  /  1875    Autoencoder loss:  0.19239725172519684\n",
      "Epoch:  0    Batch Number:  1272  /  1875    Autoencoder loss:  0.20815874636173248\n",
      "Epoch:  0    Batch Number:  1273  /  1875    Autoencoder loss:  0.20124806463718414\n",
      "Epoch:  0    Batch Number:  1274  /  1875    Autoencoder loss:  0.20496141910552979\n",
      "Epoch:  0    Batch Number:  1275  /  1875    Autoencoder loss:  0.21314717829227448\n",
      "Epoch:  0    Batch Number:  1276  /  1875    Autoencoder loss:  0.1969659924507141\n",
      "Epoch:  0    Batch Number:  1277  /  1875    Autoencoder loss:  0.17402447760105133\n",
      "Epoch:  0    Batch Number:  1278  /  1875    Autoencoder loss:  0.2114863097667694\n",
      "Epoch:  0    Batch Number:  1279  /  1875    Autoencoder loss:  0.2128700315952301\n",
      "Epoch:  0    Batch Number:  1280  /  1875    Autoencoder loss:  0.21476048231124878\n",
      "Epoch:  0    Batch Number:  1281  /  1875    Autoencoder loss:  0.19748644530773163\n",
      "Epoch:  0    Batch Number:  1282  /  1875    Autoencoder loss:  0.21510352194309235\n",
      "Epoch:  0    Batch Number:  1283  /  1875    Autoencoder loss:  0.20276284217834473\n",
      "Epoch:  0    Batch Number:  1284  /  1875    Autoencoder loss:  0.1761597990989685\n",
      "Epoch:  0    Batch Number:  1285  /  1875    Autoencoder loss:  0.2024107128381729\n",
      "Epoch:  0    Batch Number:  1286  /  1875    Autoencoder loss:  0.21124964952468872\n",
      "Epoch:  0    Batch Number:  1287  /  1875    Autoencoder loss:  0.21593965590000153\n",
      "Epoch:  0    Batch Number:  1288  /  1875    Autoencoder loss:  0.1960836946964264\n",
      "Epoch:  0    Batch Number:  1289  /  1875    Autoencoder loss:  0.19623814523220062\n",
      "Epoch:  0    Batch Number:  1290  /  1875    Autoencoder loss:  0.19184158742427826\n",
      "Epoch:  0    Batch Number:  1291  /  1875    Autoencoder loss:  0.19730381667613983\n",
      "Epoch:  0    Batch Number:  1292  /  1875    Autoencoder loss:  0.18682143092155457\n",
      "Epoch:  0    Batch Number:  1293  /  1875    Autoencoder loss:  0.1903974413871765\n",
      "Epoch:  0    Batch Number:  1294  /  1875    Autoencoder loss:  0.1818823516368866\n",
      "Epoch:  0    Batch Number:  1295  /  1875    Autoencoder loss:  0.19710665941238403\n",
      "Epoch:  0    Batch Number:  1296  /  1875    Autoencoder loss:  0.1895429491996765\n",
      "Epoch:  0    Batch Number:  1297  /  1875    Autoencoder loss:  0.2121461033821106\n",
      "Epoch:  0    Batch Number:  1298  /  1875    Autoencoder loss:  0.20275633037090302\n",
      "Epoch:  0    Batch Number:  1299  /  1875    Autoencoder loss:  0.1960555464029312\n",
      "Epoch:  0    Batch Number:  1300  /  1875    Autoencoder loss:  0.19059893488883972\n",
      "Epoch:  0    Batch Number:  1301  /  1875    Autoencoder loss:  0.2183120846748352\n",
      "Epoch:  0    Batch Number:  1302  /  1875    Autoencoder loss:  0.19668202102184296\n",
      "Epoch:  0    Batch Number:  1303  /  1875    Autoencoder loss:  0.19813041388988495\n",
      "Epoch:  0    Batch Number:  1304  /  1875    Autoencoder loss:  0.20454370975494385\n",
      "Epoch:  0    Batch Number:  1305  /  1875    Autoencoder loss:  0.21915049850940704\n",
      "Epoch:  0    Batch Number:  1306  /  1875    Autoencoder loss:  0.1950816661119461\n",
      "Epoch:  0    Batch Number:  1307  /  1875    Autoencoder loss:  0.19452029466629028\n",
      "Epoch:  0    Batch Number:  1308  /  1875    Autoencoder loss:  0.18156713247299194\n",
      "Epoch:  0    Batch Number:  1309  /  1875    Autoencoder loss:  0.20394936203956604\n",
      "Epoch:  0    Batch Number:  1310  /  1875    Autoencoder loss:  0.22796626389026642\n",
      "Epoch:  0    Batch Number:  1311  /  1875    Autoencoder loss:  0.1859886795282364\n",
      "Epoch:  0    Batch Number:  1312  /  1875    Autoencoder loss:  0.2050202190876007\n",
      "Epoch:  0    Batch Number:  1313  /  1875    Autoencoder loss:  0.19005268812179565\n",
      "Epoch:  0    Batch Number:  1314  /  1875    Autoencoder loss:  0.2162051945924759\n",
      "Epoch:  0    Batch Number:  1315  /  1875    Autoencoder loss:  0.21384596824645996\n",
      "Epoch:  0    Batch Number:  1316  /  1875    Autoencoder loss:  0.17364895343780518\n",
      "Epoch:  0    Batch Number:  1317  /  1875    Autoencoder loss:  0.1965172290802002\n",
      "Epoch:  0    Batch Number:  1318  /  1875    Autoencoder loss:  0.18995659053325653\n",
      "Epoch:  0    Batch Number:  1319  /  1875    Autoencoder loss:  0.21028417348861694\n",
      "Epoch:  0    Batch Number:  1320  /  1875    Autoencoder loss:  0.20200207829475403\n",
      "Epoch:  0    Batch Number:  1321  /  1875    Autoencoder loss:  0.17851467430591583\n",
      "Epoch:  0    Batch Number:  1322  /  1875    Autoencoder loss:  0.19975821673870087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1323  /  1875    Autoencoder loss:  0.21758721768856049\n",
      "Epoch:  0    Batch Number:  1324  /  1875    Autoencoder loss:  0.2122306078672409\n",
      "Epoch:  0    Batch Number:  1325  /  1875    Autoencoder loss:  0.18448609113693237\n",
      "Epoch:  0    Batch Number:  1326  /  1875    Autoencoder loss:  0.19388416409492493\n",
      "Epoch:  0    Batch Number:  1327  /  1875    Autoencoder loss:  0.190106600522995\n",
      "Epoch:  0    Batch Number:  1328  /  1875    Autoencoder loss:  0.19800376892089844\n",
      "Epoch:  0    Batch Number:  1329  /  1875    Autoencoder loss:  0.18914242088794708\n",
      "Epoch:  0    Batch Number:  1330  /  1875    Autoencoder loss:  0.1835830807685852\n",
      "Epoch:  0    Batch Number:  1331  /  1875    Autoencoder loss:  0.1938140094280243\n",
      "Epoch:  0    Batch Number:  1332  /  1875    Autoencoder loss:  0.17242373526096344\n",
      "Epoch:  0    Batch Number:  1333  /  1875    Autoencoder loss:  0.1808953583240509\n",
      "Epoch:  0    Batch Number:  1334  /  1875    Autoencoder loss:  0.1988786906003952\n",
      "Epoch:  0    Batch Number:  1335  /  1875    Autoencoder loss:  0.21079640090465546\n",
      "Epoch:  0    Batch Number:  1336  /  1875    Autoencoder loss:  0.19702312350273132\n",
      "Epoch:  0    Batch Number:  1337  /  1875    Autoencoder loss:  0.20405170321464539\n",
      "Epoch:  0    Batch Number:  1338  /  1875    Autoencoder loss:  0.20178598165512085\n",
      "Epoch:  0    Batch Number:  1339  /  1875    Autoencoder loss:  0.20426426827907562\n",
      "Epoch:  0    Batch Number:  1340  /  1875    Autoencoder loss:  0.2002512514591217\n",
      "Epoch:  0    Batch Number:  1341  /  1875    Autoencoder loss:  0.19729956984519958\n",
      "Epoch:  0    Batch Number:  1342  /  1875    Autoencoder loss:  0.2018495351076126\n",
      "Epoch:  0    Batch Number:  1343  /  1875    Autoencoder loss:  0.19696730375289917\n",
      "Epoch:  0    Batch Number:  1344  /  1875    Autoencoder loss:  0.20299096405506134\n",
      "Epoch:  0    Batch Number:  1345  /  1875    Autoencoder loss:  0.1979636698961258\n",
      "Epoch:  0    Batch Number:  1346  /  1875    Autoencoder loss:  0.17941665649414062\n",
      "Epoch:  0    Batch Number:  1347  /  1875    Autoencoder loss:  0.1800694316625595\n",
      "Epoch:  0    Batch Number:  1348  /  1875    Autoencoder loss:  0.19704031944274902\n",
      "Epoch:  0    Batch Number:  1349  /  1875    Autoencoder loss:  0.20831570029258728\n",
      "Epoch:  0    Batch Number:  1350  /  1875    Autoencoder loss:  0.21380837261676788\n",
      "Epoch:  0    Batch Number:  1351  /  1875    Autoencoder loss:  0.2081245481967926\n",
      "Epoch:  0    Batch Number:  1352  /  1875    Autoencoder loss:  0.18110516667366028\n",
      "Epoch:  0    Batch Number:  1353  /  1875    Autoencoder loss:  0.1951388120651245\n",
      "Epoch:  0    Batch Number:  1354  /  1875    Autoencoder loss:  0.2011188268661499\n",
      "Epoch:  0    Batch Number:  1355  /  1875    Autoencoder loss:  0.21197867393493652\n",
      "Epoch:  0    Batch Number:  1356  /  1875    Autoencoder loss:  0.19060644507408142\n",
      "Epoch:  0    Batch Number:  1357  /  1875    Autoencoder loss:  0.19384098052978516\n",
      "Epoch:  0    Batch Number:  1358  /  1875    Autoencoder loss:  0.18834425508975983\n",
      "Epoch:  0    Batch Number:  1359  /  1875    Autoencoder loss:  0.17252589762210846\n",
      "Epoch:  0    Batch Number:  1360  /  1875    Autoencoder loss:  0.1914505809545517\n",
      "Epoch:  0    Batch Number:  1361  /  1875    Autoencoder loss:  0.19918830692768097\n",
      "Epoch:  0    Batch Number:  1362  /  1875    Autoencoder loss:  0.20844587683677673\n",
      "Epoch:  0    Batch Number:  1363  /  1875    Autoencoder loss:  0.19161193072795868\n",
      "Epoch:  0    Batch Number:  1364  /  1875    Autoencoder loss:  0.18339814245700836\n",
      "Epoch:  0    Batch Number:  1365  /  1875    Autoencoder loss:  0.20506587624549866\n",
      "Epoch:  0    Batch Number:  1366  /  1875    Autoencoder loss:  0.18733632564544678\n",
      "Epoch:  0    Batch Number:  1367  /  1875    Autoencoder loss:  0.19794954359531403\n",
      "Epoch:  0    Batch Number:  1368  /  1875    Autoencoder loss:  0.2012420892715454\n",
      "Epoch:  0    Batch Number:  1369  /  1875    Autoencoder loss:  0.19852295517921448\n",
      "Epoch:  0    Batch Number:  1370  /  1875    Autoencoder loss:  0.21492742002010345\n",
      "Epoch:  0    Batch Number:  1371  /  1875    Autoencoder loss:  0.2072504460811615\n",
      "Epoch:  0    Batch Number:  1372  /  1875    Autoencoder loss:  0.2201341837644577\n",
      "Epoch:  0    Batch Number:  1373  /  1875    Autoencoder loss:  0.209975928068161\n",
      "Epoch:  0    Batch Number:  1374  /  1875    Autoencoder loss:  0.1997937113046646\n",
      "Epoch:  0    Batch Number:  1375  /  1875    Autoencoder loss:  0.17634539306163788\n",
      "Epoch:  0    Batch Number:  1376  /  1875    Autoencoder loss:  0.20857106149196625\n",
      "Epoch:  0    Batch Number:  1377  /  1875    Autoencoder loss:  0.19711972773075104\n",
      "Epoch:  0    Batch Number:  1378  /  1875    Autoencoder loss:  0.19026367366313934\n",
      "Epoch:  0    Batch Number:  1379  /  1875    Autoencoder loss:  0.2092529833316803\n",
      "Epoch:  0    Batch Number:  1380  /  1875    Autoencoder loss:  0.205471470952034\n",
      "Epoch:  0    Batch Number:  1381  /  1875    Autoencoder loss:  0.19483816623687744\n",
      "Epoch:  0    Batch Number:  1382  /  1875    Autoencoder loss:  0.19547705352306366\n",
      "Epoch:  0    Batch Number:  1383  /  1875    Autoencoder loss:  0.20000796020030975\n",
      "Epoch:  0    Batch Number:  1384  /  1875    Autoencoder loss:  0.19595035910606384\n",
      "Epoch:  0    Batch Number:  1385  /  1875    Autoencoder loss:  0.21675491333007812\n",
      "Epoch:  0    Batch Number:  1386  /  1875    Autoencoder loss:  0.19082629680633545\n",
      "Epoch:  0    Batch Number:  1387  /  1875    Autoencoder loss:  0.20380587875843048\n",
      "Epoch:  0    Batch Number:  1388  /  1875    Autoencoder loss:  0.19937831163406372\n",
      "Epoch:  0    Batch Number:  1389  /  1875    Autoencoder loss:  0.2075057327747345\n",
      "Epoch:  0    Batch Number:  1390  /  1875    Autoencoder loss:  0.19600963592529297\n",
      "Epoch:  0    Batch Number:  1391  /  1875    Autoencoder loss:  0.20104019343852997\n",
      "Epoch:  0    Batch Number:  1392  /  1875    Autoencoder loss:  0.2155134379863739\n",
      "Epoch:  0    Batch Number:  1393  /  1875    Autoencoder loss:  0.20168852806091309\n",
      "Epoch:  0    Batch Number:  1394  /  1875    Autoencoder loss:  0.21377867460250854\n",
      "Epoch:  0    Batch Number:  1395  /  1875    Autoencoder loss:  0.21547964215278625\n",
      "Epoch:  0    Batch Number:  1396  /  1875    Autoencoder loss:  0.209722638130188\n",
      "Epoch:  0    Batch Number:  1397  /  1875    Autoencoder loss:  0.21549682319164276\n",
      "Epoch:  0    Batch Number:  1398  /  1875    Autoencoder loss:  0.19119973480701447\n",
      "Epoch:  0    Batch Number:  1399  /  1875    Autoencoder loss:  0.19606617093086243\n",
      "Epoch:  0    Batch Number:  1400  /  1875    Autoencoder loss:  0.18761731684207916\n",
      "Epoch:  0    Batch Number:  1401  /  1875    Autoencoder loss:  0.19680821895599365\n",
      "Epoch:  0    Batch Number:  1402  /  1875    Autoencoder loss:  0.20434033870697021\n",
      "Epoch:  0    Batch Number:  1403  /  1875    Autoencoder loss:  0.1820576786994934\n",
      "Epoch:  0    Batch Number:  1404  /  1875    Autoencoder loss:  0.21172890067100525\n",
      "Epoch:  0    Batch Number:  1405  /  1875    Autoencoder loss:  0.18931253254413605\n",
      "Epoch:  0    Batch Number:  1406  /  1875    Autoencoder loss:  0.21993547677993774\n",
      "Epoch:  0    Batch Number:  1407  /  1875    Autoencoder loss:  0.20682252943515778\n",
      "Epoch:  0    Batch Number:  1408  /  1875    Autoencoder loss:  0.20199818909168243\n",
      "Epoch:  0    Batch Number:  1409  /  1875    Autoencoder loss:  0.20313461124897003\n",
      "Epoch:  0    Batch Number:  1410  /  1875    Autoencoder loss:  0.17480285465717316\n",
      "Epoch:  0    Batch Number:  1411  /  1875    Autoencoder loss:  0.20308928191661835\n",
      "Epoch:  0    Batch Number:  1412  /  1875    Autoencoder loss:  0.19730322062969208\n",
      "Epoch:  0    Batch Number:  1413  /  1875    Autoencoder loss:  0.20301789045333862\n",
      "Epoch:  0    Batch Number:  1414  /  1875    Autoencoder loss:  0.20636367797851562\n",
      "Epoch:  0    Batch Number:  1415  /  1875    Autoencoder loss:  0.20281927287578583\n",
      "Epoch:  0    Batch Number:  1416  /  1875    Autoencoder loss:  0.1967136263847351\n",
      "Epoch:  0    Batch Number:  1417  /  1875    Autoencoder loss:  0.19666340947151184\n",
      "Epoch:  0    Batch Number:  1418  /  1875    Autoencoder loss:  0.1814013123512268\n",
      "Epoch:  0    Batch Number:  1419  /  1875    Autoencoder loss:  0.2048650085926056\n",
      "Epoch:  0    Batch Number:  1420  /  1875    Autoencoder loss:  0.19943594932556152\n",
      "Epoch:  0    Batch Number:  1421  /  1875    Autoencoder loss:  0.20302414894104004\n",
      "Epoch:  0    Batch Number:  1422  /  1875    Autoencoder loss:  0.20212262868881226\n",
      "Epoch:  0    Batch Number:  1423  /  1875    Autoencoder loss:  0.17736954987049103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1424  /  1875    Autoencoder loss:  0.18859797716140747\n",
      "Epoch:  0    Batch Number:  1425  /  1875    Autoencoder loss:  0.17693889141082764\n",
      "Epoch:  0    Batch Number:  1426  /  1875    Autoencoder loss:  0.1967526227235794\n",
      "Epoch:  0    Batch Number:  1427  /  1875    Autoencoder loss:  0.2142830342054367\n",
      "Epoch:  0    Batch Number:  1428  /  1875    Autoencoder loss:  0.1934206634759903\n",
      "Epoch:  0    Batch Number:  1429  /  1875    Autoencoder loss:  0.18590740859508514\n",
      "Epoch:  0    Batch Number:  1430  /  1875    Autoencoder loss:  0.2004222869873047\n",
      "Epoch:  0    Batch Number:  1431  /  1875    Autoencoder loss:  0.2015547901391983\n",
      "Epoch:  0    Batch Number:  1432  /  1875    Autoencoder loss:  0.20078857243061066\n",
      "Epoch:  0    Batch Number:  1433  /  1875    Autoencoder loss:  0.17569296061992645\n",
      "Epoch:  0    Batch Number:  1434  /  1875    Autoencoder loss:  0.20178624987602234\n",
      "Epoch:  0    Batch Number:  1435  /  1875    Autoencoder loss:  0.21871943771839142\n",
      "Epoch:  0    Batch Number:  1436  /  1875    Autoencoder loss:  0.1988835632801056\n",
      "Epoch:  0    Batch Number:  1437  /  1875    Autoencoder loss:  0.20527663826942444\n",
      "Epoch:  0    Batch Number:  1438  /  1875    Autoencoder loss:  0.18954156339168549\n",
      "Epoch:  0    Batch Number:  1439  /  1875    Autoencoder loss:  0.19936273992061615\n",
      "Epoch:  0    Batch Number:  1440  /  1875    Autoencoder loss:  0.19034740328788757\n",
      "Epoch:  0    Batch Number:  1441  /  1875    Autoencoder loss:  0.21637122333049774\n",
      "Epoch:  0    Batch Number:  1442  /  1875    Autoencoder loss:  0.19297614693641663\n",
      "Epoch:  0    Batch Number:  1443  /  1875    Autoencoder loss:  0.20284076035022736\n",
      "Epoch:  0    Batch Number:  1444  /  1875    Autoencoder loss:  0.19950060546398163\n",
      "Epoch:  0    Batch Number:  1445  /  1875    Autoencoder loss:  0.20537205040454865\n",
      "Epoch:  0    Batch Number:  1446  /  1875    Autoencoder loss:  0.1764134019613266\n",
      "Epoch:  0    Batch Number:  1447  /  1875    Autoencoder loss:  0.19852720201015472\n",
      "Epoch:  0    Batch Number:  1448  /  1875    Autoencoder loss:  0.19155728816986084\n",
      "Epoch:  0    Batch Number:  1449  /  1875    Autoencoder loss:  0.2017313688993454\n",
      "Epoch:  0    Batch Number:  1450  /  1875    Autoencoder loss:  0.20370782911777496\n",
      "Epoch:  0    Batch Number:  1451  /  1875    Autoencoder loss:  0.2057494968175888\n",
      "Epoch:  0    Batch Number:  1452  /  1875    Autoencoder loss:  0.17983902990818024\n",
      "Epoch:  0    Batch Number:  1453  /  1875    Autoencoder loss:  0.2092059850692749\n",
      "Epoch:  0    Batch Number:  1454  /  1875    Autoencoder loss:  0.20919276773929596\n",
      "Epoch:  0    Batch Number:  1455  /  1875    Autoencoder loss:  0.212820366024971\n",
      "Epoch:  0    Batch Number:  1456  /  1875    Autoencoder loss:  0.2102893590927124\n",
      "Epoch:  0    Batch Number:  1457  /  1875    Autoencoder loss:  0.19758760929107666\n",
      "Epoch:  0    Batch Number:  1458  /  1875    Autoencoder loss:  0.20255641639232635\n",
      "Epoch:  0    Batch Number:  1459  /  1875    Autoencoder loss:  0.2039875090122223\n",
      "Epoch:  0    Batch Number:  1460  /  1875    Autoencoder loss:  0.1998768448829651\n",
      "Epoch:  0    Batch Number:  1461  /  1875    Autoencoder loss:  0.21558664739131927\n",
      "Epoch:  0    Batch Number:  1462  /  1875    Autoencoder loss:  0.1887952834367752\n",
      "Epoch:  0    Batch Number:  1463  /  1875    Autoencoder loss:  0.2070184051990509\n",
      "Epoch:  0    Batch Number:  1464  /  1875    Autoencoder loss:  0.1977240890264511\n",
      "Epoch:  0    Batch Number:  1465  /  1875    Autoencoder loss:  0.20414060354232788\n",
      "Epoch:  0    Batch Number:  1466  /  1875    Autoencoder loss:  0.2172933667898178\n",
      "Epoch:  0    Batch Number:  1467  /  1875    Autoencoder loss:  0.19132505357265472\n",
      "Epoch:  0    Batch Number:  1468  /  1875    Autoencoder loss:  0.19647134840488434\n",
      "Epoch:  0    Batch Number:  1469  /  1875    Autoencoder loss:  0.21516568958759308\n",
      "Epoch:  0    Batch Number:  1470  /  1875    Autoencoder loss:  0.21648788452148438\n",
      "Epoch:  0    Batch Number:  1471  /  1875    Autoencoder loss:  0.20710818469524384\n",
      "Epoch:  0    Batch Number:  1472  /  1875    Autoencoder loss:  0.19374661147594452\n",
      "Epoch:  0    Batch Number:  1473  /  1875    Autoencoder loss:  0.19363395869731903\n",
      "Epoch:  0    Batch Number:  1474  /  1875    Autoencoder loss:  0.18898747861385345\n",
      "Epoch:  0    Batch Number:  1475  /  1875    Autoencoder loss:  0.1892736852169037\n",
      "Epoch:  0    Batch Number:  1476  /  1875    Autoencoder loss:  0.17960254848003387\n",
      "Epoch:  0    Batch Number:  1477  /  1875    Autoencoder loss:  0.194636732339859\n",
      "Epoch:  0    Batch Number:  1478  /  1875    Autoencoder loss:  0.1917058527469635\n",
      "Epoch:  0    Batch Number:  1479  /  1875    Autoencoder loss:  0.1981557160615921\n",
      "Epoch:  0    Batch Number:  1480  /  1875    Autoencoder loss:  0.19790418446063995\n",
      "Epoch:  0    Batch Number:  1481  /  1875    Autoencoder loss:  0.20354601740837097\n",
      "Epoch:  0    Batch Number:  1482  /  1875    Autoencoder loss:  0.20329582691192627\n",
      "Epoch:  0    Batch Number:  1483  /  1875    Autoencoder loss:  0.21079760789871216\n",
      "Epoch:  0    Batch Number:  1484  /  1875    Autoencoder loss:  0.2069447934627533\n",
      "Epoch:  0    Batch Number:  1485  /  1875    Autoencoder loss:  0.196205735206604\n",
      "Epoch:  0    Batch Number:  1486  /  1875    Autoencoder loss:  0.21312406659126282\n",
      "Epoch:  0    Batch Number:  1487  /  1875    Autoencoder loss:  0.20417633652687073\n",
      "Epoch:  0    Batch Number:  1488  /  1875    Autoencoder loss:  0.21421849727630615\n",
      "Epoch:  0    Batch Number:  1489  /  1875    Autoencoder loss:  0.2171594649553299\n",
      "Epoch:  0    Batch Number:  1490  /  1875    Autoencoder loss:  0.20479007065296173\n",
      "Epoch:  0    Batch Number:  1491  /  1875    Autoencoder loss:  0.18278731405735016\n",
      "Epoch:  0    Batch Number:  1492  /  1875    Autoencoder loss:  0.19622695446014404\n",
      "Epoch:  0    Batch Number:  1493  /  1875    Autoencoder loss:  0.20730151236057281\n",
      "Epoch:  0    Batch Number:  1494  /  1875    Autoencoder loss:  0.1835998296737671\n",
      "Epoch:  0    Batch Number:  1495  /  1875    Autoencoder loss:  0.1858185976743698\n",
      "Epoch:  0    Batch Number:  1496  /  1875    Autoencoder loss:  0.19995887577533722\n",
      "Epoch:  0    Batch Number:  1497  /  1875    Autoencoder loss:  0.20283599197864532\n",
      "Epoch:  0    Batch Number:  1498  /  1875    Autoencoder loss:  0.1799038052558899\n",
      "Epoch:  0    Batch Number:  1499  /  1875    Autoencoder loss:  0.1823522001504898\n",
      "Epoch:  0    Batch Number:  1500  /  1875    Autoencoder loss:  0.2000645101070404\n",
      "Epoch:  0    Batch Number:  1501  /  1875    Autoencoder loss:  0.19736187160015106\n",
      "Epoch:  0    Batch Number:  1502  /  1875    Autoencoder loss:  0.21398447453975677\n",
      "Epoch:  0    Batch Number:  1503  /  1875    Autoencoder loss:  0.1877700239419937\n",
      "Epoch:  0    Batch Number:  1504  /  1875    Autoencoder loss:  0.1910039186477661\n",
      "Epoch:  0    Batch Number:  1505  /  1875    Autoencoder loss:  0.18004582822322845\n",
      "Epoch:  0    Batch Number:  1506  /  1875    Autoencoder loss:  0.19712592661380768\n",
      "Epoch:  0    Batch Number:  1507  /  1875    Autoencoder loss:  0.19017818570137024\n",
      "Epoch:  0    Batch Number:  1508  /  1875    Autoencoder loss:  0.19927993416786194\n",
      "Epoch:  0    Batch Number:  1509  /  1875    Autoencoder loss:  0.18885566294193268\n",
      "Epoch:  0    Batch Number:  1510  /  1875    Autoencoder loss:  0.19391697645187378\n",
      "Epoch:  0    Batch Number:  1511  /  1875    Autoencoder loss:  0.20572546124458313\n",
      "Epoch:  0    Batch Number:  1512  /  1875    Autoencoder loss:  0.19013455510139465\n",
      "Epoch:  0    Batch Number:  1513  /  1875    Autoencoder loss:  0.19899843633174896\n",
      "Epoch:  0    Batch Number:  1514  /  1875    Autoencoder loss:  0.2045213282108307\n",
      "Epoch:  0    Batch Number:  1515  /  1875    Autoencoder loss:  0.19348099827766418\n",
      "Epoch:  0    Batch Number:  1516  /  1875    Autoencoder loss:  0.21990884840488434\n",
      "Epoch:  0    Batch Number:  1517  /  1875    Autoencoder loss:  0.21522125601768494\n",
      "Epoch:  0    Batch Number:  1518  /  1875    Autoencoder loss:  0.18221072852611542\n",
      "Epoch:  0    Batch Number:  1519  /  1875    Autoencoder loss:  0.20326396822929382\n",
      "Epoch:  0    Batch Number:  1520  /  1875    Autoencoder loss:  0.18730223178863525\n",
      "Epoch:  0    Batch Number:  1521  /  1875    Autoencoder loss:  0.1921359896659851\n",
      "Epoch:  0    Batch Number:  1522  /  1875    Autoencoder loss:  0.19439232349395752\n",
      "Epoch:  0    Batch Number:  1523  /  1875    Autoencoder loss:  0.2109246402978897\n",
      "Epoch:  0    Batch Number:  1524  /  1875    Autoencoder loss:  0.198976531624794\n",
      "Epoch:  0    Batch Number:  1525  /  1875    Autoencoder loss:  0.2123102843761444\n",
      "Epoch:  0    Batch Number:  1526  /  1875    Autoencoder loss:  0.20631751418113708\n",
      "Epoch:  0    Batch Number:  1527  /  1875    Autoencoder loss:  0.18890197575092316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1528  /  1875    Autoencoder loss:  0.21138928830623627\n",
      "Epoch:  0    Batch Number:  1529  /  1875    Autoencoder loss:  0.20689931511878967\n",
      "Epoch:  0    Batch Number:  1530  /  1875    Autoencoder loss:  0.192891925573349\n",
      "Epoch:  0    Batch Number:  1531  /  1875    Autoencoder loss:  0.20769146084785461\n",
      "Epoch:  0    Batch Number:  1532  /  1875    Autoencoder loss:  0.202606201171875\n",
      "Epoch:  0    Batch Number:  1533  /  1875    Autoencoder loss:  0.2106592208147049\n",
      "Epoch:  0    Batch Number:  1534  /  1875    Autoencoder loss:  0.19782614707946777\n",
      "Epoch:  0    Batch Number:  1535  /  1875    Autoencoder loss:  0.20671972632408142\n",
      "Epoch:  0    Batch Number:  1536  /  1875    Autoencoder loss:  0.21349354088306427\n",
      "Epoch:  0    Batch Number:  1537  /  1875    Autoencoder loss:  0.18773122131824493\n",
      "Epoch:  0    Batch Number:  1538  /  1875    Autoencoder loss:  0.19380925595760345\n",
      "Epoch:  0    Batch Number:  1539  /  1875    Autoencoder loss:  0.19779711961746216\n",
      "Epoch:  0    Batch Number:  1540  /  1875    Autoencoder loss:  0.18441930413246155\n",
      "Epoch:  0    Batch Number:  1541  /  1875    Autoencoder loss:  0.19366766512393951\n",
      "Epoch:  0    Batch Number:  1542  /  1875    Autoencoder loss:  0.20254260301589966\n",
      "Epoch:  0    Batch Number:  1543  /  1875    Autoencoder loss:  0.19023054838180542\n",
      "Epoch:  0    Batch Number:  1544  /  1875    Autoencoder loss:  0.21468999981880188\n",
      "Epoch:  0    Batch Number:  1545  /  1875    Autoencoder loss:  0.1919727474451065\n",
      "Epoch:  0    Batch Number:  1546  /  1875    Autoencoder loss:  0.17742285132408142\n",
      "Epoch:  0    Batch Number:  1547  /  1875    Autoencoder loss:  0.20365148782730103\n",
      "Epoch:  0    Batch Number:  1548  /  1875    Autoencoder loss:  0.1925407350063324\n",
      "Epoch:  0    Batch Number:  1549  /  1875    Autoencoder loss:  0.20781579613685608\n",
      "Epoch:  0    Batch Number:  1550  /  1875    Autoencoder loss:  0.20700983703136444\n",
      "Epoch:  0    Batch Number:  1551  /  1875    Autoencoder loss:  0.19085662066936493\n",
      "Epoch:  0    Batch Number:  1552  /  1875    Autoencoder loss:  0.20366458594799042\n",
      "Epoch:  0    Batch Number:  1553  /  1875    Autoencoder loss:  0.1983983963727951\n",
      "Epoch:  0    Batch Number:  1554  /  1875    Autoencoder loss:  0.21464285254478455\n",
      "Epoch:  0    Batch Number:  1555  /  1875    Autoencoder loss:  0.17614497244358063\n",
      "Epoch:  0    Batch Number:  1556  /  1875    Autoencoder loss:  0.1917644441127777\n",
      "Epoch:  0    Batch Number:  1557  /  1875    Autoencoder loss:  0.20339296758174896\n",
      "Epoch:  0    Batch Number:  1558  /  1875    Autoencoder loss:  0.20199674367904663\n",
      "Epoch:  0    Batch Number:  1559  /  1875    Autoencoder loss:  0.1856139898300171\n",
      "Epoch:  0    Batch Number:  1560  /  1875    Autoencoder loss:  0.20088806748390198\n",
      "Epoch:  0    Batch Number:  1561  /  1875    Autoencoder loss:  0.20104070007801056\n",
      "Epoch:  0    Batch Number:  1562  /  1875    Autoencoder loss:  0.18287047743797302\n",
      "Epoch:  0    Batch Number:  1563  /  1875    Autoencoder loss:  0.188973531126976\n",
      "Epoch:  0    Batch Number:  1564  /  1875    Autoencoder loss:  0.19274696707725525\n",
      "Epoch:  0    Batch Number:  1565  /  1875    Autoencoder loss:  0.20078635215759277\n",
      "Epoch:  0    Batch Number:  1566  /  1875    Autoencoder loss:  0.20543859899044037\n",
      "Epoch:  0    Batch Number:  1567  /  1875    Autoencoder loss:  0.2079719603061676\n",
      "Epoch:  0    Batch Number:  1568  /  1875    Autoencoder loss:  0.1943015605211258\n",
      "Epoch:  0    Batch Number:  1569  /  1875    Autoencoder loss:  0.20094871520996094\n",
      "Epoch:  0    Batch Number:  1570  /  1875    Autoencoder loss:  0.1828165203332901\n",
      "Epoch:  0    Batch Number:  1571  /  1875    Autoencoder loss:  0.18628017604351044\n",
      "Epoch:  0    Batch Number:  1572  /  1875    Autoencoder loss:  0.20067965984344482\n",
      "Epoch:  0    Batch Number:  1573  /  1875    Autoencoder loss:  0.19854409992694855\n",
      "Epoch:  0    Batch Number:  1574  /  1875    Autoencoder loss:  0.19816343486309052\n",
      "Epoch:  0    Batch Number:  1575  /  1875    Autoencoder loss:  0.20733320713043213\n",
      "Epoch:  0    Batch Number:  1576  /  1875    Autoencoder loss:  0.19719548523426056\n",
      "Epoch:  0    Batch Number:  1577  /  1875    Autoencoder loss:  0.18798457086086273\n",
      "Epoch:  0    Batch Number:  1578  /  1875    Autoencoder loss:  0.1912132352590561\n",
      "Epoch:  0    Batch Number:  1579  /  1875    Autoencoder loss:  0.22096186876296997\n",
      "Epoch:  0    Batch Number:  1580  /  1875    Autoencoder loss:  0.19140543043613434\n",
      "Epoch:  0    Batch Number:  1581  /  1875    Autoencoder loss:  0.19125136733055115\n",
      "Epoch:  0    Batch Number:  1582  /  1875    Autoencoder loss:  0.17875805497169495\n",
      "Epoch:  0    Batch Number:  1583  /  1875    Autoencoder loss:  0.20165397226810455\n",
      "Epoch:  0    Batch Number:  1584  /  1875    Autoencoder loss:  0.18211497366428375\n",
      "Epoch:  0    Batch Number:  1585  /  1875    Autoencoder loss:  0.1874818205833435\n",
      "Epoch:  0    Batch Number:  1586  /  1875    Autoencoder loss:  0.18007667362689972\n",
      "Epoch:  0    Batch Number:  1587  /  1875    Autoencoder loss:  0.19853375852108002\n",
      "Epoch:  0    Batch Number:  1588  /  1875    Autoencoder loss:  0.19639238715171814\n",
      "Epoch:  0    Batch Number:  1589  /  1875    Autoencoder loss:  0.20666709542274475\n",
      "Epoch:  0    Batch Number:  1590  /  1875    Autoencoder loss:  0.17479188740253448\n",
      "Epoch:  0    Batch Number:  1591  /  1875    Autoencoder loss:  0.20928488671779633\n",
      "Epoch:  0    Batch Number:  1592  /  1875    Autoencoder loss:  0.1996580958366394\n",
      "Epoch:  0    Batch Number:  1593  /  1875    Autoencoder loss:  0.2086959332227707\n",
      "Epoch:  0    Batch Number:  1594  /  1875    Autoencoder loss:  0.2032163143157959\n",
      "Epoch:  0    Batch Number:  1595  /  1875    Autoencoder loss:  0.18909119069576263\n",
      "Epoch:  0    Batch Number:  1596  /  1875    Autoencoder loss:  0.20076143741607666\n",
      "Epoch:  0    Batch Number:  1597  /  1875    Autoencoder loss:  0.17855508625507355\n",
      "Epoch:  0    Batch Number:  1598  /  1875    Autoencoder loss:  0.20287953317165375\n",
      "Epoch:  0    Batch Number:  1599  /  1875    Autoencoder loss:  0.18607020378112793\n",
      "Epoch:  0    Batch Number:  1600  /  1875    Autoencoder loss:  0.19055791199207306\n",
      "Epoch:  0    Batch Number:  1601  /  1875    Autoencoder loss:  0.21496005356311798\n",
      "Epoch:  0    Batch Number:  1602  /  1875    Autoencoder loss:  0.2011900544166565\n",
      "Epoch:  0    Batch Number:  1603  /  1875    Autoencoder loss:  0.1889735609292984\n",
      "Epoch:  0    Batch Number:  1604  /  1875    Autoencoder loss:  0.19868314266204834\n",
      "Epoch:  0    Batch Number:  1605  /  1875    Autoencoder loss:  0.21151256561279297\n",
      "Epoch:  0    Batch Number:  1606  /  1875    Autoencoder loss:  0.20218634605407715\n",
      "Epoch:  0    Batch Number:  1607  /  1875    Autoencoder loss:  0.20218834280967712\n",
      "Epoch:  0    Batch Number:  1608  /  1875    Autoencoder loss:  0.18195699155330658\n",
      "Epoch:  0    Batch Number:  1609  /  1875    Autoencoder loss:  0.1927347332239151\n",
      "Epoch:  0    Batch Number:  1610  /  1875    Autoencoder loss:  0.20643289387226105\n",
      "Epoch:  0    Batch Number:  1611  /  1875    Autoencoder loss:  0.19652923941612244\n",
      "Epoch:  0    Batch Number:  1612  /  1875    Autoencoder loss:  0.20764876902103424\n",
      "Epoch:  0    Batch Number:  1613  /  1875    Autoencoder loss:  0.20637284219264984\n",
      "Epoch:  0    Batch Number:  1614  /  1875    Autoencoder loss:  0.20515257120132446\n",
      "Epoch:  0    Batch Number:  1615  /  1875    Autoencoder loss:  0.17524263262748718\n",
      "Epoch:  0    Batch Number:  1616  /  1875    Autoencoder loss:  0.19342710077762604\n",
      "Epoch:  0    Batch Number:  1617  /  1875    Autoencoder loss:  0.1994057446718216\n",
      "Epoch:  0    Batch Number:  1618  /  1875    Autoencoder loss:  0.19319701194763184\n",
      "Epoch:  0    Batch Number:  1619  /  1875    Autoencoder loss:  0.20041099190711975\n",
      "Epoch:  0    Batch Number:  1620  /  1875    Autoencoder loss:  0.19133080542087555\n",
      "Epoch:  0    Batch Number:  1621  /  1875    Autoencoder loss:  0.21863310039043427\n",
      "Epoch:  0    Batch Number:  1622  /  1875    Autoencoder loss:  0.17748649418354034\n",
      "Epoch:  0    Batch Number:  1623  /  1875    Autoencoder loss:  0.21990692615509033\n",
      "Epoch:  0    Batch Number:  1624  /  1875    Autoencoder loss:  0.18638081848621368\n",
      "Epoch:  0    Batch Number:  1625  /  1875    Autoencoder loss:  0.1916976124048233\n",
      "Epoch:  0    Batch Number:  1626  /  1875    Autoencoder loss:  0.21203507483005524\n",
      "Epoch:  0    Batch Number:  1627  /  1875    Autoencoder loss:  0.18034549057483673\n",
      "Epoch:  0    Batch Number:  1628  /  1875    Autoencoder loss:  0.20050260424613953\n",
      "Epoch:  0    Batch Number:  1629  /  1875    Autoencoder loss:  0.2113112211227417\n",
      "Epoch:  0    Batch Number:  1630  /  1875    Autoencoder loss:  0.1945372223854065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1631  /  1875    Autoencoder loss:  0.20001162588596344\n",
      "Epoch:  0    Batch Number:  1632  /  1875    Autoencoder loss:  0.21477088332176208\n",
      "Epoch:  0    Batch Number:  1633  /  1875    Autoencoder loss:  0.18736997246742249\n",
      "Epoch:  0    Batch Number:  1634  /  1875    Autoencoder loss:  0.18707306683063507\n",
      "Epoch:  0    Batch Number:  1635  /  1875    Autoencoder loss:  0.18534013628959656\n",
      "Epoch:  0    Batch Number:  1636  /  1875    Autoencoder loss:  0.19179889559745789\n",
      "Epoch:  0    Batch Number:  1637  /  1875    Autoencoder loss:  0.20537006855010986\n",
      "Epoch:  0    Batch Number:  1638  /  1875    Autoencoder loss:  0.18739113211631775\n",
      "Epoch:  0    Batch Number:  1639  /  1875    Autoencoder loss:  0.2000015676021576\n",
      "Epoch:  0    Batch Number:  1640  /  1875    Autoencoder loss:  0.19374066591262817\n",
      "Epoch:  0    Batch Number:  1641  /  1875    Autoencoder loss:  0.2129398137331009\n",
      "Epoch:  0    Batch Number:  1642  /  1875    Autoencoder loss:  0.1863592267036438\n",
      "Epoch:  0    Batch Number:  1643  /  1875    Autoencoder loss:  0.1786302626132965\n",
      "Epoch:  0    Batch Number:  1644  /  1875    Autoencoder loss:  0.19537539780139923\n",
      "Epoch:  0    Batch Number:  1645  /  1875    Autoencoder loss:  0.1988082081079483\n",
      "Epoch:  0    Batch Number:  1646  /  1875    Autoencoder loss:  0.2005758136510849\n",
      "Epoch:  0    Batch Number:  1647  /  1875    Autoencoder loss:  0.2011343091726303\n",
      "Epoch:  0    Batch Number:  1648  /  1875    Autoencoder loss:  0.19762349128723145\n",
      "Epoch:  0    Batch Number:  1649  /  1875    Autoencoder loss:  0.2059309035539627\n",
      "Epoch:  0    Batch Number:  1650  /  1875    Autoencoder loss:  0.19962221384048462\n",
      "Epoch:  0    Batch Number:  1651  /  1875    Autoencoder loss:  0.21114760637283325\n",
      "Epoch:  0    Batch Number:  1652  /  1875    Autoencoder loss:  0.18840546905994415\n",
      "Epoch:  0    Batch Number:  1653  /  1875    Autoencoder loss:  0.20418383181095123\n",
      "Epoch:  0    Batch Number:  1654  /  1875    Autoencoder loss:  0.19213910400867462\n",
      "Epoch:  0    Batch Number:  1655  /  1875    Autoencoder loss:  0.20123723149299622\n",
      "Epoch:  0    Batch Number:  1656  /  1875    Autoencoder loss:  0.1971176266670227\n",
      "Epoch:  0    Batch Number:  1657  /  1875    Autoencoder loss:  0.2179393172264099\n",
      "Epoch:  0    Batch Number:  1658  /  1875    Autoencoder loss:  0.1940750628709793\n",
      "Epoch:  0    Batch Number:  1659  /  1875    Autoencoder loss:  0.20573833584785461\n",
      "Epoch:  0    Batch Number:  1660  /  1875    Autoencoder loss:  0.21584179997444153\n",
      "Epoch:  0    Batch Number:  1661  /  1875    Autoencoder loss:  0.20420105755329132\n",
      "Epoch:  0    Batch Number:  1662  /  1875    Autoencoder loss:  0.1839619129896164\n",
      "Epoch:  0    Batch Number:  1663  /  1875    Autoencoder loss:  0.18651169538497925\n",
      "Epoch:  0    Batch Number:  1664  /  1875    Autoencoder loss:  0.19511599838733673\n",
      "Epoch:  0    Batch Number:  1665  /  1875    Autoencoder loss:  0.20314539968967438\n",
      "Epoch:  0    Batch Number:  1666  /  1875    Autoencoder loss:  0.20216821134090424\n",
      "Epoch:  0    Batch Number:  1667  /  1875    Autoencoder loss:  0.20310448110103607\n",
      "Epoch:  0    Batch Number:  1668  /  1875    Autoencoder loss:  0.2076774537563324\n",
      "Epoch:  0    Batch Number:  1669  /  1875    Autoencoder loss:  0.20167037844657898\n",
      "Epoch:  0    Batch Number:  1670  /  1875    Autoencoder loss:  0.21599306166172028\n",
      "Epoch:  0    Batch Number:  1671  /  1875    Autoencoder loss:  0.18399059772491455\n",
      "Epoch:  0    Batch Number:  1672  /  1875    Autoencoder loss:  0.1667143702507019\n",
      "Epoch:  0    Batch Number:  1673  /  1875    Autoencoder loss:  0.22067095339298248\n",
      "Epoch:  0    Batch Number:  1674  /  1875    Autoencoder loss:  0.20763924717903137\n",
      "Epoch:  0    Batch Number:  1675  /  1875    Autoencoder loss:  0.18436171114444733\n",
      "Epoch:  0    Batch Number:  1676  /  1875    Autoencoder loss:  0.1880027800798416\n",
      "Epoch:  0    Batch Number:  1677  /  1875    Autoencoder loss:  0.18849976360797882\n",
      "Epoch:  0    Batch Number:  1678  /  1875    Autoencoder loss:  0.1788894236087799\n",
      "Epoch:  0    Batch Number:  1679  /  1875    Autoencoder loss:  0.19301699101924896\n",
      "Epoch:  0    Batch Number:  1680  /  1875    Autoencoder loss:  0.18061096966266632\n",
      "Epoch:  0    Batch Number:  1681  /  1875    Autoencoder loss:  0.20542514324188232\n",
      "Epoch:  0    Batch Number:  1682  /  1875    Autoencoder loss:  0.19741202890872955\n",
      "Epoch:  0    Batch Number:  1683  /  1875    Autoencoder loss:  0.18196578323841095\n",
      "Epoch:  0    Batch Number:  1684  /  1875    Autoencoder loss:  0.19088612496852875\n",
      "Epoch:  0    Batch Number:  1685  /  1875    Autoencoder loss:  0.2011457234621048\n",
      "Epoch:  0    Batch Number:  1686  /  1875    Autoencoder loss:  0.17153917253017426\n",
      "Epoch:  0    Batch Number:  1687  /  1875    Autoencoder loss:  0.18888142704963684\n",
      "Epoch:  0    Batch Number:  1688  /  1875    Autoencoder loss:  0.21034324169158936\n",
      "Epoch:  0    Batch Number:  1689  /  1875    Autoencoder loss:  0.20338785648345947\n",
      "Epoch:  0    Batch Number:  1690  /  1875    Autoencoder loss:  0.19480475783348083\n",
      "Epoch:  0    Batch Number:  1691  /  1875    Autoencoder loss:  0.2001592218875885\n",
      "Epoch:  0    Batch Number:  1692  /  1875    Autoencoder loss:  0.19424761831760406\n",
      "Epoch:  0    Batch Number:  1693  /  1875    Autoencoder loss:  0.18132315576076508\n",
      "Epoch:  0    Batch Number:  1694  /  1875    Autoencoder loss:  0.18973815441131592\n",
      "Epoch:  0    Batch Number:  1695  /  1875    Autoencoder loss:  0.20272527635097504\n",
      "Epoch:  0    Batch Number:  1696  /  1875    Autoencoder loss:  0.20244847238063812\n",
      "Epoch:  0    Batch Number:  1697  /  1875    Autoencoder loss:  0.19949816167354584\n",
      "Epoch:  0    Batch Number:  1698  /  1875    Autoencoder loss:  0.19680771231651306\n",
      "Epoch:  0    Batch Number:  1699  /  1875    Autoencoder loss:  0.2105199098587036\n",
      "Epoch:  0    Batch Number:  1700  /  1875    Autoencoder loss:  0.18799999356269836\n",
      "Epoch:  0    Batch Number:  1701  /  1875    Autoencoder loss:  0.18910667300224304\n",
      "Epoch:  0    Batch Number:  1702  /  1875    Autoencoder loss:  0.198788583278656\n",
      "Epoch:  0    Batch Number:  1703  /  1875    Autoencoder loss:  0.20871548354625702\n",
      "Epoch:  0    Batch Number:  1704  /  1875    Autoencoder loss:  0.18936613202095032\n",
      "Epoch:  0    Batch Number:  1705  /  1875    Autoencoder loss:  0.21329833567142487\n",
      "Epoch:  0    Batch Number:  1706  /  1875    Autoencoder loss:  0.1919182986021042\n",
      "Epoch:  0    Batch Number:  1707  /  1875    Autoencoder loss:  0.21494750678539276\n",
      "Epoch:  0    Batch Number:  1708  /  1875    Autoencoder loss:  0.20068824291229248\n",
      "Epoch:  0    Batch Number:  1709  /  1875    Autoencoder loss:  0.20600008964538574\n",
      "Epoch:  0    Batch Number:  1710  /  1875    Autoencoder loss:  0.20632420480251312\n",
      "Epoch:  0    Batch Number:  1711  /  1875    Autoencoder loss:  0.17557358741760254\n",
      "Epoch:  0    Batch Number:  1712  /  1875    Autoencoder loss:  0.22497868537902832\n",
      "Epoch:  0    Batch Number:  1713  /  1875    Autoencoder loss:  0.19334857165813446\n",
      "Epoch:  0    Batch Number:  1714  /  1875    Autoencoder loss:  0.18721109628677368\n",
      "Epoch:  0    Batch Number:  1715  /  1875    Autoencoder loss:  0.1754538118839264\n",
      "Epoch:  0    Batch Number:  1716  /  1875    Autoencoder loss:  0.1895960569381714\n",
      "Epoch:  0    Batch Number:  1717  /  1875    Autoencoder loss:  0.20840448141098022\n",
      "Epoch:  0    Batch Number:  1718  /  1875    Autoencoder loss:  0.1859767735004425\n",
      "Epoch:  0    Batch Number:  1719  /  1875    Autoencoder loss:  0.19021238386631012\n",
      "Epoch:  0    Batch Number:  1720  /  1875    Autoencoder loss:  0.19259783625602722\n",
      "Epoch:  0    Batch Number:  1721  /  1875    Autoencoder loss:  0.19327272474765778\n",
      "Epoch:  0    Batch Number:  1722  /  1875    Autoencoder loss:  0.1923627108335495\n",
      "Epoch:  0    Batch Number:  1723  /  1875    Autoencoder loss:  0.2185126096010208\n",
      "Epoch:  0    Batch Number:  1724  /  1875    Autoencoder loss:  0.20195680856704712\n",
      "Epoch:  0    Batch Number:  1725  /  1875    Autoencoder loss:  0.17561471462249756\n",
      "Epoch:  0    Batch Number:  1726  /  1875    Autoencoder loss:  0.17054012417793274\n",
      "Epoch:  0    Batch Number:  1727  /  1875    Autoencoder loss:  0.1793982833623886\n",
      "Epoch:  0    Batch Number:  1728  /  1875    Autoencoder loss:  0.20162567496299744\n",
      "Epoch:  0    Batch Number:  1729  /  1875    Autoencoder loss:  0.20663820207118988\n",
      "Epoch:  0    Batch Number:  1730  /  1875    Autoencoder loss:  0.19975343346595764\n",
      "Epoch:  0    Batch Number:  1731  /  1875    Autoencoder loss:  0.1993715763092041\n",
      "Epoch:  0    Batch Number:  1732  /  1875    Autoencoder loss:  0.1895069181919098\n",
      "Epoch:  0    Batch Number:  1733  /  1875    Autoencoder loss:  0.173118457198143\n",
      "Epoch:  0    Batch Number:  1734  /  1875    Autoencoder loss:  0.21026568114757538\n",
      "Epoch:  0    Batch Number:  1735  /  1875    Autoencoder loss:  0.1848503053188324\n",
      "Epoch:  0    Batch Number:  1736  /  1875    Autoencoder loss:  0.20622722804546356\n",
      "Epoch:  0    Batch Number:  1737  /  1875    Autoencoder loss:  0.19277755916118622\n",
      "Epoch:  0    Batch Number:  1738  /  1875    Autoencoder loss:  0.20981496572494507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  1739  /  1875    Autoencoder loss:  0.1942078322172165\n",
      "Epoch:  0    Batch Number:  1740  /  1875    Autoencoder loss:  0.18381083011627197\n",
      "Epoch:  0    Batch Number:  1741  /  1875    Autoencoder loss:  0.21509121358394623\n",
      "Epoch:  0    Batch Number:  1742  /  1875    Autoencoder loss:  0.18225058913230896\n",
      "Epoch:  0    Batch Number:  1743  /  1875    Autoencoder loss:  0.1917218416929245\n",
      "Epoch:  0    Batch Number:  1744  /  1875    Autoencoder loss:  0.18497663736343384\n",
      "Epoch:  0    Batch Number:  1745  /  1875    Autoencoder loss:  0.2028380036354065\n",
      "Epoch:  0    Batch Number:  1746  /  1875    Autoencoder loss:  0.18411025404930115\n",
      "Epoch:  0    Batch Number:  1747  /  1875    Autoencoder loss:  0.19413001835346222\n",
      "Epoch:  0    Batch Number:  1748  /  1875    Autoencoder loss:  0.17232823371887207\n",
      "Epoch:  0    Batch Number:  1749  /  1875    Autoencoder loss:  0.18984559178352356\n",
      "Epoch:  0    Batch Number:  1750  /  1875    Autoencoder loss:  0.18498043715953827\n",
      "Epoch:  0    Batch Number:  1751  /  1875    Autoencoder loss:  0.19592545926570892\n",
      "Epoch:  0    Batch Number:  1752  /  1875    Autoencoder loss:  0.18503805994987488\n",
      "Epoch:  0    Batch Number:  1753  /  1875    Autoencoder loss:  0.1965271234512329\n",
      "Epoch:  0    Batch Number:  1754  /  1875    Autoencoder loss:  0.176976278424263\n",
      "Epoch:  0    Batch Number:  1755  /  1875    Autoencoder loss:  0.19468238949775696\n",
      "Epoch:  0    Batch Number:  1756  /  1875    Autoencoder loss:  0.18073821067810059\n",
      "Epoch:  0    Batch Number:  1757  /  1875    Autoencoder loss:  0.19692006707191467\n",
      "Epoch:  0    Batch Number:  1758  /  1875    Autoencoder loss:  0.1837322860956192\n",
      "Epoch:  0    Batch Number:  1759  /  1875    Autoencoder loss:  0.17598097026348114\n",
      "Epoch:  0    Batch Number:  1760  /  1875    Autoencoder loss:  0.2017439603805542\n",
      "Epoch:  0    Batch Number:  1761  /  1875    Autoencoder loss:  0.18510983884334564\n",
      "Epoch:  0    Batch Number:  1762  /  1875    Autoencoder loss:  0.18385562300682068\n",
      "Epoch:  0    Batch Number:  1763  /  1875    Autoencoder loss:  0.2001238316297531\n",
      "Epoch:  0    Batch Number:  1764  /  1875    Autoencoder loss:  0.1869252622127533\n",
      "Epoch:  0    Batch Number:  1765  /  1875    Autoencoder loss:  0.18841278553009033\n",
      "Epoch:  0    Batch Number:  1766  /  1875    Autoencoder loss:  0.19421634078025818\n",
      "Epoch:  0    Batch Number:  1767  /  1875    Autoencoder loss:  0.2109464555978775\n",
      "Epoch:  0    Batch Number:  1768  /  1875    Autoencoder loss:  0.18786652386188507\n",
      "Epoch:  0    Batch Number:  1769  /  1875    Autoencoder loss:  0.20910058915615082\n",
      "Epoch:  0    Batch Number:  1770  /  1875    Autoencoder loss:  0.2041344940662384\n",
      "Epoch:  0    Batch Number:  1771  /  1875    Autoencoder loss:  0.21356216073036194\n",
      "Epoch:  0    Batch Number:  1772  /  1875    Autoencoder loss:  0.18404151499271393\n",
      "Epoch:  0    Batch Number:  1773  /  1875    Autoencoder loss:  0.19063490629196167\n",
      "Epoch:  0    Batch Number:  1774  /  1875    Autoencoder loss:  0.19538478553295135\n",
      "Epoch:  0    Batch Number:  1775  /  1875    Autoencoder loss:  0.20708808302879333\n",
      "Epoch:  0    Batch Number:  1776  /  1875    Autoencoder loss:  0.1983325332403183\n",
      "Epoch:  0    Batch Number:  1777  /  1875    Autoencoder loss:  0.17199431359767914\n",
      "Epoch:  0    Batch Number:  1778  /  1875    Autoencoder loss:  0.21317848563194275\n",
      "Epoch:  0    Batch Number:  1779  /  1875    Autoencoder loss:  0.20701707899570465\n",
      "Epoch:  0    Batch Number:  1780  /  1875    Autoencoder loss:  0.20472082495689392\n",
      "Epoch:  0    Batch Number:  1781  /  1875    Autoencoder loss:  0.20880386233329773\n",
      "Epoch:  0    Batch Number:  1782  /  1875    Autoencoder loss:  0.1990005373954773\n",
      "Epoch:  0    Batch Number:  1783  /  1875    Autoencoder loss:  0.19253014028072357\n",
      "Epoch:  0    Batch Number:  1784  /  1875    Autoencoder loss:  0.20126156508922577\n",
      "Epoch:  0    Batch Number:  1785  /  1875    Autoencoder loss:  0.19758111238479614\n",
      "Epoch:  0    Batch Number:  1786  /  1875    Autoencoder loss:  0.20616647601127625\n",
      "Epoch:  0    Batch Number:  1787  /  1875    Autoencoder loss:  0.19097842276096344\n",
      "Epoch:  0    Batch Number:  1788  /  1875    Autoencoder loss:  0.18764254450798035\n",
      "Epoch:  0    Batch Number:  1789  /  1875    Autoencoder loss:  0.19053730368614197\n",
      "Epoch:  0    Batch Number:  1790  /  1875    Autoencoder loss:  0.19672420620918274\n",
      "Epoch:  0    Batch Number:  1791  /  1875    Autoencoder loss:  0.20195168256759644\n",
      "Epoch:  0    Batch Number:  1792  /  1875    Autoencoder loss:  0.1953982561826706\n",
      "Epoch:  0    Batch Number:  1793  /  1875    Autoencoder loss:  0.19433312118053436\n",
      "Epoch:  0    Batch Number:  1794  /  1875    Autoencoder loss:  0.17515870928764343\n",
      "Epoch:  0    Batch Number:  1795  /  1875    Autoencoder loss:  0.17493678629398346\n",
      "Epoch:  0    Batch Number:  1796  /  1875    Autoencoder loss:  0.18096834421157837\n",
      "Epoch:  0    Batch Number:  1797  /  1875    Autoencoder loss:  0.2028682827949524\n",
      "Epoch:  0    Batch Number:  1798  /  1875    Autoencoder loss:  0.18981143832206726\n",
      "Epoch:  0    Batch Number:  1799  /  1875    Autoencoder loss:  0.20635449886322021\n",
      "Epoch:  0    Batch Number:  1800  /  1875    Autoencoder loss:  0.2014276534318924\n",
      "Epoch:  0    Batch Number:  1801  /  1875    Autoencoder loss:  0.2020304948091507\n",
      "Epoch:  0    Batch Number:  1802  /  1875    Autoencoder loss:  0.2183617204427719\n",
      "Epoch:  0    Batch Number:  1803  /  1875    Autoencoder loss:  0.1927056610584259\n",
      "Epoch:  0    Batch Number:  1804  /  1875    Autoencoder loss:  0.2003997266292572\n",
      "Epoch:  0    Batch Number:  1805  /  1875    Autoencoder loss:  0.17963716387748718\n",
      "Epoch:  0    Batch Number:  1806  /  1875    Autoencoder loss:  0.211093470454216\n",
      "Epoch:  0    Batch Number:  1807  /  1875    Autoencoder loss:  0.1812700480222702\n",
      "Epoch:  0    Batch Number:  1808  /  1875    Autoencoder loss:  0.19474075734615326\n",
      "Epoch:  0    Batch Number:  1809  /  1875    Autoencoder loss:  0.19599463045597076\n",
      "Epoch:  0    Batch Number:  1810  /  1875    Autoencoder loss:  0.20487143099308014\n",
      "Epoch:  0    Batch Number:  1811  /  1875    Autoencoder loss:  0.20021317899227142\n",
      "Epoch:  0    Batch Number:  1812  /  1875    Autoencoder loss:  0.17832057178020477\n",
      "Epoch:  0    Batch Number:  1813  /  1875    Autoencoder loss:  0.19713984429836273\n",
      "Epoch:  0    Batch Number:  1814  /  1875    Autoencoder loss:  0.19759567081928253\n",
      "Epoch:  0    Batch Number:  1815  /  1875    Autoencoder loss:  0.1926805078983307\n",
      "Epoch:  0    Batch Number:  1816  /  1875    Autoencoder loss:  0.1943454146385193\n",
      "Epoch:  0    Batch Number:  1817  /  1875    Autoencoder loss:  0.21164728701114655\n",
      "Epoch:  0    Batch Number:  1818  /  1875    Autoencoder loss:  0.19656115770339966\n",
      "Epoch:  0    Batch Number:  1819  /  1875    Autoencoder loss:  0.21305464208126068\n",
      "Epoch:  0    Batch Number:  1820  /  1875    Autoencoder loss:  0.17308185994625092\n",
      "Epoch:  0    Batch Number:  1821  /  1875    Autoencoder loss:  0.18647287786006927\n",
      "Epoch:  0    Batch Number:  1822  /  1875    Autoencoder loss:  0.21411168575286865\n",
      "Epoch:  0    Batch Number:  1823  /  1875    Autoencoder loss:  0.21006576716899872\n",
      "Epoch:  0    Batch Number:  1824  /  1875    Autoencoder loss:  0.19348140060901642\n",
      "Epoch:  0    Batch Number:  1825  /  1875    Autoencoder loss:  0.20153746008872986\n",
      "Epoch:  0    Batch Number:  1826  /  1875    Autoencoder loss:  0.19830793142318726\n",
      "Epoch: "
     ]
    }
   ],
   "source": [
    "# Start training cycle\n",
    "\n",
    "# Build the autoencoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "autoencoder_model = build_autoencoder(encoder, decoder)\n",
    "\n",
    "# Train Autoencoder\n",
    "train(encoder, decoder, autoencoder_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
